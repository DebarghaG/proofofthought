{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ProofOfThought \u00b6 LLM-guided translation of natural language to formal logic, verified via Z3 theorem prover. Architecture \u00b6 Question (NL) \u2193 LLM Translation (few-shot prompting) \u2193 Formal Program (SMT-LIB 2.0 or JSON DSL) \u2193 Z3 Execution \u2193 SAT/UNSAT \u2192 Boolean Answer Components \u00b6 Z3ProgramGenerator ( z3adapter.reasoning.program_generator ) LLM interface for program generation. Extracts formal programs from markdown code blocks using regex. Supports error feedback via multi-turn conversations. Backend ( z3adapter.backends.abstract ) Abstract interface: execute(program_path) \u2192 VerificationResult . Concrete implementations: SMT2Backend : Subprocess call to Z3 CLI. Parses stdout/stderr for sat / unsat via regex (?<!un)\\bsat\\b and \\bunsat\\b . JSONBackend : Python API execution via Z3JSONInterpreter . Returns structured SAT/UNSAT counts. Z3JSONInterpreter ( z3adapter.interpreter ) Multi-stage pipeline for JSON DSL: SortManager: Topological sort of type dependencies, create Z3 sorts ExpressionParser: eval() with restricted globals for safety Verifier: solver.check(condition) for each verification Return SAT/UNSAT counts ProofOfThought ( z3adapter.reasoning.proof_of_thought ) High-level API. Retry loop (default max_attempts=3 ) with error feedback. Answer determination: SAT only \u2192 True , UNSAT only \u2192 False , both/neither \u2192 None . Quick Start \u00b6 from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = \"...\" ) pot = ProofOfThought ( llm_client = client , backend = \"smt2\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) # result.answer: False (UNSAT) Benchmark Results \u00b6 Datasets: ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA Model: GPT-5 (Azure deployment) Config: max_attempts=3 , verify_timeout=10000ms Backend Avg Accuracy Success Rate SMT2 86.8% 99.4% JSON 82.8% 92.8% SMT2 outperforms JSON on 4/5 datasets. Full results: Benchmarks Design Rationale \u00b6 Why external theorem prover? LLMs lack deductive closure. Z3 provides sound logical inference. Why two backends? Trade portability (SMT-LIB standard) vs LLM generation reliability (structured JSON). Why iterative refinement? Single-shot generation insufficient. Error feedback improves success rate. Implementation Notes \u00b6 SMT2 Backend - Z3 subprocess with -T:timeout flag - Output parsing: regex on stdout/stderr - Standard SMT-LIB 2.0 S-expressions JSON Backend - Python Z3 API via z3-solver package - Expression evaluation: restricted eval() with ExpressionValidator - Supports built-in sorts: BoolSort , IntSort , RealSort - Custom sorts: DeclareSort , EnumSort , BitVecSort , ArraySort - Quantifiers: ForAll , Exists with variable binding Security JSON backend uses ExpressionValidator.safe_eval() with whitelisted Z3 operators. No arbitrary code execution. See: Backends , API Reference","title":"Home"},{"location":"#proofofthought","text":"LLM-guided translation of natural language to formal logic, verified via Z3 theorem prover.","title":"ProofOfThought"},{"location":"#architecture","text":"Question (NL) \u2193 LLM Translation (few-shot prompting) \u2193 Formal Program (SMT-LIB 2.0 or JSON DSL) \u2193 Z3 Execution \u2193 SAT/UNSAT \u2192 Boolean Answer","title":"Architecture"},{"location":"#components","text":"Z3ProgramGenerator ( z3adapter.reasoning.program_generator ) LLM interface for program generation. Extracts formal programs from markdown code blocks using regex. Supports error feedback via multi-turn conversations. Backend ( z3adapter.backends.abstract ) Abstract interface: execute(program_path) \u2192 VerificationResult . Concrete implementations: SMT2Backend : Subprocess call to Z3 CLI. Parses stdout/stderr for sat / unsat via regex (?<!un)\\bsat\\b and \\bunsat\\b . JSONBackend : Python API execution via Z3JSONInterpreter . Returns structured SAT/UNSAT counts. Z3JSONInterpreter ( z3adapter.interpreter ) Multi-stage pipeline for JSON DSL: SortManager: Topological sort of type dependencies, create Z3 sorts ExpressionParser: eval() with restricted globals for safety Verifier: solver.check(condition) for each verification Return SAT/UNSAT counts ProofOfThought ( z3adapter.reasoning.proof_of_thought ) High-level API. Retry loop (default max_attempts=3 ) with error feedback. Answer determination: SAT only \u2192 True , UNSAT only \u2192 False , both/neither \u2192 None .","title":"Components"},{"location":"#quick-start","text":"from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = \"...\" ) pot = ProofOfThought ( llm_client = client , backend = \"smt2\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) # result.answer: False (UNSAT)","title":"Quick Start"},{"location":"#benchmark-results","text":"Datasets: ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA Model: GPT-5 (Azure deployment) Config: max_attempts=3 , verify_timeout=10000ms Backend Avg Accuracy Success Rate SMT2 86.8% 99.4% JSON 82.8% 92.8% SMT2 outperforms JSON on 4/5 datasets. Full results: Benchmarks","title":"Benchmark Results"},{"location":"#design-rationale","text":"Why external theorem prover? LLMs lack deductive closure. Z3 provides sound logical inference. Why two backends? Trade portability (SMT-LIB standard) vs LLM generation reliability (structured JSON). Why iterative refinement? Single-shot generation insufficient. Error feedback improves success rate.","title":"Design Rationale"},{"location":"#implementation-notes","text":"SMT2 Backend - Z3 subprocess with -T:timeout flag - Output parsing: regex on stdout/stderr - Standard SMT-LIB 2.0 S-expressions JSON Backend - Python Z3 API via z3-solver package - Expression evaluation: restricted eval() with ExpressionValidator - Supports built-in sorts: BoolSort , IntSort , RealSort - Custom sorts: DeclareSort , EnumSort , BitVecSort , ArraySort - Quantifiers: ForAll , Exists with variable binding Security JSON backend uses ExpressionValidator.safe_eval() with whitelisted Z3 operators. No arbitrary code execution. See: Backends , API Reference","title":"Implementation Notes"},{"location":"api-reference/","text":"API Reference \u00b6 ProofOfThought \u00b6 z3adapter.reasoning.proof_of_thought.ProofOfThought Constructor \u00b6 def __init__ ( self , llm_client : Any , model : str = \"gpt-5\" , backend : Literal [ \"json\" , \"smt2\" ] = \"smt2\" , max_attempts : int = 3 , verify_timeout : int = 10000 , optimize_timeout : int = 100000 , cache_dir : str | None = None , z3_path : str = \"z3\" , ) -> None Parameters: llm_client : OpenAI/AzureOpenAI client instance model : Deployment/model name (default: \"gpt-5\" ) backend : \"json\" or \"smt2\" (default: \"smt2\" ) max_attempts : Retry limit for generation (default: 3 ) verify_timeout : Z3 timeout in milliseconds (default: 10000 ) optimize_timeout : Optimization timeout in ms, JSON only (default: 100000 ) cache_dir : Program cache directory (default: tempfile.gettempdir() ) z3_path : Z3 executable path for SMT2 (default: \"z3\" ) query() \u00b6 def query ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , save_program : bool = False , program_path : str | None = None , ) -> QueryResult Parameters: question : Natural language question temperature : LLM temperature (default: 0.1 , ignored for GPT-5 which only supports 1.0 ) max_tokens : Max completion tokens (default: 16384 ) save_program : Save generated program to disk (default: False ) program_path : Custom save path (default: auto-generated in cache_dir ) Returns: QueryResult Implementation: Retry loop with error feedback for attempt in range ( 1 , max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , temperature , max_tokens ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , temperature , max_tokens ) # ... execute and check result QueryResult \u00b6 @dataclass class QueryResult : question : str # Input question answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) json_program : dict [ str , Any ] | None # Generated program if JSON backend sat_count : int # SAT occurrences in output unsat_count : int # UNSAT occurrences output : str # Raw Z3 output success : bool # Execution completed num_attempts : int # Generation attempts used error : str | None # Error message if failed EvaluationPipeline \u00b6 z3adapter.reasoning.evaluation.EvaluationPipeline Constructor \u00b6 def __init__ ( self , proof_of_thought : ProofOfThought , output_dir : str = \"evaluation_results\" , num_workers : int = 1 , ) -> None Parameters: proof_of_thought : Configured ProofOfThought instance output_dir : Results directory (default: \"evaluation_results\" ) num_workers : Parallel workers (default: 1 , uses ThreadPoolExecutor if > 1 ) evaluate() \u00b6 def evaluate ( self , dataset : list [ dict [ str , Any ]] | str , question_field : str = \"question\" , answer_field : str = \"answer\" , id_field : str | None = None , max_samples : int | None = None , skip_existing : bool = True , ) -> EvaluationResult Parameters: dataset : JSON file path or list of dicts question_field : Field name for question text (default: \"question\" ) answer_field : Field name for ground truth (default: \"answer\" ) id_field : Field for sample ID (default: None , auto-generates sample_{idx} ) max_samples : Limit samples (default: None , all) skip_existing : Skip cached results (default: True ) Returns: EvaluationResult Caching: Saves {sample_id}_result.json and {sample_id}_program{ext} to output_dir EvaluationMetrics \u00b6 @dataclass class EvaluationMetrics : accuracy : float # sklearn.metrics.accuracy_score precision : float # sklearn.metrics.precision_score (zero_division=0) recall : float # sklearn.metrics.recall_score (zero_division=0) f1_score : float # 2 * (P * R) / (P + R) specificity : float # TN / (TN + FP) false_positive_rate : float # FP / (FP + TN) false_negative_rate : float # FN / (FN + TP) tp : int # True positives fp : int # False positives tn : int # True negatives fn : int # False negatives total_samples : int # Correct + wrong + failed correct_answers : int # answer == ground_truth wrong_answers : int # answer != ground_truth failed_answers : int # success == False Computed via sklearn.metrics.confusion_matrix for binary classification. Backend \u00b6 z3adapter.backends.abstract.Backend Abstract interface: class Backend ( ABC ): @abstractmethod def execute ( self , program_path : str ) -> VerificationResult : pass @abstractmethod def get_file_extension ( self ) -> str : pass @abstractmethod def get_prompt_template ( self ) -> str : pass def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None Implementations: SMT2Backend , JSONBackend VerificationResult \u00b6 @dataclass class VerificationResult : answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) sat_count : int unsat_count : int output : str # Raw execution output success : bool # Execution completed without exception error : str | None # Error message if failed Z3ProgramGenerator \u00b6 z3adapter.reasoning.program_generator.Z3ProgramGenerator generate() \u00b6 def generate ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , ) -> GenerationResult LLM API Call: response = self . llm_client . chat . completions . create ( model = self . model , messages = [{ \"role\" : \"user\" , \"content\" : prompt }], max_completion_tokens = max_tokens , ) Note: temperature parameter not passed (GPT-5 constraint). generate_with_feedback() \u00b6 Multi-turn conversation with error feedback: messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ] Utility: Azure Config \u00b6 utils.azure_config.get_client_config() Returns: { \"llm_client\" : AzureOpenAI ( ... ), \"model\" : str # Deployment name from env } Required environment variables: - AZURE_OPENAI_API_KEY - AZURE_OPENAI_ENDPOINT - AZURE_OPENAI_API_VERSION - AZURE_GPT5_DEPLOYMENT_NAME or AZURE_GPT4O_DEPLOYMENT_NAME","title":"API Reference"},{"location":"api-reference/#api-reference","text":"","title":"API Reference"},{"location":"api-reference/#proofofthought","text":"z3adapter.reasoning.proof_of_thought.ProofOfThought","title":"ProofOfThought"},{"location":"api-reference/#constructor","text":"def __init__ ( self , llm_client : Any , model : str = \"gpt-5\" , backend : Literal [ \"json\" , \"smt2\" ] = \"smt2\" , max_attempts : int = 3 , verify_timeout : int = 10000 , optimize_timeout : int = 100000 , cache_dir : str | None = None , z3_path : str = \"z3\" , ) -> None Parameters: llm_client : OpenAI/AzureOpenAI client instance model : Deployment/model name (default: \"gpt-5\" ) backend : \"json\" or \"smt2\" (default: \"smt2\" ) max_attempts : Retry limit for generation (default: 3 ) verify_timeout : Z3 timeout in milliseconds (default: 10000 ) optimize_timeout : Optimization timeout in ms, JSON only (default: 100000 ) cache_dir : Program cache directory (default: tempfile.gettempdir() ) z3_path : Z3 executable path for SMT2 (default: \"z3\" )","title":"Constructor"},{"location":"api-reference/#query","text":"def query ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , save_program : bool = False , program_path : str | None = None , ) -> QueryResult Parameters: question : Natural language question temperature : LLM temperature (default: 0.1 , ignored for GPT-5 which only supports 1.0 ) max_tokens : Max completion tokens (default: 16384 ) save_program : Save generated program to disk (default: False ) program_path : Custom save path (default: auto-generated in cache_dir ) Returns: QueryResult Implementation: Retry loop with error feedback for attempt in range ( 1 , max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , temperature , max_tokens ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , temperature , max_tokens ) # ... execute and check result","title":"query()"},{"location":"api-reference/#queryresult","text":"@dataclass class QueryResult : question : str # Input question answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) json_program : dict [ str , Any ] | None # Generated program if JSON backend sat_count : int # SAT occurrences in output unsat_count : int # UNSAT occurrences output : str # Raw Z3 output success : bool # Execution completed num_attempts : int # Generation attempts used error : str | None # Error message if failed","title":"QueryResult"},{"location":"api-reference/#evaluationpipeline","text":"z3adapter.reasoning.evaluation.EvaluationPipeline","title":"EvaluationPipeline"},{"location":"api-reference/#constructor_1","text":"def __init__ ( self , proof_of_thought : ProofOfThought , output_dir : str = \"evaluation_results\" , num_workers : int = 1 , ) -> None Parameters: proof_of_thought : Configured ProofOfThought instance output_dir : Results directory (default: \"evaluation_results\" ) num_workers : Parallel workers (default: 1 , uses ThreadPoolExecutor if > 1 )","title":"Constructor"},{"location":"api-reference/#evaluate","text":"def evaluate ( self , dataset : list [ dict [ str , Any ]] | str , question_field : str = \"question\" , answer_field : str = \"answer\" , id_field : str | None = None , max_samples : int | None = None , skip_existing : bool = True , ) -> EvaluationResult Parameters: dataset : JSON file path or list of dicts question_field : Field name for question text (default: \"question\" ) answer_field : Field name for ground truth (default: \"answer\" ) id_field : Field for sample ID (default: None , auto-generates sample_{idx} ) max_samples : Limit samples (default: None , all) skip_existing : Skip cached results (default: True ) Returns: EvaluationResult Caching: Saves {sample_id}_result.json and {sample_id}_program{ext} to output_dir","title":"evaluate()"},{"location":"api-reference/#evaluationmetrics","text":"@dataclass class EvaluationMetrics : accuracy : float # sklearn.metrics.accuracy_score precision : float # sklearn.metrics.precision_score (zero_division=0) recall : float # sklearn.metrics.recall_score (zero_division=0) f1_score : float # 2 * (P * R) / (P + R) specificity : float # TN / (TN + FP) false_positive_rate : float # FP / (FP + TN) false_negative_rate : float # FN / (FN + TP) tp : int # True positives fp : int # False positives tn : int # True negatives fn : int # False negatives total_samples : int # Correct + wrong + failed correct_answers : int # answer == ground_truth wrong_answers : int # answer != ground_truth failed_answers : int # success == False Computed via sklearn.metrics.confusion_matrix for binary classification.","title":"EvaluationMetrics"},{"location":"api-reference/#backend","text":"z3adapter.backends.abstract.Backend Abstract interface: class Backend ( ABC ): @abstractmethod def execute ( self , program_path : str ) -> VerificationResult : pass @abstractmethod def get_file_extension ( self ) -> str : pass @abstractmethod def get_prompt_template ( self ) -> str : pass def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None Implementations: SMT2Backend , JSONBackend","title":"Backend"},{"location":"api-reference/#verificationresult","text":"@dataclass class VerificationResult : answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) sat_count : int unsat_count : int output : str # Raw execution output success : bool # Execution completed without exception error : str | None # Error message if failed","title":"VerificationResult"},{"location":"api-reference/#z3programgenerator","text":"z3adapter.reasoning.program_generator.Z3ProgramGenerator","title":"Z3ProgramGenerator"},{"location":"api-reference/#generate","text":"def generate ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , ) -> GenerationResult LLM API Call: response = self . llm_client . chat . completions . create ( model = self . model , messages = [{ \"role\" : \"user\" , \"content\" : prompt }], max_completion_tokens = max_tokens , ) Note: temperature parameter not passed (GPT-5 constraint).","title":"generate()"},{"location":"api-reference/#generate_with_feedback","text":"Multi-turn conversation with error feedback: messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ]","title":"generate_with_feedback()"},{"location":"api-reference/#utility-azure-config","text":"utils.azure_config.get_client_config() Returns: { \"llm_client\" : AzureOpenAI ( ... ), \"model\" : str # Deployment name from env } Required environment variables: - AZURE_OPENAI_API_KEY - AZURE_OPENAI_ENDPOINT - AZURE_OPENAI_API_VERSION - AZURE_GPT5_DEPLOYMENT_NAME or AZURE_GPT4O_DEPLOYMENT_NAME","title":"Utility: Azure Config"},{"location":"backends/","text":"Backends \u00b6 Two execution backends for Z3: SMT-LIB 2.0 (standard) and JSON DSL (custom). SMT2Backend \u00b6 Implementation: z3adapter/backends/smt2_backend.py Execution \u00b6 subprocess . run ([ z3_path , f \"-T: { timeout_seconds } \" , program_path ]) Z3 CLI subprocess with timeout flag Hard timeout: timeout_seconds + 10 Output captured from stdout + stderr Result Parsing \u00b6 sat_pattern = r \"(?<!un)\\bsat\\b\" # Negative lookbehind to exclude \"unsat\" unsat_pattern = r \"\\bunsat\\b\" Counts occurrences in Z3 output. Answer logic: - sat_count > 0, unsat_count == 0 \u2192 True - unsat_count > 0, sat_count == 0 \u2192 False - Otherwise \u2192 None Prompt Template \u00b6 Source: z3adapter/reasoning/smt2_prompt_template.py Instructions for generating SMT-LIB 2.0 programs. Key requirements: - All commands as S-expressions: (command args...) - Declare sorts before use - Single (check-sat) per program - Semantic: sat = constraint satisfiable, unsat = contradicts knowledge base File Extension \u00b6 .smt2 JSON Backend \u00b6 Implementation: z3adapter/backends/json_backend.py Execution Pipeline \u00b6 interpreter = Z3JSONInterpreter ( program_path , verify_timeout , optimize_timeout ) interpreter . run () sat_count , unsat_count = interpreter . get_verification_counts () Z3JSONInterpreter Pipeline \u00b6 Step 1: SortManager ( z3adapter/dsl/sorts.py ) Topologically sorts type definitions to handle dependencies. Creates Z3 sorts: Built-in: BoolSort() , IntSort() , RealSort() (pre-defined) Custom: DeclareSort(name) , EnumSort(name, values) , BitVecSort(n) , ArraySort(domain, range) Example ArraySort dependency: { \"name\" : \"IntArray\" , \"type\" : \"ArraySort(IntSort, IntSort)\" } Requires IntSort already defined (built-in) before creating IntArray . Step 2: ExpressionParser ( z3adapter/dsl/expressions.py ) Parses logical expressions from strings via restricted eval() : safe_globals = { ** Z3_OPERATORS , ** functions } context = { ** functions , ** constants , ** variables , ** quantified_vars } ExpressionValidator . safe_eval ( expr_str , safe_globals , context ) Whitelisted operators: Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" } Step 3: Verifier ( z3adapter/verification/verifier.py ) For each verification condition: result = solver . check ( condition ) # Adds condition as hypothesis to KB if result == sat : sat_count += 1 elif result == unsat : unsat_count += 1 Verification Semantics: - solver.check(\u03c6) asks: \"Is KB \u2227 \u03c6 satisfiable?\" - SAT: \u03c6 is consistent with KB (possible) - UNSAT: \u03c6 contradicts KB (impossible) Prompt Template \u00b6 Source: z3adapter/reasoning/prompt_template.py Comprehensive 546-line specification of JSON DSL. Key sections: Sorts: { \"name\" : \"Person\" , \"type\" : \"DeclareSort\" } Functions: { \"name\" : \"supports\" , \"domain\" : [ \"Person\" , \"Issue\" ], \"range\" : \"BoolSort\" } Constants: { \"persons\" : { \"sort\" : \"Person\" , \"members\" : [ \"nancy_pelosi\" ]}} Variables: Free variables for quantifier binding: { \"name\" : \"p\" , \"sort\" : \"Person\" } Knowledge Base: [ \"ForAll([p], Implies(is_democrat(p), supports_abortion(p)))\" ] Verifications: Three types: Simple constraint: { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } Existential: { \"name\" : \"test\" , \"exists\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"constraint\" : \"x > 0\" } Universal: { \"name\" : \"test\" , \"forall\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"implies\" : { \"antecedent\" : \"x > 0\" , \"consequent\" : \"x >= 1\" }} Critical constraint: Single verification per question (avoid ambiguous results from testing both \u03c6 and \u00ac\u03c6). File Extension \u00b6 .json Benchmark Performance \u00b6 Results from experiments_pipeline.py (100 samples per dataset, GPT-5, max_attempts=3 ): Dataset SMT2 Accuracy JSON Accuracy SMT2 Success JSON Success ProntoQA 100% 99% 100% 100% FOLIO 69% 76% 99% 94% ProofWriter 99% 96% 99% 96% ConditionalQA 83% 76% 100% 89% StrategyQA 84% 68% 100% 86% Success Rate = percentage of queries completing without error (generation + execution). SMT2 higher accuracy on 4/5 datasets. JSON higher success rate variance (86-100% vs 99-100%). Implementation Differences \u00b6 Program Generation \u00b6 SMT2: Extract from markdown via: pattern = r \"```smt2\\s*([\\s\\S]*?)\\s*```\" JSON: Extract and parse via: pattern = r \"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" json . loads ( match . group ( 1 )) Error Handling \u00b6 SMT2: - Subprocess timeout \u2192 TimeoutExpired - Parse errors \u2192 regex mismatch \u2192 answer=None - Z3 errors in stderr \u2192 still parsed JSON: - JSON parse error \u2192 extraction failure - Z3 Python API exception \u2192 caught in try/except - Invalid sort reference \u2192 ValueError during SortManager - Expression eval error \u2192 ValueError during ExpressionParser Timeout Configuration \u00b6 SMT2: - Single timeout parameter: verify_timeout (ms) - Converted to seconds for Z3 CLI: verify_timeout // 1000 - Hard subprocess timeout: timeout_seconds + 10 JSON: - Two timeouts: verify_timeout (ms), optimize_timeout (ms) - Set via solver.set(\"timeout\", verify_timeout) in Verifier - Applies per solver.check() call Backend Selection Code \u00b6 if backend == \"json\" : from z3adapter.backends.json_backend import JSONBackend backend_instance = JSONBackend ( verify_timeout , optimize_timeout ) else : # smt2 from z3adapter.backends.smt2_backend import SMT2Backend backend_instance = SMT2Backend ( verify_timeout , z3_path ) File: z3adapter/reasoning/proof_of_thought.py:78-90 Prompt Selection \u00b6 if self . backend == \"json\" : prompt = build_prompt ( question ) else : # smt2 prompt = build_smt2_prompt ( question ) File: z3adapter/reasoning/program_generator.py:78-81 Prompts include few-shot examples and format specifications. SMT2 prompt emphasizes S-expression syntax. JSON prompt details variable scoping and quantifier semantics.","title":"Backends"},{"location":"backends/#backends","text":"Two execution backends for Z3: SMT-LIB 2.0 (standard) and JSON DSL (custom).","title":"Backends"},{"location":"backends/#smt2backend","text":"Implementation: z3adapter/backends/smt2_backend.py","title":"SMT2Backend"},{"location":"backends/#execution","text":"subprocess . run ([ z3_path , f \"-T: { timeout_seconds } \" , program_path ]) Z3 CLI subprocess with timeout flag Hard timeout: timeout_seconds + 10 Output captured from stdout + stderr","title":"Execution"},{"location":"backends/#result-parsing","text":"sat_pattern = r \"(?<!un)\\bsat\\b\" # Negative lookbehind to exclude \"unsat\" unsat_pattern = r \"\\bunsat\\b\" Counts occurrences in Z3 output. Answer logic: - sat_count > 0, unsat_count == 0 \u2192 True - unsat_count > 0, sat_count == 0 \u2192 False - Otherwise \u2192 None","title":"Result Parsing"},{"location":"backends/#prompt-template","text":"Source: z3adapter/reasoning/smt2_prompt_template.py Instructions for generating SMT-LIB 2.0 programs. Key requirements: - All commands as S-expressions: (command args...) - Declare sorts before use - Single (check-sat) per program - Semantic: sat = constraint satisfiable, unsat = contradicts knowledge base","title":"Prompt Template"},{"location":"backends/#file-extension","text":".smt2","title":"File Extension"},{"location":"backends/#json-backend","text":"Implementation: z3adapter/backends/json_backend.py","title":"JSON Backend"},{"location":"backends/#execution-pipeline","text":"interpreter = Z3JSONInterpreter ( program_path , verify_timeout , optimize_timeout ) interpreter . run () sat_count , unsat_count = interpreter . get_verification_counts ()","title":"Execution Pipeline"},{"location":"backends/#z3jsoninterpreter-pipeline","text":"Step 1: SortManager ( z3adapter/dsl/sorts.py ) Topologically sorts type definitions to handle dependencies. Creates Z3 sorts: Built-in: BoolSort() , IntSort() , RealSort() (pre-defined) Custom: DeclareSort(name) , EnumSort(name, values) , BitVecSort(n) , ArraySort(domain, range) Example ArraySort dependency: { \"name\" : \"IntArray\" , \"type\" : \"ArraySort(IntSort, IntSort)\" } Requires IntSort already defined (built-in) before creating IntArray . Step 2: ExpressionParser ( z3adapter/dsl/expressions.py ) Parses logical expressions from strings via restricted eval() : safe_globals = { ** Z3_OPERATORS , ** functions } context = { ** functions , ** constants , ** variables , ** quantified_vars } ExpressionValidator . safe_eval ( expr_str , safe_globals , context ) Whitelisted operators: Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" } Step 3: Verifier ( z3adapter/verification/verifier.py ) For each verification condition: result = solver . check ( condition ) # Adds condition as hypothesis to KB if result == sat : sat_count += 1 elif result == unsat : unsat_count += 1 Verification Semantics: - solver.check(\u03c6) asks: \"Is KB \u2227 \u03c6 satisfiable?\" - SAT: \u03c6 is consistent with KB (possible) - UNSAT: \u03c6 contradicts KB (impossible)","title":"Z3JSONInterpreter Pipeline"},{"location":"backends/#prompt-template_1","text":"Source: z3adapter/reasoning/prompt_template.py Comprehensive 546-line specification of JSON DSL. Key sections: Sorts: { \"name\" : \"Person\" , \"type\" : \"DeclareSort\" } Functions: { \"name\" : \"supports\" , \"domain\" : [ \"Person\" , \"Issue\" ], \"range\" : \"BoolSort\" } Constants: { \"persons\" : { \"sort\" : \"Person\" , \"members\" : [ \"nancy_pelosi\" ]}} Variables: Free variables for quantifier binding: { \"name\" : \"p\" , \"sort\" : \"Person\" } Knowledge Base: [ \"ForAll([p], Implies(is_democrat(p), supports_abortion(p)))\" ] Verifications: Three types: Simple constraint: { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } Existential: { \"name\" : \"test\" , \"exists\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"constraint\" : \"x > 0\" } Universal: { \"name\" : \"test\" , \"forall\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"implies\" : { \"antecedent\" : \"x > 0\" , \"consequent\" : \"x >= 1\" }} Critical constraint: Single verification per question (avoid ambiguous results from testing both \u03c6 and \u00ac\u03c6).","title":"Prompt Template"},{"location":"backends/#file-extension_1","text":".json","title":"File Extension"},{"location":"backends/#benchmark-performance","text":"Results from experiments_pipeline.py (100 samples per dataset, GPT-5, max_attempts=3 ): Dataset SMT2 Accuracy JSON Accuracy SMT2 Success JSON Success ProntoQA 100% 99% 100% 100% FOLIO 69% 76% 99% 94% ProofWriter 99% 96% 99% 96% ConditionalQA 83% 76% 100% 89% StrategyQA 84% 68% 100% 86% Success Rate = percentage of queries completing without error (generation + execution). SMT2 higher accuracy on 4/5 datasets. JSON higher success rate variance (86-100% vs 99-100%).","title":"Benchmark Performance"},{"location":"backends/#implementation-differences","text":"","title":"Implementation Differences"},{"location":"backends/#program-generation","text":"SMT2: Extract from markdown via: pattern = r \"```smt2\\s*([\\s\\S]*?)\\s*```\" JSON: Extract and parse via: pattern = r \"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" json . loads ( match . group ( 1 ))","title":"Program Generation"},{"location":"backends/#error-handling","text":"SMT2: - Subprocess timeout \u2192 TimeoutExpired - Parse errors \u2192 regex mismatch \u2192 answer=None - Z3 errors in stderr \u2192 still parsed JSON: - JSON parse error \u2192 extraction failure - Z3 Python API exception \u2192 caught in try/except - Invalid sort reference \u2192 ValueError during SortManager - Expression eval error \u2192 ValueError during ExpressionParser","title":"Error Handling"},{"location":"backends/#timeout-configuration","text":"SMT2: - Single timeout parameter: verify_timeout (ms) - Converted to seconds for Z3 CLI: verify_timeout // 1000 - Hard subprocess timeout: timeout_seconds + 10 JSON: - Two timeouts: verify_timeout (ms), optimize_timeout (ms) - Set via solver.set(\"timeout\", verify_timeout) in Verifier - Applies per solver.check() call","title":"Timeout Configuration"},{"location":"backends/#backend-selection-code","text":"if backend == \"json\" : from z3adapter.backends.json_backend import JSONBackend backend_instance = JSONBackend ( verify_timeout , optimize_timeout ) else : # smt2 from z3adapter.backends.smt2_backend import SMT2Backend backend_instance = SMT2Backend ( verify_timeout , z3_path ) File: z3adapter/reasoning/proof_of_thought.py:78-90","title":"Backend Selection Code"},{"location":"backends/#prompt-selection","text":"if self . backend == \"json\" : prompt = build_prompt ( question ) else : # smt2 prompt = build_smt2_prompt ( question ) File: z3adapter/reasoning/program_generator.py:78-81 Prompts include few-shot examples and format specifications. SMT2 prompt emphasizes S-expression syntax. JSON prompt details variable scoping and quantifier semantics.","title":"Prompt Selection"},{"location":"benchmarks/","text":"Benchmarks \u00b6 Evaluation on 5 logical reasoning datasets using Azure GPT-5. Methodology \u00b6 Model: Azure GPT-5 deployment Configuration: - max_attempts=3 (retry with error feedback) - verify_timeout=10000ms - optimize_timeout=100000ms (JSON backend only) - num_workers=10 (ThreadPoolExecutor for parallel processing) Metrics: sklearn.metrics - Accuracy: accuracy_score(y_true, y_pred) - Precision: precision_score(y_true, y_pred, zero_division=0) - Recall: recall_score(y_true, y_pred, zero_division=0) - F1: 2 * (precision * recall) / (precision + recall) - Success Rate: (total - failed) / total Execution: experiments_pipeline.py runs all benchmarks sequentially, modifying BACKEND variable in each benchmark/bench_*.py script via regex substitution. Results \u00b6 Last Updated: 2025-10-16 18:14:07 Benchmark Backend Samples Accuracy Precision Recall F1 Score Success Rate ProntoQA SMT2 100 100.00% 1.0000 1.0000 1.0000 100.00% FOLIO SMT2 100 69.00% 0.6949 0.7736 0.7321 99.00% ProofWriter SMT2 96 98.96% 1.0000 1.0000 1.0000 98.96% ConditionalQA SMT2 100 83.00% 0.9375 0.8219 0.8759 100.00% StrategyQA SMT2 100 84.00% 0.8205 0.7805 0.8000 100.00% ProntoQA JSON 100 99.00% 1.0000 0.9815 0.9907 100.00% FOLIO JSON 100 76.00% 0.7619 0.9412 0.8421 94.00% ProofWriter JSON 96 95.83% 1.0000 1.0000 1.0000 95.83% ConditionalQA JSON 100 76.00% 0.9180 0.8750 0.8960 89.00% StrategyQA JSON 100 68.00% 0.7500 0.7895 0.7692 86.00% Dataset Characteristics \u00b6 ProntoQA \u00b6 Synthetic first-order logic with deterministic inference. Example: Facts: \"Stella is a lion. All lions are brown.\" Question: \"Is Stella brown?\" Answer: True Performance: - SMT2: 100% (100/100) - JSON: 99% (99/100) Both backends near-perfect. Simplest dataset. FOLIO \u00b6 First-order logic from Wikipedia articles. Characteristics: Complex nested quantifiers, longer inference chains. Performance: - SMT2: 69% (69/100) - JSON: 76% (76/100) JSON outperforms SMT2 (+7%). Most challenging dataset. Lower success rate for JSON (94% vs 99%) indicates generation difficulties. ProofWriter \u00b6 Deductive reasoning over explicit facts and rules. Example: Facts: \"The bear is red. If something is red, then it is kind.\" Question: \"Is the bear kind?\" Answer: True Performance: - SMT2: 98.96% (95/96) - JSON: 95.83% (92/96) High accuracy for both. SMT2 slight edge (+3%). ConditionalQA \u00b6 Conditional reasoning with if-then statements. Performance: - SMT2: 83% (83/100) - JSON: 76% (76/100) SMT2 better accuracy (+7%) and higher success rate (100% vs 89%). StrategyQA \u00b6 Multi-hop reasoning requiring implicit world knowledge. Example: Question: \"Would a vegetarian eat a burger made of plants?\" Answer: True (requires knowing: vegetarians avoid meat, plant burgers have no meat) Performance: - SMT2: 84% (84/100) - JSON: 68% (68/100) Largest gap (+16% for SMT2). Both achieve 100%/86% success rates respectively. Analysis \u00b6 Accuracy Summary \u00b6 SMT2: 86.8% average across datasets JSON: 82.8% average across datasets SMT2 superior on 4/5 datasets (FOLIO exception where JSON +7%). Success Rate Summary \u00b6 SMT2: 99.4% average (range: 98.96-100%) JSON: 92.8% average (range: 86-100%) SMT2 more reliable program generation and execution. JSON success rate variance higher, indicating LLM generation issues on some datasets. Failure Modes \u00b6 SMT2 failures: - JSON extraction from markdown: regex mismatch - Z3 subprocess timeout (rare with 10s limit) - Invalid SMT-LIB syntax (caught by Z3 parser) JSON failures: - JSON parsing errors post-extraction - Invalid sort references (e.g., undefined Person sort) - Expression evaluation errors in ExpressionParser.parse_expression() - Z3 Python API exceptions Reproducing Results \u00b6 Full benchmark suite \u00b6 python experiments_pipeline.py Generates: - results/benchmark_results.json - Raw metrics - results/benchmark_results.md - Markdown table - Updates README.md between <!-- BENCHMARK_RESULTS_START/END --> markers Single benchmark \u00b6 python benchmark/bench_strategyqa.py Modify BACKEND variable in script ( smt2 or json ). Custom evaluation \u00b6 from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought , EvaluationPipeline config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"Precision: { result . metrics . precision : .4f } \" ) print ( f \"Recall: { result . metrics . recall : .4f } \" ) print ( f \"F1: { result . metrics . f1_score : .4f } \" ) Dataset Sources \u00b6 ProntoQA: data/prontoqa_test.json FOLIO: data/folio_test.json ProofWriter: data/proof_writer_test.json ConditionalQA: data/conditionalQA_test.json StrategyQA: data/strategyQA_train.json Format: JSON arrays with question and answer fields (boolean). Implementation Notes \u00b6 Parallel Processing: Benchmark scripts use num_workers=10 with ThreadPoolExecutor (not ProcessPoolExecutor due to ProofOfThought unpicklability). Caching: skip_existing=True enables resumption. Results cached as: - output/{backend}_evaluation_{dataset}/{sample_id}_result.json - output/{backend}_programs_{dataset}/{sample_id}_program.{ext} Timeout Handling: experiments_pipeline.py sets 1-hour subprocess timeout per benchmark. Individual Z3 calls timeout at 10s (verify) or 100s (optimize).","title":"Benchmarks"},{"location":"benchmarks/#benchmarks","text":"Evaluation on 5 logical reasoning datasets using Azure GPT-5.","title":"Benchmarks"},{"location":"benchmarks/#methodology","text":"Model: Azure GPT-5 deployment Configuration: - max_attempts=3 (retry with error feedback) - verify_timeout=10000ms - optimize_timeout=100000ms (JSON backend only) - num_workers=10 (ThreadPoolExecutor for parallel processing) Metrics: sklearn.metrics - Accuracy: accuracy_score(y_true, y_pred) - Precision: precision_score(y_true, y_pred, zero_division=0) - Recall: recall_score(y_true, y_pred, zero_division=0) - F1: 2 * (precision * recall) / (precision + recall) - Success Rate: (total - failed) / total Execution: experiments_pipeline.py runs all benchmarks sequentially, modifying BACKEND variable in each benchmark/bench_*.py script via regex substitution.","title":"Methodology"},{"location":"benchmarks/#results","text":"Last Updated: 2025-10-16 18:14:07 Benchmark Backend Samples Accuracy Precision Recall F1 Score Success Rate ProntoQA SMT2 100 100.00% 1.0000 1.0000 1.0000 100.00% FOLIO SMT2 100 69.00% 0.6949 0.7736 0.7321 99.00% ProofWriter SMT2 96 98.96% 1.0000 1.0000 1.0000 98.96% ConditionalQA SMT2 100 83.00% 0.9375 0.8219 0.8759 100.00% StrategyQA SMT2 100 84.00% 0.8205 0.7805 0.8000 100.00% ProntoQA JSON 100 99.00% 1.0000 0.9815 0.9907 100.00% FOLIO JSON 100 76.00% 0.7619 0.9412 0.8421 94.00% ProofWriter JSON 96 95.83% 1.0000 1.0000 1.0000 95.83% ConditionalQA JSON 100 76.00% 0.9180 0.8750 0.8960 89.00% StrategyQA JSON 100 68.00% 0.7500 0.7895 0.7692 86.00%","title":"Results"},{"location":"benchmarks/#dataset-characteristics","text":"","title":"Dataset Characteristics"},{"location":"benchmarks/#prontoqa","text":"Synthetic first-order logic with deterministic inference. Example: Facts: \"Stella is a lion. All lions are brown.\" Question: \"Is Stella brown?\" Answer: True Performance: - SMT2: 100% (100/100) - JSON: 99% (99/100) Both backends near-perfect. Simplest dataset.","title":"ProntoQA"},{"location":"benchmarks/#folio","text":"First-order logic from Wikipedia articles. Characteristics: Complex nested quantifiers, longer inference chains. Performance: - SMT2: 69% (69/100) - JSON: 76% (76/100) JSON outperforms SMT2 (+7%). Most challenging dataset. Lower success rate for JSON (94% vs 99%) indicates generation difficulties.","title":"FOLIO"},{"location":"benchmarks/#proofwriter","text":"Deductive reasoning over explicit facts and rules. Example: Facts: \"The bear is red. If something is red, then it is kind.\" Question: \"Is the bear kind?\" Answer: True Performance: - SMT2: 98.96% (95/96) - JSON: 95.83% (92/96) High accuracy for both. SMT2 slight edge (+3%).","title":"ProofWriter"},{"location":"benchmarks/#conditionalqa","text":"Conditional reasoning with if-then statements. Performance: - SMT2: 83% (83/100) - JSON: 76% (76/100) SMT2 better accuracy (+7%) and higher success rate (100% vs 89%).","title":"ConditionalQA"},{"location":"benchmarks/#strategyqa","text":"Multi-hop reasoning requiring implicit world knowledge. Example: Question: \"Would a vegetarian eat a burger made of plants?\" Answer: True (requires knowing: vegetarians avoid meat, plant burgers have no meat) Performance: - SMT2: 84% (84/100) - JSON: 68% (68/100) Largest gap (+16% for SMT2). Both achieve 100%/86% success rates respectively.","title":"StrategyQA"},{"location":"benchmarks/#analysis","text":"","title":"Analysis"},{"location":"benchmarks/#accuracy-summary","text":"SMT2: 86.8% average across datasets JSON: 82.8% average across datasets SMT2 superior on 4/5 datasets (FOLIO exception where JSON +7%).","title":"Accuracy Summary"},{"location":"benchmarks/#success-rate-summary","text":"SMT2: 99.4% average (range: 98.96-100%) JSON: 92.8% average (range: 86-100%) SMT2 more reliable program generation and execution. JSON success rate variance higher, indicating LLM generation issues on some datasets.","title":"Success Rate Summary"},{"location":"benchmarks/#failure-modes","text":"SMT2 failures: - JSON extraction from markdown: regex mismatch - Z3 subprocess timeout (rare with 10s limit) - Invalid SMT-LIB syntax (caught by Z3 parser) JSON failures: - JSON parsing errors post-extraction - Invalid sort references (e.g., undefined Person sort) - Expression evaluation errors in ExpressionParser.parse_expression() - Z3 Python API exceptions","title":"Failure Modes"},{"location":"benchmarks/#reproducing-results","text":"","title":"Reproducing Results"},{"location":"benchmarks/#full-benchmark-suite","text":"python experiments_pipeline.py Generates: - results/benchmark_results.json - Raw metrics - results/benchmark_results.md - Markdown table - Updates README.md between <!-- BENCHMARK_RESULTS_START/END --> markers","title":"Full benchmark suite"},{"location":"benchmarks/#single-benchmark","text":"python benchmark/bench_strategyqa.py Modify BACKEND variable in script ( smt2 or json ).","title":"Single benchmark"},{"location":"benchmarks/#custom-evaluation","text":"from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought , EvaluationPipeline config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"Precision: { result . metrics . precision : .4f } \" ) print ( f \"Recall: { result . metrics . recall : .4f } \" ) print ( f \"F1: { result . metrics . f1_score : .4f } \" )","title":"Custom evaluation"},{"location":"benchmarks/#dataset-sources","text":"ProntoQA: data/prontoqa_test.json FOLIO: data/folio_test.json ProofWriter: data/proof_writer_test.json ConditionalQA: data/conditionalQA_test.json StrategyQA: data/strategyQA_train.json Format: JSON arrays with question and answer fields (boolean).","title":"Dataset Sources"},{"location":"benchmarks/#implementation-notes","text":"Parallel Processing: Benchmark scripts use num_workers=10 with ThreadPoolExecutor (not ProcessPoolExecutor due to ProofOfThought unpicklability). Caching: skip_existing=True enables resumption. Results cached as: - output/{backend}_evaluation_{dataset}/{sample_id}_result.json - output/{backend}_programs_{dataset}/{sample_id}_program.{ext} Timeout Handling: experiments_pipeline.py sets 1-hour subprocess timeout per benchmark. Individual Z3 calls timeout at 10s (verify) or 100s (optimize).","title":"Implementation Notes"},{"location":"dsl-specification/","text":"DSL Specification \u00b6 Technical details of the JSON DSL implementation. Rules vs Verifications \u00b6 Critical distinction: Rules modify the solver state. Verifications query the solver state. Rules \u00b6 Implementation: z3adapter/dsl/expressions.py:159-204 Operation: solver.add(assertion) Rules are permanently asserted into the solver's knowledge base during step 6 of interpretation. # Line 189: Implication rule solver . add ( ForAll ( variables , Implies ( antecedent , consequent ))) # Line 194-196: Constraint rule if variables : solver . add ( ForAll ( variables , constraint )) else : solver . add ( constraint ) Structure: { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } Effect: Defines axioms. Every subsequent verification will inherit this constraint. Verifications \u00b6 Implementation: z3adapter/verification/verifier.py:84-127 Operation: solver.check(condition) Verifications test conditions against the existing knowledge base without modifying it. # Line 113 result = solver . check ( condition ) # Temporary hypothesis check if result == sat : self . sat_count += 1 elif result == unsat : self . unsat_count += 1 Semantics: Checks if KB \u2227 condition is satisfiable. SAT : condition is consistent with KB UNSAT : condition contradicts KB Structure: { \"name\" : \"test_pelosi\" , \"constraint\" : \"publicly_denounce(nancy, abortion)\" } Effect: Returns SAT/UNSAT but does NOT add to KB. Example \u00b6 { \"rules\" : [ { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } ], \"knowledge_base\" : [ \"is_democrat(nancy)\" ], \"verifications\" : [ { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } ] } Execution: 1. solver.add(ForAll([p], Implies(is_democrat(p), supports_abortion(p)))) \u2014 rule 2. solver.add(is_democrat(nancy)) \u2014 knowledge base 3. solver.check(supports_abortion(nancy)) \u2014 verification \u2192 SAT Variable Scoping \u00b6 Implementation: z3adapter/dsl/expressions.py:69-107 Free Variables \u00b6 Declared in global \"variables\" section. Available throughout program. \"variables\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }] Added to evaluation context (line 91): context . update ( self . variables ) Quantified Variables \u00b6 Bound by ForAll or Exists quantifiers. Temporarily shadow free variables within quantified scope. \"knowledge_base\" : [ \"ForAll([x], x > 0)\" ] Here x must exist in context (from \"variables\" ) to be bound by ForAll . Shadowing \u00b6 Code checks for shadowing (lines 100-106): for v in quantified_vars : var_name = v . decl () . name () if var_name in context and var_name not in [ ... ]: logger . warning ( f \"Quantified variable ' { var_name } ' shadows existing symbol\" ) context [ var_name ] = v Variables bound by quantifiers override free variables in local scope. Answer Determination \u00b6 Implementation: z3adapter/backends/abstract.py:52-67 def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None # Ambiguous Ambiguous results ( None ) occur when: - sat_count > 0 and unsat_count > 0 \u2014 multiple verifications with conflicting results - sat_count == 0 and unsat_count == 0 \u2014 no verifications or all unknown Handling: proof_of_thought.py:183-191 treats None as error and retries with feedback: if verify_result . answer is None : error_trace = ( f \"Ambiguous verification result: \" f \"SAT= { verify_result . sat_count } , UNSAT= { verify_result . unsat_count } \" ) continue # Retry with error feedback Best practice: Single verification per program (enforced by prompt template line 416). Security Model \u00b6 Implementation: z3adapter/security/validator.py AST Validation \u00b6 Before eval() , parses to AST and checks for dangerous constructs (lines 21-42): Blocked: - Dunder attributes: __import__ , __class__ , etc. (line 24) - Imports: import , from ... import (line 29) - Function/class definitions (line 32) - Builtin abuse: eval , exec , compile , __import__ (line 36-42) Restricted Evaluation \u00b6 # Line 66 eval ( code , { \"__builtins__\" : {}}, { ** safe_globals , ** context }) No builtins : __builtins__: {} prevents access to open , print , etc. Whitelisted globals : Only Z3 operators and user-defined functions Local context : Constants, variables, quantified vars Whitelisted operators ( expressions.py:33-47 ): Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" } Sort Dependency Resolution \u00b6 Implementation: z3adapter/dsl/sorts.py:36-97 Uses Kahn's algorithm for topological sorting. Dependency Extraction \u00b6 ArraySort creates dependencies (lines 59-62): if sort_type . startswith ( \"ArraySort(\" ): domain_range = sort_type [ len ( \"ArraySort(\" ) : - 1 ] parts = [ s . strip () for s in domain_range . split ( \",\" )] deps . extend ( parts ) Example: { \"name\" : \"MyArray\" , \"type\" : \"ArraySort(IntSort, Person)\" } Depends on: IntSort (built-in, skip), Person (must be defined first). Topological Sort \u00b6 Kahn's algorithm (lines 66-87): 1. Calculate in-degree (dependency count) for each sort 2. Process sorts with zero dependencies first 3. Reduce in-degree of dependents 4. Detect cycles if not all sorts processed (lines 90-92) Circular dependency detection: if len ( sorted_names ) != len ( dependencies ): remaining = set ( dependencies . keys ()) - set ( sorted_names ) raise ValueError ( f \"Circular dependency detected in sorts: { remaining } \" ) Optimizer Independence \u00b6 Implementation: z3adapter/optimization/optimizer.py:29-39 def __init__ ( self , ... ): self . optimizer = Optimize () # Separate instance Critical: Optimize() is separate from Solver() . Does NOT share constraints. From docstring (line 38-39): The optimizer is separate from the solver and doesn't share constraints. This is intentional to allow independent optimization problems. Optimizer has its own variables and constraints (lines 49-69). Can reference global constants via extended context (line 60-61): base_context = self . expression_parser . build_context () opt_context = { ** base_context , ** optimization_vars } Execution Pipeline \u00b6 Implementation: z3adapter/interpreter.py:135-197 8-step execution sequence: # Step 1: Create sorts self . sort_manager . create_sorts ( self . config [ \"sorts\" ]) # Step 2: Create functions functions = self . sort_manager . create_functions ( self . config [ \"functions\" ]) # Step 3: Create constants self . sort_manager . create_constants ( self . config [ \"constants\" ]) # Step 4: Create variables variables = self . sort_manager . create_variables ( self . config . get ( \"variables\" , [])) # Step 5: Initialize expression parser self . expression_parser = ExpressionParser ( functions , constants , variables ) self . expression_parser . mark_symbols_loaded () # Enable context caching # Step 6: Add knowledge base self . expression_parser . add_knowledge_base ( self . solver , self . config [ \"knowledge_base\" ]) # Step 7: Add rules self . expression_parser . add_rules ( self . solver , self . config [ \"rules\" ], sorts ) # Step 8: Initialize verifier and add verifications self . verifier = Verifier ( self . expression_parser , sorts ) self . verifier . add_verifications ( self . config [ \"verifications\" ]) # Step 9: Perform actions (e.g., \"verify_conditions\") self . perform_actions () Symbol loading: Line 172 calls mark_symbols_loaded() to enable context caching (lines 78-84 in expressions.py ). After this, build_context() returns cached dict instead of rebuilding. Retry Mechanism \u00b6 Implementation: z3adapter/reasoning/proof_of_thought.py:123-191 Retry loop with error feedback: for attempt in range ( 1 , self . max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , ... ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , ... ) Failure modes triggering retry: Generation failure (lines 143-149): python if not gen_result.success or gen_result.program is None: error_trace = gen_result.error or \"Failed to generate program\" continue Execution failure (lines 176-180): python if not verify_result.success: error_trace = verify_result.error or \"Z3 verification failed\" continue Ambiguous result (lines 183-191): python if verify_result.answer is None: error_trace = f\"Ambiguous verification result: SAT={sat_count}, UNSAT={unsat_count}\" continue Error feedback: Multi-turn conversation ( program_generator.py:130-174 ): messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ] Solver Semantics \u00b6 Implementation: z3adapter/solvers/z3_solver.py:20-24 def check ( self , condition : Any = None ) -> Any : if condition is not None : return self . solver . check ( condition ) # Temporary hypothesis return self . solver . check () # Check all assertions Two modes: solver.check() : Checks satisfiability of all solver.add() assertions solver.check(\u03c6) : Checks satisfiability of assertions \u2227 \u03c6 without adding \u03c6 Verifications use mode 2 ( verifier.py:113 ): result = solver . check ( condition ) This is temporary \u2014 condition is NOT added to solver permanently. Contrast with rules: - Rules: solver.add(\u03c6) \u2192 permanent - Verifications: solver.check(\u03c6) \u2192 temporary test Built-in Sorts \u00b6 Implementation: z3adapter/dsl/sorts.py:31-34 Three built-in sorts pre-initialized: def _initialize_builtin_sorts ( self ) -> None : built_in_sorts = { \"BoolSort\" : BoolSort (), \"IntSort\" : IntSort (), \"RealSort\" : RealSort ()} self . sorts . update ( built_in_sorts ) Important: Reference as \"BoolSort\" , \"IntSort\" , \"RealSort\" in JSON (not \"Bool\" , \"Int\" , \"Real\" ). Do NOT declare in \"sorts\" section \u2014 already available. Prompt Template Constraints \u00b6 Implementation: z3adapter/reasoning/prompt_template.py Key constraints enforced by prompt (extracted from code): Line 228: Rules with \"implies\" MUST have non-empty \"forall\" field # expressions.py:184-186 if \"implies\" in rule : if not variables : raise ValueError ( \"Implication rules require quantified variables\" ) Line 298: Empty quantifier lists forbidden # verifier.py:42-43, 55-56 if not exists_vars : raise ValueError ( f \"Empty 'exists' list in verification\" ) Line 416: Single verification per program (avoid ambiguous results) # Directly impacts determine_answer() \u2014 mixed SAT/UNSAT returns None Line 531: Output format requirement - Must wrap JSON in markdown code block: ```json ... ``` - Regex extraction: r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" ( program_generator.py:224 )","title":"DSL Specification"},{"location":"dsl-specification/#dsl-specification","text":"Technical details of the JSON DSL implementation.","title":"DSL Specification"},{"location":"dsl-specification/#rules-vs-verifications","text":"Critical distinction: Rules modify the solver state. Verifications query the solver state.","title":"Rules vs Verifications"},{"location":"dsl-specification/#rules","text":"Implementation: z3adapter/dsl/expressions.py:159-204 Operation: solver.add(assertion) Rules are permanently asserted into the solver's knowledge base during step 6 of interpretation. # Line 189: Implication rule solver . add ( ForAll ( variables , Implies ( antecedent , consequent ))) # Line 194-196: Constraint rule if variables : solver . add ( ForAll ( variables , constraint )) else : solver . add ( constraint ) Structure: { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } Effect: Defines axioms. Every subsequent verification will inherit this constraint.","title":"Rules"},{"location":"dsl-specification/#verifications","text":"Implementation: z3adapter/verification/verifier.py:84-127 Operation: solver.check(condition) Verifications test conditions against the existing knowledge base without modifying it. # Line 113 result = solver . check ( condition ) # Temporary hypothesis check if result == sat : self . sat_count += 1 elif result == unsat : self . unsat_count += 1 Semantics: Checks if KB \u2227 condition is satisfiable. SAT : condition is consistent with KB UNSAT : condition contradicts KB Structure: { \"name\" : \"test_pelosi\" , \"constraint\" : \"publicly_denounce(nancy, abortion)\" } Effect: Returns SAT/UNSAT but does NOT add to KB.","title":"Verifications"},{"location":"dsl-specification/#example","text":"{ \"rules\" : [ { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } ], \"knowledge_base\" : [ \"is_democrat(nancy)\" ], \"verifications\" : [ { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } ] } Execution: 1. solver.add(ForAll([p], Implies(is_democrat(p), supports_abortion(p)))) \u2014 rule 2. solver.add(is_democrat(nancy)) \u2014 knowledge base 3. solver.check(supports_abortion(nancy)) \u2014 verification \u2192 SAT","title":"Example"},{"location":"dsl-specification/#variable-scoping","text":"Implementation: z3adapter/dsl/expressions.py:69-107","title":"Variable Scoping"},{"location":"dsl-specification/#free-variables","text":"Declared in global \"variables\" section. Available throughout program. \"variables\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }] Added to evaluation context (line 91): context . update ( self . variables )","title":"Free Variables"},{"location":"dsl-specification/#quantified-variables","text":"Bound by ForAll or Exists quantifiers. Temporarily shadow free variables within quantified scope. \"knowledge_base\" : [ \"ForAll([x], x > 0)\" ] Here x must exist in context (from \"variables\" ) to be bound by ForAll .","title":"Quantified Variables"},{"location":"dsl-specification/#shadowing","text":"Code checks for shadowing (lines 100-106): for v in quantified_vars : var_name = v . decl () . name () if var_name in context and var_name not in [ ... ]: logger . warning ( f \"Quantified variable ' { var_name } ' shadows existing symbol\" ) context [ var_name ] = v Variables bound by quantifiers override free variables in local scope.","title":"Shadowing"},{"location":"dsl-specification/#answer-determination","text":"Implementation: z3adapter/backends/abstract.py:52-67 def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None # Ambiguous Ambiguous results ( None ) occur when: - sat_count > 0 and unsat_count > 0 \u2014 multiple verifications with conflicting results - sat_count == 0 and unsat_count == 0 \u2014 no verifications or all unknown Handling: proof_of_thought.py:183-191 treats None as error and retries with feedback: if verify_result . answer is None : error_trace = ( f \"Ambiguous verification result: \" f \"SAT= { verify_result . sat_count } , UNSAT= { verify_result . unsat_count } \" ) continue # Retry with error feedback Best practice: Single verification per program (enforced by prompt template line 416).","title":"Answer Determination"},{"location":"dsl-specification/#security-model","text":"Implementation: z3adapter/security/validator.py","title":"Security Model"},{"location":"dsl-specification/#ast-validation","text":"Before eval() , parses to AST and checks for dangerous constructs (lines 21-42): Blocked: - Dunder attributes: __import__ , __class__ , etc. (line 24) - Imports: import , from ... import (line 29) - Function/class definitions (line 32) - Builtin abuse: eval , exec , compile , __import__ (line 36-42)","title":"AST Validation"},{"location":"dsl-specification/#restricted-evaluation","text":"# Line 66 eval ( code , { \"__builtins__\" : {}}, { ** safe_globals , ** context }) No builtins : __builtins__: {} prevents access to open , print , etc. Whitelisted globals : Only Z3 operators and user-defined functions Local context : Constants, variables, quantified vars Whitelisted operators ( expressions.py:33-47 ): Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" }","title":"Restricted Evaluation"},{"location":"dsl-specification/#sort-dependency-resolution","text":"Implementation: z3adapter/dsl/sorts.py:36-97 Uses Kahn's algorithm for topological sorting.","title":"Sort Dependency Resolution"},{"location":"dsl-specification/#dependency-extraction","text":"ArraySort creates dependencies (lines 59-62): if sort_type . startswith ( \"ArraySort(\" ): domain_range = sort_type [ len ( \"ArraySort(\" ) : - 1 ] parts = [ s . strip () for s in domain_range . split ( \",\" )] deps . extend ( parts ) Example: { \"name\" : \"MyArray\" , \"type\" : \"ArraySort(IntSort, Person)\" } Depends on: IntSort (built-in, skip), Person (must be defined first).","title":"Dependency Extraction"},{"location":"dsl-specification/#topological-sort","text":"Kahn's algorithm (lines 66-87): 1. Calculate in-degree (dependency count) for each sort 2. Process sorts with zero dependencies first 3. Reduce in-degree of dependents 4. Detect cycles if not all sorts processed (lines 90-92) Circular dependency detection: if len ( sorted_names ) != len ( dependencies ): remaining = set ( dependencies . keys ()) - set ( sorted_names ) raise ValueError ( f \"Circular dependency detected in sorts: { remaining } \" )","title":"Topological Sort"},{"location":"dsl-specification/#optimizer-independence","text":"Implementation: z3adapter/optimization/optimizer.py:29-39 def __init__ ( self , ... ): self . optimizer = Optimize () # Separate instance Critical: Optimize() is separate from Solver() . Does NOT share constraints. From docstring (line 38-39): The optimizer is separate from the solver and doesn't share constraints. This is intentional to allow independent optimization problems. Optimizer has its own variables and constraints (lines 49-69). Can reference global constants via extended context (line 60-61): base_context = self . expression_parser . build_context () opt_context = { ** base_context , ** optimization_vars }","title":"Optimizer Independence"},{"location":"dsl-specification/#execution-pipeline","text":"Implementation: z3adapter/interpreter.py:135-197 8-step execution sequence: # Step 1: Create sorts self . sort_manager . create_sorts ( self . config [ \"sorts\" ]) # Step 2: Create functions functions = self . sort_manager . create_functions ( self . config [ \"functions\" ]) # Step 3: Create constants self . sort_manager . create_constants ( self . config [ \"constants\" ]) # Step 4: Create variables variables = self . sort_manager . create_variables ( self . config . get ( \"variables\" , [])) # Step 5: Initialize expression parser self . expression_parser = ExpressionParser ( functions , constants , variables ) self . expression_parser . mark_symbols_loaded () # Enable context caching # Step 6: Add knowledge base self . expression_parser . add_knowledge_base ( self . solver , self . config [ \"knowledge_base\" ]) # Step 7: Add rules self . expression_parser . add_rules ( self . solver , self . config [ \"rules\" ], sorts ) # Step 8: Initialize verifier and add verifications self . verifier = Verifier ( self . expression_parser , sorts ) self . verifier . add_verifications ( self . config [ \"verifications\" ]) # Step 9: Perform actions (e.g., \"verify_conditions\") self . perform_actions () Symbol loading: Line 172 calls mark_symbols_loaded() to enable context caching (lines 78-84 in expressions.py ). After this, build_context() returns cached dict instead of rebuilding.","title":"Execution Pipeline"},{"location":"dsl-specification/#retry-mechanism","text":"Implementation: z3adapter/reasoning/proof_of_thought.py:123-191 Retry loop with error feedback: for attempt in range ( 1 , self . max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , ... ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , ... ) Failure modes triggering retry: Generation failure (lines 143-149): python if not gen_result.success or gen_result.program is None: error_trace = gen_result.error or \"Failed to generate program\" continue Execution failure (lines 176-180): python if not verify_result.success: error_trace = verify_result.error or \"Z3 verification failed\" continue Ambiguous result (lines 183-191): python if verify_result.answer is None: error_trace = f\"Ambiguous verification result: SAT={sat_count}, UNSAT={unsat_count}\" continue Error feedback: Multi-turn conversation ( program_generator.py:130-174 ): messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ]","title":"Retry Mechanism"},{"location":"dsl-specification/#solver-semantics","text":"Implementation: z3adapter/solvers/z3_solver.py:20-24 def check ( self , condition : Any = None ) -> Any : if condition is not None : return self . solver . check ( condition ) # Temporary hypothesis return self . solver . check () # Check all assertions Two modes: solver.check() : Checks satisfiability of all solver.add() assertions solver.check(\u03c6) : Checks satisfiability of assertions \u2227 \u03c6 without adding \u03c6 Verifications use mode 2 ( verifier.py:113 ): result = solver . check ( condition ) This is temporary \u2014 condition is NOT added to solver permanently. Contrast with rules: - Rules: solver.add(\u03c6) \u2192 permanent - Verifications: solver.check(\u03c6) \u2192 temporary test","title":"Solver Semantics"},{"location":"dsl-specification/#built-in-sorts","text":"Implementation: z3adapter/dsl/sorts.py:31-34 Three built-in sorts pre-initialized: def _initialize_builtin_sorts ( self ) -> None : built_in_sorts = { \"BoolSort\" : BoolSort (), \"IntSort\" : IntSort (), \"RealSort\" : RealSort ()} self . sorts . update ( built_in_sorts ) Important: Reference as \"BoolSort\" , \"IntSort\" , \"RealSort\" in JSON (not \"Bool\" , \"Int\" , \"Real\" ). Do NOT declare in \"sorts\" section \u2014 already available.","title":"Built-in Sorts"},{"location":"dsl-specification/#prompt-template-constraints","text":"Implementation: z3adapter/reasoning/prompt_template.py Key constraints enforced by prompt (extracted from code): Line 228: Rules with \"implies\" MUST have non-empty \"forall\" field # expressions.py:184-186 if \"implies\" in rule : if not variables : raise ValueError ( \"Implication rules require quantified variables\" ) Line 298: Empty quantifier lists forbidden # verifier.py:42-43, 55-56 if not exists_vars : raise ValueError ( f \"Empty 'exists' list in verification\" ) Line 416: Single verification per program (avoid ambiguous results) # Directly impacts determine_answer() \u2014 mixed SAT/UNSAT returns None Line 531: Output format requirement - Must wrap JSON in markdown code block: ```json ... ``` - Regex extraction: r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" ( program_generator.py:224 )","title":"Prompt Template Constraints"},{"location":"examples/","text":"Examples \u00b6 All examples in examples/ . Run from project root: python examples/ { script } .py Basic Query \u00b6 examples/simple_usage.py from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = os . getenv ( \"OPENAI_API_KEY\" )) pot = ProofOfThought ( llm_client = client , model = \"gpt-4o\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) print ( result . answer ) # False Azure OpenAI \u00b6 examples/azure_simple_example.py from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ]) result = pot . query ( \"Can fish breathe underwater?\" ) print ( result . answer ) # True Backend Comparison \u00b6 examples/backend_comparison.py config = get_client_config () question = \"Can fish breathe underwater?\" pot_json = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"json\" ) pot_smt2 = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) result_json = pot_json . query ( question ) result_smt2 = pot_smt2 . query ( question ) print ( f \"JSON: { result_json . answer } \" ) print ( f \"SMT2: { result_smt2 . answer } \" ) Batch Evaluation \u00b6 examples/batch_evaluation.py from z3adapter.reasoning import EvaluationPipeline , ProofOfThought pot = ProofOfThought ( llm_client = client ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = \"results/\" ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , question_field = \"question\" , answer_field = \"answer\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"F1 Score: { result . metrics . f1_score : .4f } \" ) Azure + SMT2 Evaluation \u00b6 examples/batch_evaluation_smt2_azure.py config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( \"data/strategyQA_train.json\" , max_samples = 50 ) Full Benchmark Suite \u00b6 experiments_pipeline.py Runs all 5 benchmarks (ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA) with both backends: python experiments_pipeline.py Implementation: - Modifies benchmark/bench_*.py files to set backend via regex - Runs each script as subprocess with 1-hour timeout - Collects metrics from output/{backend}_evaluation_{benchmark}/ directories - Generates markdown table and updates README.md Configuration ( experiments_pipeline.py:29-41 ): BENCHMARKS = { \"prontoqa\" : \"benchmark/bench_prontoqa.py\" , \"folio\" : \"benchmark/bench_folio.py\" , \"proofwriter\" : \"benchmark/bench_proofwriter.py\" , \"conditionalqa\" : \"benchmark/bench_conditionalqa.py\" , \"strategyqa\" : \"benchmark/bench_strategyqa.py\" , } BACKENDS = [ \"smt2\" , \"json\" ] Benchmark Script Structure \u00b6 benchmark/bench_strategyqa.py (representative): config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = BACKEND , # Modified by experiments_pipeline.py max_attempts = 3 , cache_dir = f \"output/ { BACKEND } _programs_strategyqa\" , ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = f \"output/ { BACKEND } _evaluation_strategyqa\" , num_workers = 10 , # ThreadPoolExecutor for parallel processing ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , id_field = \"qid\" , max_samples = 100 , skip_existing = True , # Resume interrupted runs ) Dataset Format \u00b6 JSON array of objects: [ { \"question\" : \"Can fish breathe underwater?\" , \"answer\" : true }, { \"question\" : \"Do humans have wings?\" , \"answer\" : false } ] Optional ID field: { \"qid\" : \"sample_123\" , \"question\" : \"...\" , \"answer\" : true } Custom field names via question_field , answer_field , id_field parameters. Saving Programs \u00b6 result = pot . query ( \"Can fish breathe underwater?\" , save_program = True , program_path = \"output/my_program.smt2\" ) Default path: {cache_dir}/{auto_generated}{ext} Advanced Configuration \u00b6 pot = ProofOfThought ( llm_client = client , model = \"gpt-5\" , backend = \"smt2\" , max_attempts = 5 , # More retries verify_timeout = 20000 , # 20s timeout z3_path = \"/custom/z3\" # Custom Z3 binary )","title":"Examples"},{"location":"examples/#examples","text":"All examples in examples/ . Run from project root: python examples/ { script } .py","title":"Examples"},{"location":"examples/#basic-query","text":"examples/simple_usage.py from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = os . getenv ( \"OPENAI_API_KEY\" )) pot = ProofOfThought ( llm_client = client , model = \"gpt-4o\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) print ( result . answer ) # False","title":"Basic Query"},{"location":"examples/#azure-openai","text":"examples/azure_simple_example.py from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ]) result = pot . query ( \"Can fish breathe underwater?\" ) print ( result . answer ) # True","title":"Azure OpenAI"},{"location":"examples/#backend-comparison","text":"examples/backend_comparison.py config = get_client_config () question = \"Can fish breathe underwater?\" pot_json = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"json\" ) pot_smt2 = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) result_json = pot_json . query ( question ) result_smt2 = pot_smt2 . query ( question ) print ( f \"JSON: { result_json . answer } \" ) print ( f \"SMT2: { result_smt2 . answer } \" )","title":"Backend Comparison"},{"location":"examples/#batch-evaluation","text":"examples/batch_evaluation.py from z3adapter.reasoning import EvaluationPipeline , ProofOfThought pot = ProofOfThought ( llm_client = client ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = \"results/\" ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , question_field = \"question\" , answer_field = \"answer\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"F1 Score: { result . metrics . f1_score : .4f } \" )","title":"Batch Evaluation"},{"location":"examples/#azure-smt2-evaluation","text":"examples/batch_evaluation_smt2_azure.py config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( \"data/strategyQA_train.json\" , max_samples = 50 )","title":"Azure + SMT2 Evaluation"},{"location":"examples/#full-benchmark-suite","text":"experiments_pipeline.py Runs all 5 benchmarks (ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA) with both backends: python experiments_pipeline.py Implementation: - Modifies benchmark/bench_*.py files to set backend via regex - Runs each script as subprocess with 1-hour timeout - Collects metrics from output/{backend}_evaluation_{benchmark}/ directories - Generates markdown table and updates README.md Configuration ( experiments_pipeline.py:29-41 ): BENCHMARKS = { \"prontoqa\" : \"benchmark/bench_prontoqa.py\" , \"folio\" : \"benchmark/bench_folio.py\" , \"proofwriter\" : \"benchmark/bench_proofwriter.py\" , \"conditionalqa\" : \"benchmark/bench_conditionalqa.py\" , \"strategyqa\" : \"benchmark/bench_strategyqa.py\" , } BACKENDS = [ \"smt2\" , \"json\" ]","title":"Full Benchmark Suite"},{"location":"examples/#benchmark-script-structure","text":"benchmark/bench_strategyqa.py (representative): config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = BACKEND , # Modified by experiments_pipeline.py max_attempts = 3 , cache_dir = f \"output/ { BACKEND } _programs_strategyqa\" , ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = f \"output/ { BACKEND } _evaluation_strategyqa\" , num_workers = 10 , # ThreadPoolExecutor for parallel processing ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , id_field = \"qid\" , max_samples = 100 , skip_existing = True , # Resume interrupted runs )","title":"Benchmark Script Structure"},{"location":"examples/#dataset-format","text":"JSON array of objects: [ { \"question\" : \"Can fish breathe underwater?\" , \"answer\" : true }, { \"question\" : \"Do humans have wings?\" , \"answer\" : false } ] Optional ID field: { \"qid\" : \"sample_123\" , \"question\" : \"...\" , \"answer\" : true } Custom field names via question_field , answer_field , id_field parameters.","title":"Dataset Format"},{"location":"examples/#saving-programs","text":"result = pot . query ( \"Can fish breathe underwater?\" , save_program = True , program_path = \"output/my_program.smt2\" ) Default path: {cache_dir}/{auto_generated}{ext}","title":"Saving Programs"},{"location":"examples/#advanced-configuration","text":"pot = ProofOfThought ( llm_client = client , model = \"gpt-5\" , backend = \"smt2\" , max_attempts = 5 , # More retries verify_timeout = 20000 , # 20s timeout z3_path = \"/custom/z3\" # Custom Z3 binary )","title":"Advanced Configuration"},{"location":"installation/","text":"Installation \u00b6 Dependencies \u00b6 Python 3.12+ required ( pyproject.toml specifies requires-python = \">=3.12\" ). Core \u00b6 pip install -r requirements.txt Installs: z3-solver>=4.15.0 - Z3 Python API (JSONBackend) + CLI binary (SMT2Backend) openai>=2.0.0 - LLM client (supports Azure OpenAI via same interface) scikit-learn>=1.7.0 - Evaluation metrics ( confusion_matrix , accuracy_score , etc.) numpy>=2.3.0 - Numerical operations python-dotenv>=1.1.0 - Environment variable management Development (Optional) \u00b6 pip install -e \".[dev]\" Additional tools: black>=25.9.0 - Code formatter ruff>=0.13.0 - Linter mypy>=1.18.0 - Type checker pytest>=8.0.0 - Test runner pre-commit>=4.3.0 - Git hooks Z3 Verification \u00b6 JSON Backend \u00b6 No additional setup. z3-solver package includes Python API. SMT2 Backend \u00b6 Requires Z3 CLI in PATH: z3 --version If missing, z3-solver package includes CLI in site-packages . Locate via: python -c \"import z3; print(z3.__file__)\" # CLI typically at: .../site-packages/z3/bin/z3 macOS/Linux: Add to PATH or specify in code: ProofOfThought ( ... , z3_path = \"/path/to/z3\" ) API Keys \u00b6 OpenAI \u00b6 .env : OPENAI_API_KEY = sk-... Azure OpenAI \u00b6 .env : AZURE_OPENAI_API_KEY = ... AZURE_OPENAI_ENDPOINT = https://....openai.azure.com/ AZURE_OPENAI_API_VERSION = 2024 -08-01-preview AZURE_GPT5_DEPLOYMENT_NAME = gpt-5 AZURE_GPT4O_DEPLOYMENT_NAME = gpt-4o Usage: from utils.azure_config import get_client_config config = get_client_config () # Returns {\"llm_client\": AzureOpenAI(...), \"model\": str} pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ]) Verification \u00b6 python examples/simple_usage.py Expected output structure: Question: Would Nancy Pelosi publicly denounce abortion? Answer: False Success: True Attempts: 1 Troubleshooting \u00b6 Z3 CLI not found (SMT2 backend) Error: FileNotFoundError: Z3 executable not found: 'z3' Solutions: 1. Use JSON backend: ProofOfThought(backend=\"json\") 2. Specify Z3 path: ProofOfThought(z3_path=\"/path/to/z3\") 3. Add to PATH: export PATH=$PATH:/path/to/z3/bin Import errors when running examples Wrong: cd examples python simple_usage.py # \u274c ModuleNotFoundError Correct: cd /path/to/proofofthought python examples/simple_usage.py # \u2713 Reason: examples/*.py use sys.path.insert(0, str(Path(__file__).parent.parent)) to find z3adapter and utils modules from project root. Azure authentication errors Verify .env variables are set and endpoint URL is correct. Test via: from utils.azure_config import get_client_config config = get_client_config () # Should not raise Version Constraints \u00b6 From pyproject.toml and requirements.txt : Python: >=3.12 Z3: >=4.15.0 (tested with 4.15.3.0 ) OpenAI: >=2.0.0 (tested with 2.0.1 ) scikit-learn: >=1.7.0 (tested with 1.7.2 ) NumPy: >=2.3.0 (tested with 2.3.3 )","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#dependencies","text":"Python 3.12+ required ( pyproject.toml specifies requires-python = \">=3.12\" ).","title":"Dependencies"},{"location":"installation/#core","text":"pip install -r requirements.txt Installs: z3-solver>=4.15.0 - Z3 Python API (JSONBackend) + CLI binary (SMT2Backend) openai>=2.0.0 - LLM client (supports Azure OpenAI via same interface) scikit-learn>=1.7.0 - Evaluation metrics ( confusion_matrix , accuracy_score , etc.) numpy>=2.3.0 - Numerical operations python-dotenv>=1.1.0 - Environment variable management","title":"Core"},{"location":"installation/#development-optional","text":"pip install -e \".[dev]\" Additional tools: black>=25.9.0 - Code formatter ruff>=0.13.0 - Linter mypy>=1.18.0 - Type checker pytest>=8.0.0 - Test runner pre-commit>=4.3.0 - Git hooks","title":"Development (Optional)"},{"location":"installation/#z3-verification","text":"","title":"Z3 Verification"},{"location":"installation/#json-backend","text":"No additional setup. z3-solver package includes Python API.","title":"JSON Backend"},{"location":"installation/#smt2-backend","text":"Requires Z3 CLI in PATH: z3 --version If missing, z3-solver package includes CLI in site-packages . Locate via: python -c \"import z3; print(z3.__file__)\" # CLI typically at: .../site-packages/z3/bin/z3 macOS/Linux: Add to PATH or specify in code: ProofOfThought ( ... , z3_path = \"/path/to/z3\" )","title":"SMT2 Backend"},{"location":"installation/#api-keys","text":"","title":"API Keys"},{"location":"installation/#openai","text":".env : OPENAI_API_KEY = sk-...","title":"OpenAI"},{"location":"installation/#azure-openai","text":".env : AZURE_OPENAI_API_KEY = ... AZURE_OPENAI_ENDPOINT = https://....openai.azure.com/ AZURE_OPENAI_API_VERSION = 2024 -08-01-preview AZURE_GPT5_DEPLOYMENT_NAME = gpt-5 AZURE_GPT4O_DEPLOYMENT_NAME = gpt-4o Usage: from utils.azure_config import get_client_config config = get_client_config () # Returns {\"llm_client\": AzureOpenAI(...), \"model\": str} pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ])","title":"Azure OpenAI"},{"location":"installation/#verification","text":"python examples/simple_usage.py Expected output structure: Question: Would Nancy Pelosi publicly denounce abortion? Answer: False Success: True Attempts: 1","title":"Verification"},{"location":"installation/#troubleshooting","text":"Z3 CLI not found (SMT2 backend) Error: FileNotFoundError: Z3 executable not found: 'z3' Solutions: 1. Use JSON backend: ProofOfThought(backend=\"json\") 2. Specify Z3 path: ProofOfThought(z3_path=\"/path/to/z3\") 3. Add to PATH: export PATH=$PATH:/path/to/z3/bin Import errors when running examples Wrong: cd examples python simple_usage.py # \u274c ModuleNotFoundError Correct: cd /path/to/proofofthought python examples/simple_usage.py # \u2713 Reason: examples/*.py use sys.path.insert(0, str(Path(__file__).parent.parent)) to find z3adapter and utils modules from project root. Azure authentication errors Verify .env variables are set and endpoint URL is correct. Test via: from utils.azure_config import get_client_config config = get_client_config () # Should not raise","title":"Troubleshooting"},{"location":"installation/#version-constraints","text":"From pyproject.toml and requirements.txt : Python: >=3.12 Z3: >=4.15.0 (tested with 4.15.3.0 ) OpenAI: >=2.0.0 (tested with 2.0.1 ) scikit-learn: >=1.7.0 (tested with 1.7.2 ) NumPy: >=2.3.0 (tested with 2.3.3 )","title":"Version Constraints"}]}