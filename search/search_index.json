{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ProofOfThought \u00b6 ProofOfThought provides LLM-guided translation of natural language questions into formal logic, which is then verified using the Z3 theorem prover. Architecture \u00b6 The system follows a multi-stage pipeline to transform questions into verifiable answers: Question (NL) \u2193 LLM Translation (few-shot prompting) \u2193 Formal Program (SMT-LIB 2.0 or JSON DSL) \u2193 Z3 Execution \u2193 SAT/UNSAT \u2192 Boolean Answer Components \u00b6 The architecture consists of several key components that work together: Z3ProgramGenerator ( z3adapter.reasoning.program_generator ) Provides the LLM interface for program generation. It extracts formal programs from markdown code blocks using regex and supports error feedback through multi-turn conversations. Backend ( z3adapter.backends.abstract ) Defines an abstract interface with execute(program_path) \u2192 VerificationResult . Two concrete implementations are available: SMT2Backend : Subprocess call to Z3 CLI. Parses stdout/stderr for sat / unsat via regex (?<!un)\\bsat\\b and \\bunsat\\b . JSONBackend : Python API execution via Z3JSONInterpreter . Returns structured SAT/UNSAT counts. Z3JSONInterpreter ( z3adapter.interpreter ) Implements a multi-stage pipeline for processing the JSON DSL: SortManager : Performs topological sorting of type dependencies and creates Z3 sorts ExpressionParser : Evaluates expressions using eval() with restricted globals for security Verifier : Runs solver.check(condition) for each verification Finally returns SAT/UNSAT counts ProofOfThought ( z3adapter.reasoning.proof_of_thought ) Provides the high-level API with a retry loop (default max_attempts=3 ) and error feedback. Answer determination follows: SAT only \u2192 True , UNSAT only \u2192 False , both/neither \u2192 None . Quick Start \u00b6 from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = \"...\" ) pot = ProofOfThought ( llm_client = client , backend = \"smt2\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) # result.answer: False (UNSAT) Benchmark Results \u00b6 ProofOfThought has been evaluated on multiple reasoning datasets using the following configuration: Datasets : ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA Model : GPT-5 (Azure deployment) Config : max_attempts=3 , verify_timeout=10000ms Backend Avg Accuracy Success Rate SMT2 86.8% 99.4% JSON 82.8% 92.8% The SMT2 backend outperforms JSON on 4 out of 5 datasets. For detailed results, see Benchmarks . Design Rationale \u00b6 Several key design decisions shape the architecture: Why use an external theorem prover? LLMs lack deductive closure, meaning they cannot guarantee sound logical inference. Z3 provides this soundness by formally verifying the logical reasoning. Why offer two backends? The choice trades off portability (SMT-LIB is a widely-supported standard) against LLM generation reliability (structured JSON is easier for models to produce correctly). Why use iterative refinement? Single-shot generation is often insufficient for complex reasoning. By incorporating error feedback, the system significantly improves its success rate. Implementation Notes \u00b6 Each backend has distinct implementation characteristics: SMT2 Backend: Runs Z3 as a subprocess with the -T:timeout flag Parses output using regex patterns on stdout/stderr Uses standard SMT-LIB 2.0 S-expressions JSON Backend: Leverages the Python Z3 API through the z3-solver package Evaluates expressions using restricted eval() with ExpressionValidator Supports built-in sorts: BoolSort , IntSort , RealSort Supports custom sorts: DeclareSort , EnumSort , BitVecSort , ArraySort Handles quantifiers: ForAll and Exists with proper variable binding Security: The JSON backend employs ExpressionValidator.safe_eval() with a whitelist of allowed Z3 operators, preventing arbitrary code execution. For more details, see Backends and API Reference .","title":"Home"},{"location":"#proofofthought","text":"ProofOfThought provides LLM-guided translation of natural language questions into formal logic, which is then verified using the Z3 theorem prover.","title":"ProofOfThought"},{"location":"#architecture","text":"The system follows a multi-stage pipeline to transform questions into verifiable answers: Question (NL) \u2193 LLM Translation (few-shot prompting) \u2193 Formal Program (SMT-LIB 2.0 or JSON DSL) \u2193 Z3 Execution \u2193 SAT/UNSAT \u2192 Boolean Answer","title":"Architecture"},{"location":"#components","text":"The architecture consists of several key components that work together: Z3ProgramGenerator ( z3adapter.reasoning.program_generator ) Provides the LLM interface for program generation. It extracts formal programs from markdown code blocks using regex and supports error feedback through multi-turn conversations. Backend ( z3adapter.backends.abstract ) Defines an abstract interface with execute(program_path) \u2192 VerificationResult . Two concrete implementations are available: SMT2Backend : Subprocess call to Z3 CLI. Parses stdout/stderr for sat / unsat via regex (?<!un)\\bsat\\b and \\bunsat\\b . JSONBackend : Python API execution via Z3JSONInterpreter . Returns structured SAT/UNSAT counts. Z3JSONInterpreter ( z3adapter.interpreter ) Implements a multi-stage pipeline for processing the JSON DSL: SortManager : Performs topological sorting of type dependencies and creates Z3 sorts ExpressionParser : Evaluates expressions using eval() with restricted globals for security Verifier : Runs solver.check(condition) for each verification Finally returns SAT/UNSAT counts ProofOfThought ( z3adapter.reasoning.proof_of_thought ) Provides the high-level API with a retry loop (default max_attempts=3 ) and error feedback. Answer determination follows: SAT only \u2192 True , UNSAT only \u2192 False , both/neither \u2192 None .","title":"Components"},{"location":"#quick-start","text":"from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = \"...\" ) pot = ProofOfThought ( llm_client = client , backend = \"smt2\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) # result.answer: False (UNSAT)","title":"Quick Start"},{"location":"#benchmark-results","text":"ProofOfThought has been evaluated on multiple reasoning datasets using the following configuration: Datasets : ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA Model : GPT-5 (Azure deployment) Config : max_attempts=3 , verify_timeout=10000ms Backend Avg Accuracy Success Rate SMT2 86.8% 99.4% JSON 82.8% 92.8% The SMT2 backend outperforms JSON on 4 out of 5 datasets. For detailed results, see Benchmarks .","title":"Benchmark Results"},{"location":"#design-rationale","text":"Several key design decisions shape the architecture: Why use an external theorem prover? LLMs lack deductive closure, meaning they cannot guarantee sound logical inference. Z3 provides this soundness by formally verifying the logical reasoning. Why offer two backends? The choice trades off portability (SMT-LIB is a widely-supported standard) against LLM generation reliability (structured JSON is easier for models to produce correctly). Why use iterative refinement? Single-shot generation is often insufficient for complex reasoning. By incorporating error feedback, the system significantly improves its success rate.","title":"Design Rationale"},{"location":"#implementation-notes","text":"Each backend has distinct implementation characteristics: SMT2 Backend: Runs Z3 as a subprocess with the -T:timeout flag Parses output using regex patterns on stdout/stderr Uses standard SMT-LIB 2.0 S-expressions JSON Backend: Leverages the Python Z3 API through the z3-solver package Evaluates expressions using restricted eval() with ExpressionValidator Supports built-in sorts: BoolSort , IntSort , RealSort Supports custom sorts: DeclareSort , EnumSort , BitVecSort , ArraySort Handles quantifiers: ForAll and Exists with proper variable binding Security: The JSON backend employs ExpressionValidator.safe_eval() with a whitelist of allowed Z3 operators, preventing arbitrary code execution. For more details, see Backends and API Reference .","title":"Implementation Notes"},{"location":"api-reference/","text":"API Reference \u00b6 This reference documents the public API for ProofOfThought. ProofOfThought \u00b6 The main entry point for the reasoning system. Location: z3adapter.reasoning.proof_of_thought.ProofOfThought Constructor \u00b6 def __init__ ( self , llm_client : Any , model : str = \"gpt-5\" , backend : Literal [ \"json\" , \"smt2\" ] = \"smt2\" , max_attempts : int = 3 , verify_timeout : int = 10000 , optimize_timeout : int = 100000 , cache_dir : str | None = None , z3_path : str = \"z3\" , ) -> None Parameters: llm_client : OpenAI/AzureOpenAI client instance model : Deployment/model name (default: \"gpt-5\" ) backend : \"json\" or \"smt2\" (default: \"smt2\" ) max_attempts : Retry limit for generation (default: 3 ) verify_timeout : Z3 timeout in milliseconds (default: 10000 ) optimize_timeout : Optimization timeout in ms, JSON only (default: 100000 ) cache_dir : Program cache directory (default: tempfile.gettempdir() ) z3_path : Z3 executable path for SMT2 (default: \"z3\" ) query() \u00b6 def query ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , save_program : bool = False , program_path : str | None = None , ) -> QueryResult Parameters: question : Natural language question temperature : LLM temperature (default: 0.1 , ignored for GPT-5 which only supports 1.0 ) max_tokens : Max completion tokens (default: 16384 ) save_program : Save generated program to disk (default: False ) program_path : Custom save path (default: auto-generated in cache_dir ) Returns: QueryResult Implementation details: The method implements a retry loop with error feedback: for attempt in range ( 1 , max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , temperature , max_tokens ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , temperature , max_tokens ) # ... execute and check result QueryResult \u00b6 Contains the results of a reasoning query. @dataclass class QueryResult : question : str # Input question answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) json_program : dict [ str , Any ] | None # Generated program if JSON backend sat_count : int # SAT occurrences in output unsat_count : int # UNSAT occurrences output : str # Raw Z3 output success : bool # Execution completed num_attempts : int # Generation attempts used error : str | None # Error message if failed EvaluationPipeline \u00b6 Facilitates batch evaluation of reasoning questions on datasets. Location: z3adapter.reasoning.evaluation.EvaluationPipeline Constructor \u00b6 def __init__ ( self , proof_of_thought : ProofOfThought , output_dir : str = \"evaluation_results\" , num_workers : int = 1 , ) -> None Parameters: proof_of_thought : Configured ProofOfThought instance output_dir : Results directory (default: \"evaluation_results\" ) num_workers : Parallel workers (default: 1 , uses ThreadPoolExecutor if > 1 ) evaluate() \u00b6 def evaluate ( self , dataset : list [ dict [ str , Any ]] | str , question_field : str = \"question\" , answer_field : str = \"answer\" , id_field : str | None = None , max_samples : int | None = None , skip_existing : bool = True , ) -> EvaluationResult Parameters: dataset : JSON file path or list of dicts question_field : Field name for question text (default: \"question\" ) answer_field : Field name for ground truth (default: \"answer\" ) id_field : Field for sample ID (default: None , auto-generates sample_{idx} ) max_samples : Limit samples (default: None , all) skip_existing : Skip cached results (default: True ) Returns: EvaluationResult Caching behavior: Results are cached by saving {sample_id}_result.json and {sample_id}_program{ext} files to output_dir . EvaluationMetrics \u00b6 Provides comprehensive metrics for evaluation results. @dataclass class EvaluationMetrics : accuracy : float # sklearn.metrics.accuracy_score precision : float # sklearn.metrics.precision_score (zero_division=0) recall : float # sklearn.metrics.recall_score (zero_division=0) f1_score : float # 2 * (P * R) / (P + R) specificity : float # TN / (TN + FP) false_positive_rate : float # FP / (FP + TN) false_negative_rate : float # FN / (FN + TP) tp : int # True positives fp : int # False positives tn : int # True negatives fn : int # False negatives total_samples : int # Correct + wrong + failed correct_answers : int # answer == ground_truth wrong_answers : int # answer != ground_truth failed_answers : int # success == False Metrics are computed using sklearn.metrics.confusion_matrix for binary classification. Backend \u00b6 Defines the abstract interface for execution backends. Location: z3adapter.backends.abstract.Backend Interface Methods \u00b6 class Backend ( ABC ): @abstractmethod def execute ( self , program_path : str ) -> VerificationResult : pass @abstractmethod def get_file_extension ( self ) -> str : pass @abstractmethod def get_prompt_template ( self ) -> str : pass def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None Concrete implementations are provided by SMT2Backend and JSONBackend . VerificationResult \u00b6 Encapsulates the results of Z3 verification execution. @dataclass class VerificationResult : answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) sat_count : int unsat_count : int output : str # Raw execution output success : bool # Execution completed without exception error : str | None # Error message if failed Z3ProgramGenerator \u00b6 Handles LLM-based program generation with error recovery. Location: z3adapter.reasoning.program_generator.Z3ProgramGenerator generate() \u00b6 def generate ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , ) -> GenerationResult LLM API Call: response = self . llm_client . chat . completions . create ( model = self . model , messages = [{ \"role\" : \"user\" , \"content\" : prompt }], max_completion_tokens = max_tokens , ) Note that the temperature parameter is not passed to the API due to GPT-5 constraints. generate_with_feedback() \u00b6 Enables multi-turn conversation with error feedback: messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ] Utility: Azure Config \u00b6 Provides convenient configuration for Azure OpenAI deployments. Location: utils.azure_config.get_client_config() Returns: { \"llm_client\" : AzureOpenAI ( ... ), \"model\" : str # Deployment name from env } Required environment variables: AZURE_OPENAI_API_KEY AZURE_OPENAI_ENDPOINT AZURE_OPENAI_API_VERSION AZURE_GPT5_DEPLOYMENT_NAME or AZURE_GPT4O_DEPLOYMENT_NAME","title":"API Reference"},{"location":"api-reference/#api-reference","text":"This reference documents the public API for ProofOfThought.","title":"API Reference"},{"location":"api-reference/#proofofthought","text":"The main entry point for the reasoning system. Location: z3adapter.reasoning.proof_of_thought.ProofOfThought","title":"ProofOfThought"},{"location":"api-reference/#constructor","text":"def __init__ ( self , llm_client : Any , model : str = \"gpt-5\" , backend : Literal [ \"json\" , \"smt2\" ] = \"smt2\" , max_attempts : int = 3 , verify_timeout : int = 10000 , optimize_timeout : int = 100000 , cache_dir : str | None = None , z3_path : str = \"z3\" , ) -> None Parameters: llm_client : OpenAI/AzureOpenAI client instance model : Deployment/model name (default: \"gpt-5\" ) backend : \"json\" or \"smt2\" (default: \"smt2\" ) max_attempts : Retry limit for generation (default: 3 ) verify_timeout : Z3 timeout in milliseconds (default: 10000 ) optimize_timeout : Optimization timeout in ms, JSON only (default: 100000 ) cache_dir : Program cache directory (default: tempfile.gettempdir() ) z3_path : Z3 executable path for SMT2 (default: \"z3\" )","title":"Constructor"},{"location":"api-reference/#query","text":"def query ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , save_program : bool = False , program_path : str | None = None , ) -> QueryResult Parameters: question : Natural language question temperature : LLM temperature (default: 0.1 , ignored for GPT-5 which only supports 1.0 ) max_tokens : Max completion tokens (default: 16384 ) save_program : Save generated program to disk (default: False ) program_path : Custom save path (default: auto-generated in cache_dir ) Returns: QueryResult Implementation details: The method implements a retry loop with error feedback: for attempt in range ( 1 , max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , temperature , max_tokens ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , temperature , max_tokens ) # ... execute and check result","title":"query()"},{"location":"api-reference/#queryresult","text":"Contains the results of a reasoning query. @dataclass class QueryResult : question : str # Input question answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) json_program : dict [ str , Any ] | None # Generated program if JSON backend sat_count : int # SAT occurrences in output unsat_count : int # UNSAT occurrences output : str # Raw Z3 output success : bool # Execution completed num_attempts : int # Generation attempts used error : str | None # Error message if failed","title":"QueryResult"},{"location":"api-reference/#evaluationpipeline","text":"Facilitates batch evaluation of reasoning questions on datasets. Location: z3adapter.reasoning.evaluation.EvaluationPipeline","title":"EvaluationPipeline"},{"location":"api-reference/#constructor_1","text":"def __init__ ( self , proof_of_thought : ProofOfThought , output_dir : str = \"evaluation_results\" , num_workers : int = 1 , ) -> None Parameters: proof_of_thought : Configured ProofOfThought instance output_dir : Results directory (default: \"evaluation_results\" ) num_workers : Parallel workers (default: 1 , uses ThreadPoolExecutor if > 1 )","title":"Constructor"},{"location":"api-reference/#evaluate","text":"def evaluate ( self , dataset : list [ dict [ str , Any ]] | str , question_field : str = \"question\" , answer_field : str = \"answer\" , id_field : str | None = None , max_samples : int | None = None , skip_existing : bool = True , ) -> EvaluationResult Parameters: dataset : JSON file path or list of dicts question_field : Field name for question text (default: \"question\" ) answer_field : Field name for ground truth (default: \"answer\" ) id_field : Field for sample ID (default: None , auto-generates sample_{idx} ) max_samples : Limit samples (default: None , all) skip_existing : Skip cached results (default: True ) Returns: EvaluationResult Caching behavior: Results are cached by saving {sample_id}_result.json and {sample_id}_program{ext} files to output_dir .","title":"evaluate()"},{"location":"api-reference/#evaluationmetrics","text":"Provides comprehensive metrics for evaluation results. @dataclass class EvaluationMetrics : accuracy : float # sklearn.metrics.accuracy_score precision : float # sklearn.metrics.precision_score (zero_division=0) recall : float # sklearn.metrics.recall_score (zero_division=0) f1_score : float # 2 * (P * R) / (P + R) specificity : float # TN / (TN + FP) false_positive_rate : float # FP / (FP + TN) false_negative_rate : float # FN / (FN + TP) tp : int # True positives fp : int # False positives tn : int # True negatives fn : int # False negatives total_samples : int # Correct + wrong + failed correct_answers : int # answer == ground_truth wrong_answers : int # answer != ground_truth failed_answers : int # success == False Metrics are computed using sklearn.metrics.confusion_matrix for binary classification.","title":"EvaluationMetrics"},{"location":"api-reference/#backend","text":"Defines the abstract interface for execution backends. Location: z3adapter.backends.abstract.Backend","title":"Backend"},{"location":"api-reference/#interface-methods","text":"class Backend ( ABC ): @abstractmethod def execute ( self , program_path : str ) -> VerificationResult : pass @abstractmethod def get_file_extension ( self ) -> str : pass @abstractmethod def get_prompt_template ( self ) -> str : pass def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None Concrete implementations are provided by SMT2Backend and JSONBackend .","title":"Interface Methods"},{"location":"api-reference/#verificationresult","text":"Encapsulates the results of Z3 verification execution. @dataclass class VerificationResult : answer : bool | None # True (SAT), False (UNSAT), None (ambiguous/error) sat_count : int unsat_count : int output : str # Raw execution output success : bool # Execution completed without exception error : str | None # Error message if failed","title":"VerificationResult"},{"location":"api-reference/#z3programgenerator","text":"Handles LLM-based program generation with error recovery. Location: z3adapter.reasoning.program_generator.Z3ProgramGenerator","title":"Z3ProgramGenerator"},{"location":"api-reference/#generate","text":"def generate ( self , question : str , temperature : float = 0.1 , max_tokens : int = 16384 , ) -> GenerationResult LLM API Call: response = self . llm_client . chat . completions . create ( model = self . model , messages = [{ \"role\" : \"user\" , \"content\" : prompt }], max_completion_tokens = max_tokens , ) Note that the temperature parameter is not passed to the API due to GPT-5 constraints.","title":"generate()"},{"location":"api-reference/#generate_with_feedback","text":"Enables multi-turn conversation with error feedback: messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ]","title":"generate_with_feedback()"},{"location":"api-reference/#utility-azure-config","text":"Provides convenient configuration for Azure OpenAI deployments. Location: utils.azure_config.get_client_config() Returns: { \"llm_client\" : AzureOpenAI ( ... ), \"model\" : str # Deployment name from env } Required environment variables: AZURE_OPENAI_API_KEY AZURE_OPENAI_ENDPOINT AZURE_OPENAI_API_VERSION AZURE_GPT5_DEPLOYMENT_NAME or AZURE_GPT4O_DEPLOYMENT_NAME","title":"Utility: Azure Config"},{"location":"backends/","text":"Backends \u00b6 ProofOfThought supports two execution backends for Z3: the standard SMT-LIB 2.0 format and a custom JSON DSL. SMT2Backend \u00b6 The SMT2 backend leverages Z3's standard command-line interface. Implementation: z3adapter/backends/smt2_backend.py Execution \u00b6 subprocess . run ([ z3_path , f \"-T: { timeout_seconds } \" , program_path ]) The execution process involves: Running Z3 as a CLI subprocess with a timeout flag Applying a hard timeout of timeout_seconds + 10 to prevent hanging Capturing output from both stdout and stderr Result Parsing \u00b6 sat_pattern = r \"(?<!un)\\bsat\\b\" # Negative lookbehind to exclude \"unsat\" unsat_pattern = r \"\\bunsat\\b\" The parser counts occurrences in Z3 output and applies the following answer logic: sat_count > 0, unsat_count == 0 \u2192 True unsat_count > 0, sat_count == 0 \u2192 False Otherwise \u2192 None Prompt Template \u00b6 The prompt template guides LLM program generation. Source: z3adapter/reasoning/smt2_prompt_template.py The template provides instructions for generating SMT-LIB 2.0 programs with these key requirements: - All commands as S-expressions: (command args...) - Declare sorts before use - Single (check-sat) per program - Semantic: sat = constraint satisfiable, unsat = contradicts knowledge base File Extension \u00b6 .smt2 JSON Backend \u00b6 The JSON backend uses Z3's Python API for direct programmatic access. Implementation: z3adapter/backends/json_backend.py Execution Pipeline \u00b6 interpreter = Z3JSONInterpreter ( program_path , verify_timeout , optimize_timeout ) interpreter . run () sat_count , unsat_count = interpreter . get_verification_counts () Z3JSONInterpreter Pipeline \u00b6 The interpreter processes JSON programs through three main stages: Step 1: SortManager ( z3adapter/dsl/sorts.py ) First, the system topologically sorts type definitions to handle dependencies, then creates Z3 sorts: Built-in: BoolSort() , IntSort() , RealSort() (pre-defined) Custom: DeclareSort(name) , EnumSort(name, values) , BitVecSort(n) , ArraySort(domain, range) For example, an ArraySort creates dependencies: { \"name\" : \"IntArray\" , \"type\" : \"ArraySort(IntSort, IntSort)\" } This requires IntSort to be defined first (fortunately, it's built-in) before creating IntArray . Step 2: ExpressionParser ( z3adapter/dsl/expressions.py ) Next, the parser evaluates logical expressions from strings using a restricted eval() : safe_globals = { ** Z3_OPERATORS , ** functions } context = { ** functions , ** constants , ** variables , ** quantified_vars } ExpressionValidator . safe_eval ( expr_str , safe_globals , context ) Only whitelisted operators are permitted: Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" } Step 3: Verifier ( z3adapter/verification/verifier.py ) Finally, the verifier tests each verification condition: result = solver . check ( condition ) # Adds condition as hypothesis to KB if result == sat : sat_count += 1 elif result == unsat : unsat_count += 1 Verification Semantics: When calling solver.check(\u03c6) , the system asks: \"Is KB \u2227 \u03c6 satisfiable?\" SAT : \u03c6 is consistent with the knowledge base (possible scenario) UNSAT : \u03c6 contradicts the knowledge base (impossible scenario) Prompt Template \u00b6 The JSON prompt template is more comprehensive than its SMT2 counterpart. Source: z3adapter/reasoning/prompt_template.py This 546-line specification of the JSON DSL includes these key sections: Sorts: { \"name\" : \"Person\" , \"type\" : \"DeclareSort\" } Functions: { \"name\" : \"supports\" , \"domain\" : [ \"Person\" , \"Issue\" ], \"range\" : \"BoolSort\" } Constants: { \"persons\" : { \"sort\" : \"Person\" , \"members\" : [ \"nancy_pelosi\" ]}} Variables: Free variables for quantifier binding: { \"name\" : \"p\" , \"sort\" : \"Person\" } Knowledge Base: [ \"ForAll([p], Implies(is_democrat(p), supports_abortion(p)))\" ] Verifications: The DSL supports three types of verifications: Simple constraint: { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } Existential: { \"name\" : \"test\" , \"exists\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"constraint\" : \"x > 0\" } Universal: { \"name\" : \"test\" , \"forall\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"implies\" : { \"antecedent\" : \"x > 0\" , \"consequent\" : \"x >= 1\" }} Critical constraint: The prompt enforces a single verification per question to avoid ambiguous results from testing both \u03c6 and \u00ac\u03c6. File Extension \u00b6 .json Benchmark Performance \u00b6 Performance comparison across datasets reveals notable differences between the backends. Results from experiments_pipeline.py (100 samples per dataset, GPT-5, max_attempts=3 ): Dataset SMT2 Accuracy JSON Accuracy SMT2 Success JSON Success ProntoQA 100% 99% 100% 100% FOLIO 69% 76% 99% 94% ProofWriter 99% 96% 99% 96% ConditionalQA 83% 76% 100% 89% StrategyQA 84% 68% 100% 86% Success Rate represents the percentage of queries that complete without error (including both generation and execution). Overall, SMT2 achieves higher accuracy on 4 out of 5 datasets, while JSON shows greater success rate variance (86-100% compared to SMT2's 99-100%). Implementation Differences \u00b6 The backends differ in several implementation details. Program Generation \u00b6 SMT2: Extracts programs from markdown via: pattern = r \"```smt2\\s*([\\s\\S]*?)\\s*```\" JSON: Extracts and parses via: pattern = r \"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" json . loads ( match . group ( 1 )) Error Handling \u00b6 Error handling varies significantly between backends. SMT2: - Subprocess timeout \u2192 TimeoutExpired - Parse errors \u2192 regex mismatch \u2192 answer=None - Z3 errors in stderr \u2192 still parsed JSON: - JSON parse error \u2192 extraction failure - Z3 Python API exception \u2192 caught in try/except - Invalid sort reference \u2192 ValueError during SortManager - Expression eval error \u2192 ValueError during ExpressionParser Timeout Configuration \u00b6 Timeout handling differs between the two backends. SMT2: - Uses a single timeout parameter: verify_timeout (ms) - Converts to seconds for Z3 CLI: verify_timeout // 1000 - Applies a hard subprocess timeout: timeout_seconds + 10 JSON: - Uses two separate timeouts: verify_timeout (ms) and optimize_timeout (ms) - Sets timeout via solver.set(\"timeout\", verify_timeout) in Verifier - Timeout applies per individual solver.check() call Backend Selection Code \u00b6 The system selects backends at runtime based on configuration: if backend == \"json\" : from z3adapter.backends.json_backend import JSONBackend backend_instance = JSONBackend ( verify_timeout , optimize_timeout ) else : # smt2 from z3adapter.backends.smt2_backend import SMT2Backend backend_instance = SMT2Backend ( verify_timeout , z3_path ) File: z3adapter/reasoning/proof_of_thought.py:78-90 Prompt Selection \u00b6 The appropriate prompt template is chosen based on the selected backend: if self . backend == \"json\" : prompt = build_prompt ( question ) else : # smt2 prompt = build_smt2_prompt ( question ) File: z3adapter/reasoning/program_generator.py:78-81 Both prompts include few-shot examples and format specifications. The SMT2 prompt emphasizes S-expression syntax, while the JSON prompt provides detailed guidance on variable scoping and quantifier semantics.","title":"Backends"},{"location":"backends/#backends","text":"ProofOfThought supports two execution backends for Z3: the standard SMT-LIB 2.0 format and a custom JSON DSL.","title":"Backends"},{"location":"backends/#smt2backend","text":"The SMT2 backend leverages Z3's standard command-line interface. Implementation: z3adapter/backends/smt2_backend.py","title":"SMT2Backend"},{"location":"backends/#execution","text":"subprocess . run ([ z3_path , f \"-T: { timeout_seconds } \" , program_path ]) The execution process involves: Running Z3 as a CLI subprocess with a timeout flag Applying a hard timeout of timeout_seconds + 10 to prevent hanging Capturing output from both stdout and stderr","title":"Execution"},{"location":"backends/#result-parsing","text":"sat_pattern = r \"(?<!un)\\bsat\\b\" # Negative lookbehind to exclude \"unsat\" unsat_pattern = r \"\\bunsat\\b\" The parser counts occurrences in Z3 output and applies the following answer logic: sat_count > 0, unsat_count == 0 \u2192 True unsat_count > 0, sat_count == 0 \u2192 False Otherwise \u2192 None","title":"Result Parsing"},{"location":"backends/#prompt-template","text":"The prompt template guides LLM program generation. Source: z3adapter/reasoning/smt2_prompt_template.py The template provides instructions for generating SMT-LIB 2.0 programs with these key requirements: - All commands as S-expressions: (command args...) - Declare sorts before use - Single (check-sat) per program - Semantic: sat = constraint satisfiable, unsat = contradicts knowledge base","title":"Prompt Template"},{"location":"backends/#file-extension","text":".smt2","title":"File Extension"},{"location":"backends/#json-backend","text":"The JSON backend uses Z3's Python API for direct programmatic access. Implementation: z3adapter/backends/json_backend.py","title":"JSON Backend"},{"location":"backends/#execution-pipeline","text":"interpreter = Z3JSONInterpreter ( program_path , verify_timeout , optimize_timeout ) interpreter . run () sat_count , unsat_count = interpreter . get_verification_counts ()","title":"Execution Pipeline"},{"location":"backends/#z3jsoninterpreter-pipeline","text":"The interpreter processes JSON programs through three main stages: Step 1: SortManager ( z3adapter/dsl/sorts.py ) First, the system topologically sorts type definitions to handle dependencies, then creates Z3 sorts: Built-in: BoolSort() , IntSort() , RealSort() (pre-defined) Custom: DeclareSort(name) , EnumSort(name, values) , BitVecSort(n) , ArraySort(domain, range) For example, an ArraySort creates dependencies: { \"name\" : \"IntArray\" , \"type\" : \"ArraySort(IntSort, IntSort)\" } This requires IntSort to be defined first (fortunately, it's built-in) before creating IntArray . Step 2: ExpressionParser ( z3adapter/dsl/expressions.py ) Next, the parser evaluates logical expressions from strings using a restricted eval() : safe_globals = { ** Z3_OPERATORS , ** functions } context = { ** functions , ** constants , ** variables , ** quantified_vars } ExpressionValidator . safe_eval ( expr_str , safe_globals , context ) Only whitelisted operators are permitted: Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" } Step 3: Verifier ( z3adapter/verification/verifier.py ) Finally, the verifier tests each verification condition: result = solver . check ( condition ) # Adds condition as hypothesis to KB if result == sat : sat_count += 1 elif result == unsat : unsat_count += 1 Verification Semantics: When calling solver.check(\u03c6) , the system asks: \"Is KB \u2227 \u03c6 satisfiable?\" SAT : \u03c6 is consistent with the knowledge base (possible scenario) UNSAT : \u03c6 contradicts the knowledge base (impossible scenario)","title":"Z3JSONInterpreter Pipeline"},{"location":"backends/#prompt-template_1","text":"The JSON prompt template is more comprehensive than its SMT2 counterpart. Source: z3adapter/reasoning/prompt_template.py This 546-line specification of the JSON DSL includes these key sections: Sorts: { \"name\" : \"Person\" , \"type\" : \"DeclareSort\" } Functions: { \"name\" : \"supports\" , \"domain\" : [ \"Person\" , \"Issue\" ], \"range\" : \"BoolSort\" } Constants: { \"persons\" : { \"sort\" : \"Person\" , \"members\" : [ \"nancy_pelosi\" ]}} Variables: Free variables for quantifier binding: { \"name\" : \"p\" , \"sort\" : \"Person\" } Knowledge Base: [ \"ForAll([p], Implies(is_democrat(p), supports_abortion(p)))\" ] Verifications: The DSL supports three types of verifications: Simple constraint: { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } Existential: { \"name\" : \"test\" , \"exists\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"constraint\" : \"x > 0\" } Universal: { \"name\" : \"test\" , \"forall\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }], \"implies\" : { \"antecedent\" : \"x > 0\" , \"consequent\" : \"x >= 1\" }} Critical constraint: The prompt enforces a single verification per question to avoid ambiguous results from testing both \u03c6 and \u00ac\u03c6.","title":"Prompt Template"},{"location":"backends/#file-extension_1","text":".json","title":"File Extension"},{"location":"backends/#benchmark-performance","text":"Performance comparison across datasets reveals notable differences between the backends. Results from experiments_pipeline.py (100 samples per dataset, GPT-5, max_attempts=3 ): Dataset SMT2 Accuracy JSON Accuracy SMT2 Success JSON Success ProntoQA 100% 99% 100% 100% FOLIO 69% 76% 99% 94% ProofWriter 99% 96% 99% 96% ConditionalQA 83% 76% 100% 89% StrategyQA 84% 68% 100% 86% Success Rate represents the percentage of queries that complete without error (including both generation and execution). Overall, SMT2 achieves higher accuracy on 4 out of 5 datasets, while JSON shows greater success rate variance (86-100% compared to SMT2's 99-100%).","title":"Benchmark Performance"},{"location":"backends/#implementation-differences","text":"The backends differ in several implementation details.","title":"Implementation Differences"},{"location":"backends/#program-generation","text":"SMT2: Extracts programs from markdown via: pattern = r \"```smt2\\s*([\\s\\S]*?)\\s*```\" JSON: Extracts and parses via: pattern = r \"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" json . loads ( match . group ( 1 ))","title":"Program Generation"},{"location":"backends/#error-handling","text":"Error handling varies significantly between backends. SMT2: - Subprocess timeout \u2192 TimeoutExpired - Parse errors \u2192 regex mismatch \u2192 answer=None - Z3 errors in stderr \u2192 still parsed JSON: - JSON parse error \u2192 extraction failure - Z3 Python API exception \u2192 caught in try/except - Invalid sort reference \u2192 ValueError during SortManager - Expression eval error \u2192 ValueError during ExpressionParser","title":"Error Handling"},{"location":"backends/#timeout-configuration","text":"Timeout handling differs between the two backends. SMT2: - Uses a single timeout parameter: verify_timeout (ms) - Converts to seconds for Z3 CLI: verify_timeout // 1000 - Applies a hard subprocess timeout: timeout_seconds + 10 JSON: - Uses two separate timeouts: verify_timeout (ms) and optimize_timeout (ms) - Sets timeout via solver.set(\"timeout\", verify_timeout) in Verifier - Timeout applies per individual solver.check() call","title":"Timeout Configuration"},{"location":"backends/#backend-selection-code","text":"The system selects backends at runtime based on configuration: if backend == \"json\" : from z3adapter.backends.json_backend import JSONBackend backend_instance = JSONBackend ( verify_timeout , optimize_timeout ) else : # smt2 from z3adapter.backends.smt2_backend import SMT2Backend backend_instance = SMT2Backend ( verify_timeout , z3_path ) File: z3adapter/reasoning/proof_of_thought.py:78-90","title":"Backend Selection Code"},{"location":"backends/#prompt-selection","text":"The appropriate prompt template is chosen based on the selected backend: if self . backend == \"json\" : prompt = build_prompt ( question ) else : # smt2 prompt = build_smt2_prompt ( question ) File: z3adapter/reasoning/program_generator.py:78-81 Both prompts include few-shot examples and format specifications. The SMT2 prompt emphasizes S-expression syntax, while the JSON prompt provides detailed guidance on variable scoping and quantifier semantics.","title":"Prompt Selection"},{"location":"benchmarks/","text":"Benchmarks \u00b6 This page presents evaluation results on 5 logical reasoning datasets using Azure GPT-5. Methodology \u00b6 The evaluation follows a consistent methodology across all datasets. Model: Azure GPT-5 deployment Configuration: - max_attempts=3 (retry with error feedback) - verify_timeout=10000ms - optimize_timeout=100000ms (JSON backend only) - num_workers=10 (ThreadPoolExecutor for parallel processing) Metrics (computed via sklearn.metrics ): Accuracy: accuracy_score(y_true, y_pred) Precision: precision_score(y_true, y_pred, zero_division=0) Recall: recall_score(y_true, y_pred, zero_division=0) F1: 2 * (precision * recall) / (precision + recall) Success Rate: (total - failed) / total Execution: The experiments_pipeline.py script runs all benchmarks sequentially, modifying the BACKEND variable in each benchmark/bench_*.py script via regex substitution. Results \u00b6 Results from the most recent benchmark run. Last Updated: 2025-10-16 18:14:07 Benchmark Backend Samples Accuracy Precision Recall F1 Score Success Rate ProntoQA SMT2 100 100.00% 1.0000 1.0000 1.0000 100.00% FOLIO SMT2 100 69.00% 0.6949 0.7736 0.7321 99.00% ProofWriter SMT2 96 98.96% 1.0000 1.0000 1.0000 98.96% ConditionalQA SMT2 100 83.00% 0.9375 0.8219 0.8759 100.00% StrategyQA SMT2 100 84.00% 0.8205 0.7805 0.8000 100.00% ProntoQA JSON 100 99.00% 1.0000 0.9815 0.9907 100.00% FOLIO JSON 100 76.00% 0.7619 0.9412 0.8421 94.00% ProofWriter JSON 96 95.83% 1.0000 1.0000 1.0000 95.83% ConditionalQA JSON 100 76.00% 0.9180 0.8750 0.8960 89.00% StrategyQA JSON 100 68.00% 0.7500 0.7895 0.7692 86.00% Dataset Characteristics \u00b6 Each dataset tests different aspects of logical reasoning. ProntoQA \u00b6 ProntoQA features synthetic first-order logic problems with deterministic inference. Example: Facts: \"Stella is a lion. All lions are brown.\" Question: \"Is Stella brown?\" Answer: True Performance: SMT2: 100% (100/100) JSON: 99% (99/100) Both backends achieve near-perfect results, making this the simplest dataset in the benchmark suite. FOLIO \u00b6 FOLIO presents first-order logic problems derived from Wikipedia articles. Characteristics: Features complex nested quantifiers and longer inference chains. Performance: SMT2: 69% (69/100) JSON: 76% (76/100) JSON outperforms SMT2 by 7% on this dataset, which is the most challenging in the suite. However, JSON's lower success rate (94% vs 99%) indicates greater difficulty in program generation. ProofWriter \u00b6 ProofWriter tests deductive reasoning over explicit facts and rules. Example: Facts: \"The bear is red. If something is red, then it is kind.\" Question: \"Is the bear kind?\" Answer: True Performance: SMT2: 98.96% (95/96) JSON: 95.83% (92/96) Both backends achieve high accuracy on this dataset, with SMT2 holding a slight 3% edge. ConditionalQA \u00b6 ConditionalQA focuses on conditional reasoning with if-then statements. Performance: SMT2: 83% (83/100) JSON: 76% (76/100) SMT2 demonstrates better accuracy (+7%) and also achieves a higher success rate (100% vs 89%). StrategyQA \u00b6 StrategyQA tests multi-hop reasoning that requires implicit world knowledge. Example: Question: \"Would a vegetarian eat a burger made of plants?\" Answer: True (requires knowing: vegetarians avoid meat, plant burgers have no meat) Performance: SMT2: 84% (84/100) JSON: 68% (68/100) This dataset shows the largest performance gap, with SMT2 leading by 16%. Both backends achieve good success rates of 100% and 86% respectively. Analysis \u00b6 Accuracy Summary \u00b6 Aggregating results across all datasets: SMT2: 86.8% average accuracy JSON: 82.8% average accuracy SMT2 proves superior on 4 out of 5 datasets, with FOLIO being the exception where JSON leads by 7%. Success Rate Summary \u00b6 The success rate measures program generation and execution reliability: SMT2: 99.4% average (range: 98.96-100%) JSON: 92.8% average (range: 86-100%) SMT2 demonstrates more reliable program generation and execution overall. JSON's higher success rate variance indicates LLM generation challenges on certain datasets. Failure Modes \u00b6 Understanding failure modes helps identify areas for improvement. SMT2 failures: Program extraction from markdown: regex mismatch Z3 subprocess timeout (rare with 10s limit) Invalid SMT-LIB syntax (caught by Z3 parser) JSON failures: JSON parsing errors after extraction Invalid sort references (e.g., undefined Person sort) Expression evaluation errors in ExpressionParser.parse_expression() Z3 Python API exceptions Reproducing Results \u00b6 Full benchmark suite \u00b6 To run the complete benchmark suite: python experiments_pipeline.py This generates: results/benchmark_results.json - Raw metrics data results/benchmark_results.md - Formatted markdown table Updates README.md between <!-- BENCHMARK_RESULTS_START/END --> markers Single benchmark \u00b6 To run just one benchmark: python benchmark/bench_strategyqa.py You'll need to modify the BACKEND variable in the script to either smt2 or json . Custom evaluation \u00b6 For custom evaluation on your own dataset: from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought , EvaluationPipeline config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"Precision: { result . metrics . precision : .4f } \" ) print ( f \"Recall: { result . metrics . recall : .4f } \" ) print ( f \"F1: { result . metrics . f1_score : .4f } \" ) Dataset Sources \u00b6 The benchmark datasets are located in the data/ directory: ProntoQA: data/prontoqa_test.json FOLIO: data/folio_test.json ProofWriter: data/proof_writer_test.json ConditionalQA: data/conditionalQA_test.json StrategyQA: data/strategyQA_train.json All datasets follow the same format: JSON arrays with question and answer fields (boolean values). Implementation Notes \u00b6 Parallel Processing \u00b6 Benchmark scripts use num_workers=10 with ThreadPoolExecutor for parallel processing. Note that ProcessPoolExecutor cannot be used due to ProofOfThought being unpicklable. Caching \u00b6 Setting skip_existing=True enables resumption of interrupted runs. Results are cached as: output/{backend}_evaluation_{dataset}/{sample_id}_result.json output/{backend}_programs_{dataset}/{sample_id}_program.{ext} Timeout Handling \u00b6 The experiments_pipeline.py script sets a 1-hour subprocess timeout for each benchmark. Individual Z3 verification calls timeout at 10 seconds, while optimization calls timeout at 100 seconds.","title":"Benchmarks"},{"location":"benchmarks/#benchmarks","text":"This page presents evaluation results on 5 logical reasoning datasets using Azure GPT-5.","title":"Benchmarks"},{"location":"benchmarks/#methodology","text":"The evaluation follows a consistent methodology across all datasets. Model: Azure GPT-5 deployment Configuration: - max_attempts=3 (retry with error feedback) - verify_timeout=10000ms - optimize_timeout=100000ms (JSON backend only) - num_workers=10 (ThreadPoolExecutor for parallel processing) Metrics (computed via sklearn.metrics ): Accuracy: accuracy_score(y_true, y_pred) Precision: precision_score(y_true, y_pred, zero_division=0) Recall: recall_score(y_true, y_pred, zero_division=0) F1: 2 * (precision * recall) / (precision + recall) Success Rate: (total - failed) / total Execution: The experiments_pipeline.py script runs all benchmarks sequentially, modifying the BACKEND variable in each benchmark/bench_*.py script via regex substitution.","title":"Methodology"},{"location":"benchmarks/#results","text":"Results from the most recent benchmark run. Last Updated: 2025-10-16 18:14:07 Benchmark Backend Samples Accuracy Precision Recall F1 Score Success Rate ProntoQA SMT2 100 100.00% 1.0000 1.0000 1.0000 100.00% FOLIO SMT2 100 69.00% 0.6949 0.7736 0.7321 99.00% ProofWriter SMT2 96 98.96% 1.0000 1.0000 1.0000 98.96% ConditionalQA SMT2 100 83.00% 0.9375 0.8219 0.8759 100.00% StrategyQA SMT2 100 84.00% 0.8205 0.7805 0.8000 100.00% ProntoQA JSON 100 99.00% 1.0000 0.9815 0.9907 100.00% FOLIO JSON 100 76.00% 0.7619 0.9412 0.8421 94.00% ProofWriter JSON 96 95.83% 1.0000 1.0000 1.0000 95.83% ConditionalQA JSON 100 76.00% 0.9180 0.8750 0.8960 89.00% StrategyQA JSON 100 68.00% 0.7500 0.7895 0.7692 86.00%","title":"Results"},{"location":"benchmarks/#dataset-characteristics","text":"Each dataset tests different aspects of logical reasoning.","title":"Dataset Characteristics"},{"location":"benchmarks/#prontoqa","text":"ProntoQA features synthetic first-order logic problems with deterministic inference. Example: Facts: \"Stella is a lion. All lions are brown.\" Question: \"Is Stella brown?\" Answer: True Performance: SMT2: 100% (100/100) JSON: 99% (99/100) Both backends achieve near-perfect results, making this the simplest dataset in the benchmark suite.","title":"ProntoQA"},{"location":"benchmarks/#folio","text":"FOLIO presents first-order logic problems derived from Wikipedia articles. Characteristics: Features complex nested quantifiers and longer inference chains. Performance: SMT2: 69% (69/100) JSON: 76% (76/100) JSON outperforms SMT2 by 7% on this dataset, which is the most challenging in the suite. However, JSON's lower success rate (94% vs 99%) indicates greater difficulty in program generation.","title":"FOLIO"},{"location":"benchmarks/#proofwriter","text":"ProofWriter tests deductive reasoning over explicit facts and rules. Example: Facts: \"The bear is red. If something is red, then it is kind.\" Question: \"Is the bear kind?\" Answer: True Performance: SMT2: 98.96% (95/96) JSON: 95.83% (92/96) Both backends achieve high accuracy on this dataset, with SMT2 holding a slight 3% edge.","title":"ProofWriter"},{"location":"benchmarks/#conditionalqa","text":"ConditionalQA focuses on conditional reasoning with if-then statements. Performance: SMT2: 83% (83/100) JSON: 76% (76/100) SMT2 demonstrates better accuracy (+7%) and also achieves a higher success rate (100% vs 89%).","title":"ConditionalQA"},{"location":"benchmarks/#strategyqa","text":"StrategyQA tests multi-hop reasoning that requires implicit world knowledge. Example: Question: \"Would a vegetarian eat a burger made of plants?\" Answer: True (requires knowing: vegetarians avoid meat, plant burgers have no meat) Performance: SMT2: 84% (84/100) JSON: 68% (68/100) This dataset shows the largest performance gap, with SMT2 leading by 16%. Both backends achieve good success rates of 100% and 86% respectively.","title":"StrategyQA"},{"location":"benchmarks/#analysis","text":"","title":"Analysis"},{"location":"benchmarks/#accuracy-summary","text":"Aggregating results across all datasets: SMT2: 86.8% average accuracy JSON: 82.8% average accuracy SMT2 proves superior on 4 out of 5 datasets, with FOLIO being the exception where JSON leads by 7%.","title":"Accuracy Summary"},{"location":"benchmarks/#success-rate-summary","text":"The success rate measures program generation and execution reliability: SMT2: 99.4% average (range: 98.96-100%) JSON: 92.8% average (range: 86-100%) SMT2 demonstrates more reliable program generation and execution overall. JSON's higher success rate variance indicates LLM generation challenges on certain datasets.","title":"Success Rate Summary"},{"location":"benchmarks/#failure-modes","text":"Understanding failure modes helps identify areas for improvement. SMT2 failures: Program extraction from markdown: regex mismatch Z3 subprocess timeout (rare with 10s limit) Invalid SMT-LIB syntax (caught by Z3 parser) JSON failures: JSON parsing errors after extraction Invalid sort references (e.g., undefined Person sort) Expression evaluation errors in ExpressionParser.parse_expression() Z3 Python API exceptions","title":"Failure Modes"},{"location":"benchmarks/#reproducing-results","text":"","title":"Reproducing Results"},{"location":"benchmarks/#full-benchmark-suite","text":"To run the complete benchmark suite: python experiments_pipeline.py This generates: results/benchmark_results.json - Raw metrics data results/benchmark_results.md - Formatted markdown table Updates README.md between <!-- BENCHMARK_RESULTS_START/END --> markers","title":"Full benchmark suite"},{"location":"benchmarks/#single-benchmark","text":"To run just one benchmark: python benchmark/bench_strategyqa.py You'll need to modify the BACKEND variable in the script to either smt2 or json .","title":"Single benchmark"},{"location":"benchmarks/#custom-evaluation","text":"For custom evaluation on your own dataset: from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought , EvaluationPipeline config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"Precision: { result . metrics . precision : .4f } \" ) print ( f \"Recall: { result . metrics . recall : .4f } \" ) print ( f \"F1: { result . metrics . f1_score : .4f } \" )","title":"Custom evaluation"},{"location":"benchmarks/#dataset-sources","text":"The benchmark datasets are located in the data/ directory: ProntoQA: data/prontoqa_test.json FOLIO: data/folio_test.json ProofWriter: data/proof_writer_test.json ConditionalQA: data/conditionalQA_test.json StrategyQA: data/strategyQA_train.json All datasets follow the same format: JSON arrays with question and answer fields (boolean values).","title":"Dataset Sources"},{"location":"benchmarks/#implementation-notes","text":"","title":"Implementation Notes"},{"location":"benchmarks/#parallel-processing","text":"Benchmark scripts use num_workers=10 with ThreadPoolExecutor for parallel processing. Note that ProcessPoolExecutor cannot be used due to ProofOfThought being unpicklable.","title":"Parallel Processing"},{"location":"benchmarks/#caching","text":"Setting skip_existing=True enables resumption of interrupted runs. Results are cached as: output/{backend}_evaluation_{dataset}/{sample_id}_result.json output/{backend}_programs_{dataset}/{sample_id}_program.{ext}","title":"Caching"},{"location":"benchmarks/#timeout-handling","text":"The experiments_pipeline.py script sets a 1-hour subprocess timeout for each benchmark. Individual Z3 verification calls timeout at 10 seconds, while optimization calls timeout at 100 seconds.","title":"Timeout Handling"},{"location":"dsl-specification/","text":"DSL Specification \u00b6 This page provides technical details of the JSON DSL implementation. Rules vs Verifications \u00b6 Understanding the distinction between rules and verifications is critical to using the DSL correctly. Key difference: Rules modify the solver state, while verifications query it. Rules \u00b6 Rules define axioms that permanently affect the solver's knowledge base. Implementation: z3adapter/dsl/expressions.py:159-204 Operation: solver.add(assertion) Rules are permanently asserted into the solver's knowledge base during step 6 of the interpretation pipeline. # Line 189: Implication rule solver . add ( ForAll ( variables , Implies ( antecedent , consequent ))) # Line 194-196: Constraint rule if variables : solver . add ( ForAll ( variables , constraint )) else : solver . add ( constraint ) Structure: { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } Effect: This defines an axiom that every subsequent verification will inherit. Verifications \u00b6 Verifications test conditions without modifying the knowledge base. Implementation: z3adapter/verification/verifier.py:84-127 Operation: solver.check(condition) Verifications test conditions against the existing knowledge base without modifying it. # Line 113 result = solver . check ( condition ) # Temporary hypothesis check if result == sat : self . sat_count += 1 elif result == unsat : self . unsat_count += 1 Semantics: The check determines if KB \u2227 condition is satisfiable. SAT : The condition is consistent with the knowledge base UNSAT : The condition contradicts the knowledge base Structure: { \"name\" : \"test_pelosi\" , \"constraint\" : \"publicly_denounce(nancy, abortion)\" } Effect: Returns SAT/UNSAT result but does NOT add the condition to the knowledge base. Example \u00b6 Here's a complete example showing the interaction between rules and verifications: { \"rules\" : [ { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } ], \"knowledge_base\" : [ \"is_democrat(nancy)\" ], \"verifications\" : [ { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } ] } Execution: 1. solver.add(ForAll([p], Implies(is_democrat(p), supports_abortion(p)))) \u2014 rule 2. solver.add(is_democrat(nancy)) \u2014 knowledge base 3. solver.check(supports_abortion(nancy)) \u2014 verification \u2192 SAT Variable Scoping \u00b6 The DSL supports both free and quantified variables with careful scoping rules. Implementation: z3adapter/dsl/expressions.py:69-107 Free Variables \u00b6 Free variables are declared in the global \"variables\" section and remain available throughout the program. \"variables\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }] These are added to the evaluation context at line 91: context . update ( self . variables ) Quantified Variables \u00b6 Quantified variables are bound by ForAll or Exists operators and temporarily shadow free variables within their scope. \"knowledge_base\" : [ \"ForAll([x], x > 0)\" ] In this example, x must already exist in the context (from \"variables\" ) to be bound by the ForAll operator. Shadowing \u00b6 The implementation includes shadowing checks at lines 100-106: for v in quantified_vars : var_name = v . decl () . name () if var_name in context and var_name not in [ ... ]: logger . warning ( f \"Quantified variable ' { var_name } ' shadows existing symbol\" ) context [ var_name ] = v When shadowing occurs, variables bound by quantifiers take precedence over free variables within their local scope. Answer Determination \u00b6 The system determines the final answer based on verification counts. Implementation: z3adapter/backends/abstract.py:52-67 def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None # Ambiguous Ambiguous results (returning None ) occur in two cases: sat_count > 0 and unsat_count > 0 \u2014 multiple verifications produced conflicting results sat_count == 0 and unsat_count == 0 \u2014 no verifications ran or all returned unknown Handling: The system treats None as an error and retries with feedback (see proof_of_thought.py:183-191 ): if verify_result . answer is None : error_trace = ( f \"Ambiguous verification result: \" f \"SAT= { verify_result . sat_count } , UNSAT= { verify_result . unsat_count } \" ) continue # Retry with error feedback Best practice: Use a single verification per program to avoid ambiguous results. This is enforced by the prompt template at line 416. Security Model \u00b6 The DSL includes multiple security layers to prevent code injection attacks. Implementation: z3adapter/security/validator.py AST Validation \u00b6 Before executing eval() , the system parses expressions to an AST and checks for dangerous constructs (lines 21-42). Blocked constructs: - Dunder attributes: __import__ , __class__ , etc. (line 24) - Imports: import , from ... import (line 29) - Function/class definitions (line 32) - Builtin abuse: eval , exec , compile , __import__ (line 36-42) Restricted Evaluation \u00b6 The evaluation environment is carefully sandboxed: # Line 66 eval ( code , { \"__builtins__\" : {}}, { ** safe_globals , ** context }) Three layers of protection: No builtins : Setting __builtins__: {} prevents access to open , print , and other dangerous functions Whitelisted globals : Only Z3 operators and user-defined functions are available Local context : Limited to constants, variables, and quantified variables Whitelisted operators (from expressions.py:33-47 ): Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" } Sort Dependency Resolution \u00b6 Complex types may depend on other types, requiring careful ordering during creation. Implementation: z3adapter/dsl/sorts.py:36-97 The system uses Kahn's algorithm for topological sorting of type definitions. Dependency Extraction \u00b6 ArraySort declarations create dependencies that must be resolved (lines 59-62): if sort_type . startswith ( \"ArraySort(\" ): domain_range = sort_type [ len ( \"ArraySort(\" ) : - 1 ] parts = [ s . strip () for s in domain_range . split ( \",\" )] deps . extend ( parts ) For example: { \"name\" : \"MyArray\" , \"type\" : \"ArraySort(IntSort, Person)\" } This depends on: IntSort (built-in, can skip) and Person (must be defined first). Topological Sort \u00b6 Kahn's algorithm processes types in dependency order (lines 66-87): Calculate the in-degree (dependency count) for each sort Process sorts with zero dependencies first Reduce the in-degree of dependent sorts as their dependencies are satisfied Detect cycles if not all sorts can be processed (lines 90-92) Circular dependency detection: if len ( sorted_names ) != len ( dependencies ): remaining = set ( dependencies . keys ()) - set ( sorted_names ) raise ValueError ( f \"Circular dependency detected in sorts: { remaining } \" ) Optimizer Independence \u00b6 The optimizer operates independently from the main solver. Implementation: z3adapter/optimization/optimizer.py:29-39 def __init__ ( self , ... ): self . optimizer = Optimize () # Separate instance Critical detail: The Optimize() instance is completely separate from the main Solver() and does NOT share constraints. As stated in the docstring (line 38-39): The optimizer is separate from the solver and doesn't share constraints. This is intentional to allow independent optimization problems. The optimizer maintains its own variables and constraints (lines 49-69). However, it can reference global constants through an extended context (line 60-61): base_context = self . expression_parser . build_context () opt_context = { ** base_context , ** optimization_vars } Execution Pipeline \u00b6 The interpreter follows a carefully ordered execution sequence. Implementation: z3adapter/interpreter.py:135-197 8-step execution sequence: # Step 1: Create sorts self . sort_manager . create_sorts ( self . config [ \"sorts\" ]) # Step 2: Create functions functions = self . sort_manager . create_functions ( self . config [ \"functions\" ]) # Step 3: Create constants self . sort_manager . create_constants ( self . config [ \"constants\" ]) # Step 4: Create variables variables = self . sort_manager . create_variables ( self . config . get ( \"variables\" , [])) # Step 5: Initialize expression parser self . expression_parser = ExpressionParser ( functions , constants , variables ) self . expression_parser . mark_symbols_loaded () # Enable context caching # Step 6: Add knowledge base self . expression_parser . add_knowledge_base ( self . solver , self . config [ \"knowledge_base\" ]) # Step 7: Add rules self . expression_parser . add_rules ( self . solver , self . config [ \"rules\" ], sorts ) # Step 8: Initialize verifier and add verifications self . verifier = Verifier ( self . expression_parser , sorts ) self . verifier . add_verifications ( self . config [ \"verifications\" ]) # Step 9: Perform actions (e.g., \"verify_conditions\") self . perform_actions () Symbol loading optimization: At line 172, calling mark_symbols_loaded() enables context caching (see lines 78-84 in expressions.py ). After this point, build_context() returns a cached dictionary instead of rebuilding it on each call. Retry Mechanism \u00b6 The system can automatically retry failed program generation with error feedback. Implementation: z3adapter/reasoning/proof_of_thought.py:123-191 Retry loop with error feedback: for attempt in range ( 1 , self . max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , ... ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , ... ) Failure modes that trigger retry: Generation failure (lines 143-149): python if not gen_result.success or gen_result.program is None: error_trace = gen_result.error or \"Failed to generate program\" continue Execution failure (lines 176-180): python if not verify_result.success: error_trace = verify_result.error or \"Z3 verification failed\" continue Ambiguous result (lines 183-191): python if verify_result.answer is None: error_trace = f\"Ambiguous verification result: SAT={sat_count}, UNSAT={unsat_count}\" continue Error feedback mechanism: Uses multi-turn conversation (see program_generator.py:130-174 ): messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ] Solver Semantics \u00b6 The solver's check method has two distinct modes of operation. Implementation: z3adapter/solvers/z3_solver.py:20-24 def check ( self , condition : Any = None ) -> Any : if condition is not None : return self . solver . check ( condition ) # Temporary hypothesis return self . solver . check () # Check all assertions Two modes of operation: solver.check() : Checks the satisfiability of all assertions added via solver.add() solver.check(\u03c6) : Checks the satisfiability of assertions \u2227 \u03c6 without permanently adding \u03c6 Verifications use mode 2 (see verifier.py:113 ): result = solver . check ( condition ) This is a temporary check \u2014 the condition is NOT permanently added to the solver. Contrast with rules: Rules : solver.add(\u03c6) \u2192 permanently modifies solver state Verifications : solver.check(\u03c6) \u2192 temporary test without modification Built-in Sorts \u00b6 The DSL includes three pre-initialized built-in sorts. Implementation: z3adapter/dsl/sorts.py:31-34 def _initialize_builtin_sorts ( self ) -> None : built_in_sorts = { \"BoolSort\" : BoolSort (), \"IntSort\" : IntSort (), \"RealSort\" : RealSort ()} self . sorts . update ( built_in_sorts ) Important: Always reference these as \"BoolSort\" , \"IntSort\" , and \"RealSort\" in JSON (not \"Bool\" , \"Int\" , or \"Real\" ). These sorts are already available and should NOT be declared in the \"sorts\" section. Prompt Template Constraints \u00b6 The prompt template enforces several important constraints on DSL programs. Implementation: z3adapter/reasoning/prompt_template.py Key constraints (extracted from code): Line 228: Implication Rules \u00b6 Rules with \"implies\" MUST have non-empty \"forall\" field: # expressions.py:184-186 if \"implies\" in rule : if not variables : raise ValueError ( \"Implication rules require quantified variables\" ) Line 298: Quantifier Lists \u00b6 Empty quantifier lists are forbidden: # verifier.py:42-43, 55-56 if not exists_vars : raise ValueError ( f \"Empty 'exists' list in verification\" ) Line 416: Single Verification \u00b6 Use a single verification per program to avoid ambiguous results: # Directly impacts determine_answer() \u2014 mixed SAT/UNSAT returns None Line 531: Output Format \u00b6 LLM output requirements: Must wrap JSON in a markdown code block: ```json ... ``` Extracted via regex: r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" (see program_generator.py:224 )","title":"DSL Specification"},{"location":"dsl-specification/#dsl-specification","text":"This page provides technical details of the JSON DSL implementation.","title":"DSL Specification"},{"location":"dsl-specification/#rules-vs-verifications","text":"Understanding the distinction between rules and verifications is critical to using the DSL correctly. Key difference: Rules modify the solver state, while verifications query it.","title":"Rules vs Verifications"},{"location":"dsl-specification/#rules","text":"Rules define axioms that permanently affect the solver's knowledge base. Implementation: z3adapter/dsl/expressions.py:159-204 Operation: solver.add(assertion) Rules are permanently asserted into the solver's knowledge base during step 6 of the interpretation pipeline. # Line 189: Implication rule solver . add ( ForAll ( variables , Implies ( antecedent , consequent ))) # Line 194-196: Constraint rule if variables : solver . add ( ForAll ( variables , constraint )) else : solver . add ( constraint ) Structure: { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } Effect: This defines an axiom that every subsequent verification will inherit.","title":"Rules"},{"location":"dsl-specification/#verifications","text":"Verifications test conditions without modifying the knowledge base. Implementation: z3adapter/verification/verifier.py:84-127 Operation: solver.check(condition) Verifications test conditions against the existing knowledge base without modifying it. # Line 113 result = solver . check ( condition ) # Temporary hypothesis check if result == sat : self . sat_count += 1 elif result == unsat : self . unsat_count += 1 Semantics: The check determines if KB \u2227 condition is satisfiable. SAT : The condition is consistent with the knowledge base UNSAT : The condition contradicts the knowledge base Structure: { \"name\" : \"test_pelosi\" , \"constraint\" : \"publicly_denounce(nancy, abortion)\" } Effect: Returns SAT/UNSAT result but does NOT add the condition to the knowledge base.","title":"Verifications"},{"location":"dsl-specification/#example","text":"Here's a complete example showing the interaction between rules and verifications: { \"rules\" : [ { \"forall\" : [{ \"name\" : \"p\" , \"sort\" : \"Person\" }], \"implies\" : { \"antecedent\" : \"is_democrat(p)\" , \"consequent\" : \"supports_abortion(p)\" } } ], \"knowledge_base\" : [ \"is_democrat(nancy)\" ], \"verifications\" : [ { \"name\" : \"test\" , \"constraint\" : \"supports_abortion(nancy)\" } ] } Execution: 1. solver.add(ForAll([p], Implies(is_democrat(p), supports_abortion(p)))) \u2014 rule 2. solver.add(is_democrat(nancy)) \u2014 knowledge base 3. solver.check(supports_abortion(nancy)) \u2014 verification \u2192 SAT","title":"Example"},{"location":"dsl-specification/#variable-scoping","text":"The DSL supports both free and quantified variables with careful scoping rules. Implementation: z3adapter/dsl/expressions.py:69-107","title":"Variable Scoping"},{"location":"dsl-specification/#free-variables","text":"Free variables are declared in the global \"variables\" section and remain available throughout the program. \"variables\" : [{ \"name\" : \"x\" , \"sort\" : \"Int\" }] These are added to the evaluation context at line 91: context . update ( self . variables )","title":"Free Variables"},{"location":"dsl-specification/#quantified-variables","text":"Quantified variables are bound by ForAll or Exists operators and temporarily shadow free variables within their scope. \"knowledge_base\" : [ \"ForAll([x], x > 0)\" ] In this example, x must already exist in the context (from \"variables\" ) to be bound by the ForAll operator.","title":"Quantified Variables"},{"location":"dsl-specification/#shadowing","text":"The implementation includes shadowing checks at lines 100-106: for v in quantified_vars : var_name = v . decl () . name () if var_name in context and var_name not in [ ... ]: logger . warning ( f \"Quantified variable ' { var_name } ' shadows existing symbol\" ) context [ var_name ] = v When shadowing occurs, variables bound by quantifiers take precedence over free variables within their local scope.","title":"Shadowing"},{"location":"dsl-specification/#answer-determination","text":"The system determines the final answer based on verification counts. Implementation: z3adapter/backends/abstract.py:52-67 def determine_answer ( self , sat_count : int , unsat_count : int ) -> bool | None : if sat_count > 0 and unsat_count == 0 : return True elif unsat_count > 0 and sat_count == 0 : return False else : return None # Ambiguous Ambiguous results (returning None ) occur in two cases: sat_count > 0 and unsat_count > 0 \u2014 multiple verifications produced conflicting results sat_count == 0 and unsat_count == 0 \u2014 no verifications ran or all returned unknown Handling: The system treats None as an error and retries with feedback (see proof_of_thought.py:183-191 ): if verify_result . answer is None : error_trace = ( f \"Ambiguous verification result: \" f \"SAT= { verify_result . sat_count } , UNSAT= { verify_result . unsat_count } \" ) continue # Retry with error feedback Best practice: Use a single verification per program to avoid ambiguous results. This is enforced by the prompt template at line 416.","title":"Answer Determination"},{"location":"dsl-specification/#security-model","text":"The DSL includes multiple security layers to prevent code injection attacks. Implementation: z3adapter/security/validator.py","title":"Security Model"},{"location":"dsl-specification/#ast-validation","text":"Before executing eval() , the system parses expressions to an AST and checks for dangerous constructs (lines 21-42). Blocked constructs: - Dunder attributes: __import__ , __class__ , etc. (line 24) - Imports: import , from ... import (line 29) - Function/class definitions (line 32) - Builtin abuse: eval , exec , compile , __import__ (line 36-42)","title":"AST Validation"},{"location":"dsl-specification/#restricted-evaluation","text":"The evaluation environment is carefully sandboxed: # Line 66 eval ( code , { \"__builtins__\" : {}}, { ** safe_globals , ** context }) Three layers of protection: No builtins : Setting __builtins__: {} prevents access to open , print , and other dangerous functions Whitelisted globals : Only Z3 operators and user-defined functions are available Local context : Limited to constants, variables, and quantified variables Whitelisted operators (from expressions.py:33-47 ): Z3_OPERATORS = { \"And\" , \"Or\" , \"Not\" , \"Implies\" , \"If\" , \"Distinct\" , \"Sum\" , \"Product\" , \"ForAll\" , \"Exists\" , \"Function\" , \"Array\" , \"BitVecVal\" }","title":"Restricted Evaluation"},{"location":"dsl-specification/#sort-dependency-resolution","text":"Complex types may depend on other types, requiring careful ordering during creation. Implementation: z3adapter/dsl/sorts.py:36-97 The system uses Kahn's algorithm for topological sorting of type definitions.","title":"Sort Dependency Resolution"},{"location":"dsl-specification/#dependency-extraction","text":"ArraySort declarations create dependencies that must be resolved (lines 59-62): if sort_type . startswith ( \"ArraySort(\" ): domain_range = sort_type [ len ( \"ArraySort(\" ) : - 1 ] parts = [ s . strip () for s in domain_range . split ( \",\" )] deps . extend ( parts ) For example: { \"name\" : \"MyArray\" , \"type\" : \"ArraySort(IntSort, Person)\" } This depends on: IntSort (built-in, can skip) and Person (must be defined first).","title":"Dependency Extraction"},{"location":"dsl-specification/#topological-sort","text":"Kahn's algorithm processes types in dependency order (lines 66-87): Calculate the in-degree (dependency count) for each sort Process sorts with zero dependencies first Reduce the in-degree of dependent sorts as their dependencies are satisfied Detect cycles if not all sorts can be processed (lines 90-92) Circular dependency detection: if len ( sorted_names ) != len ( dependencies ): remaining = set ( dependencies . keys ()) - set ( sorted_names ) raise ValueError ( f \"Circular dependency detected in sorts: { remaining } \" )","title":"Topological Sort"},{"location":"dsl-specification/#optimizer-independence","text":"The optimizer operates independently from the main solver. Implementation: z3adapter/optimization/optimizer.py:29-39 def __init__ ( self , ... ): self . optimizer = Optimize () # Separate instance Critical detail: The Optimize() instance is completely separate from the main Solver() and does NOT share constraints. As stated in the docstring (line 38-39): The optimizer is separate from the solver and doesn't share constraints. This is intentional to allow independent optimization problems. The optimizer maintains its own variables and constraints (lines 49-69). However, it can reference global constants through an extended context (line 60-61): base_context = self . expression_parser . build_context () opt_context = { ** base_context , ** optimization_vars }","title":"Optimizer Independence"},{"location":"dsl-specification/#execution-pipeline","text":"The interpreter follows a carefully ordered execution sequence. Implementation: z3adapter/interpreter.py:135-197 8-step execution sequence: # Step 1: Create sorts self . sort_manager . create_sorts ( self . config [ \"sorts\" ]) # Step 2: Create functions functions = self . sort_manager . create_functions ( self . config [ \"functions\" ]) # Step 3: Create constants self . sort_manager . create_constants ( self . config [ \"constants\" ]) # Step 4: Create variables variables = self . sort_manager . create_variables ( self . config . get ( \"variables\" , [])) # Step 5: Initialize expression parser self . expression_parser = ExpressionParser ( functions , constants , variables ) self . expression_parser . mark_symbols_loaded () # Enable context caching # Step 6: Add knowledge base self . expression_parser . add_knowledge_base ( self . solver , self . config [ \"knowledge_base\" ]) # Step 7: Add rules self . expression_parser . add_rules ( self . solver , self . config [ \"rules\" ], sorts ) # Step 8: Initialize verifier and add verifications self . verifier = Verifier ( self . expression_parser , sorts ) self . verifier . add_verifications ( self . config [ \"verifications\" ]) # Step 9: Perform actions (e.g., \"verify_conditions\") self . perform_actions () Symbol loading optimization: At line 172, calling mark_symbols_loaded() enables context caching (see lines 78-84 in expressions.py ). After this point, build_context() returns a cached dictionary instead of rebuilding it on each call.","title":"Execution Pipeline"},{"location":"dsl-specification/#retry-mechanism","text":"The system can automatically retry failed program generation with error feedback. Implementation: z3adapter/reasoning/proof_of_thought.py:123-191 Retry loop with error feedback: for attempt in range ( 1 , self . max_attempts + 1 ): if attempt == 1 : gen_result = self . generator . generate ( question , ... ) else : gen_result = self . generator . generate_with_feedback ( question , error_trace , previous_response , ... ) Failure modes that trigger retry: Generation failure (lines 143-149): python if not gen_result.success or gen_result.program is None: error_trace = gen_result.error or \"Failed to generate program\" continue Execution failure (lines 176-180): python if not verify_result.success: error_trace = verify_result.error or \"Z3 verification failed\" continue Ambiguous result (lines 183-191): python if verify_result.answer is None: error_trace = f\"Ambiguous verification result: SAT={sat_count}, UNSAT={unsat_count}\" continue Error feedback mechanism: Uses multi-turn conversation (see program_generator.py:130-174 ): messages = [ { \"role\" : \"user\" , \"content\" : prompt }, { \"role\" : \"assistant\" , \"content\" : previous_response }, { \"role\" : \"user\" , \"content\" : feedback_message }, ]","title":"Retry Mechanism"},{"location":"dsl-specification/#solver-semantics","text":"The solver's check method has two distinct modes of operation. Implementation: z3adapter/solvers/z3_solver.py:20-24 def check ( self , condition : Any = None ) -> Any : if condition is not None : return self . solver . check ( condition ) # Temporary hypothesis return self . solver . check () # Check all assertions Two modes of operation: solver.check() : Checks the satisfiability of all assertions added via solver.add() solver.check(\u03c6) : Checks the satisfiability of assertions \u2227 \u03c6 without permanently adding \u03c6 Verifications use mode 2 (see verifier.py:113 ): result = solver . check ( condition ) This is a temporary check \u2014 the condition is NOT permanently added to the solver. Contrast with rules: Rules : solver.add(\u03c6) \u2192 permanently modifies solver state Verifications : solver.check(\u03c6) \u2192 temporary test without modification","title":"Solver Semantics"},{"location":"dsl-specification/#built-in-sorts","text":"The DSL includes three pre-initialized built-in sorts. Implementation: z3adapter/dsl/sorts.py:31-34 def _initialize_builtin_sorts ( self ) -> None : built_in_sorts = { \"BoolSort\" : BoolSort (), \"IntSort\" : IntSort (), \"RealSort\" : RealSort ()} self . sorts . update ( built_in_sorts ) Important: Always reference these as \"BoolSort\" , \"IntSort\" , and \"RealSort\" in JSON (not \"Bool\" , \"Int\" , or \"Real\" ). These sorts are already available and should NOT be declared in the \"sorts\" section.","title":"Built-in Sorts"},{"location":"dsl-specification/#prompt-template-constraints","text":"The prompt template enforces several important constraints on DSL programs. Implementation: z3adapter/reasoning/prompt_template.py Key constraints (extracted from code):","title":"Prompt Template Constraints"},{"location":"dsl-specification/#line-228-implication-rules","text":"Rules with \"implies\" MUST have non-empty \"forall\" field: # expressions.py:184-186 if \"implies\" in rule : if not variables : raise ValueError ( \"Implication rules require quantified variables\" )","title":"Line 228: Implication Rules"},{"location":"dsl-specification/#line-298-quantifier-lists","text":"Empty quantifier lists are forbidden: # verifier.py:42-43, 55-56 if not exists_vars : raise ValueError ( f \"Empty 'exists' list in verification\" )","title":"Line 298: Quantifier Lists"},{"location":"dsl-specification/#line-416-single-verification","text":"Use a single verification per program to avoid ambiguous results: # Directly impacts determine_answer() \u2014 mixed SAT/UNSAT returns None","title":"Line 416: Single Verification"},{"location":"dsl-specification/#line-531-output-format","text":"LLM output requirements: Must wrap JSON in a markdown code block: ```json ... ``` Extracted via regex: r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\" (see program_generator.py:224 )","title":"Line 531: Output Format"},{"location":"examples/","text":"Examples \u00b6 This page demonstrates common usage patterns through example scripts. All examples are located in the examples/ directory and should be run from the project root: python examples/ { script } .py Basic Query \u00b6 The simplest way to use ProofOfThought is through a single query. File: examples/simple_usage.py from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = os . getenv ( \"OPENAI_API_KEY\" )) pot = ProofOfThought ( llm_client = client , model = \"gpt-4o\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) print ( result . answer ) # False Azure OpenAI \u00b6 For Azure OpenAI deployments, use the provided configuration utility. File: examples/azure_simple_example.py from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ]) result = pot . query ( \"Can fish breathe underwater?\" ) print ( result . answer ) # True Backend Comparison \u00b6 You can compare how the two backends perform on the same question. File: examples/backend_comparison.py config = get_client_config () question = \"Can fish breathe underwater?\" pot_json = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"json\" ) pot_smt2 = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) result_json = pot_json . query ( question ) result_smt2 = pot_smt2 . query ( question ) print ( f \"JSON: { result_json . answer } \" ) print ( f \"SMT2: { result_smt2 . answer } \" ) Batch Evaluation \u00b6 For evaluating multiple questions from a dataset, use the evaluation pipeline. File: examples/batch_evaluation.py from z3adapter.reasoning import EvaluationPipeline , ProofOfThought pot = ProofOfThought ( llm_client = client ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = \"results/\" ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , question_field = \"question\" , answer_field = \"answer\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"F1 Score: { result . metrics . f1_score : .4f } \" ) Azure + SMT2 Evaluation \u00b6 This example combines Azure OpenAI with the SMT2 backend for batch evaluation. File: examples/batch_evaluation_smt2_azure.py config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( \"data/strategyQA_train.json\" , max_samples = 50 ) Full Benchmark Suite \u00b6 For comprehensive benchmarking, the experiments pipeline runs all datasets with both backends. File: experiments_pipeline.py This script runs all 5 benchmarks (ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA) with both backends: python experiments_pipeline.py Implementation details: Modifies benchmark/bench_*.py files to set the backend via regex substitution Runs each benchmark script as a subprocess with a 1-hour timeout Collects metrics from output/{backend}_evaluation_{benchmark}/ directories Generates a markdown table and updates README.md with results Configuration ( experiments_pipeline.py:29-41 ): BENCHMARKS = { \"prontoqa\" : \"benchmark/bench_prontoqa.py\" , \"folio\" : \"benchmark/bench_folio.py\" , \"proofwriter\" : \"benchmark/bench_proofwriter.py\" , \"conditionalqa\" : \"benchmark/bench_conditionalqa.py\" , \"strategyqa\" : \"benchmark/bench_strategyqa.py\" , } BACKENDS = [ \"smt2\" , \"json\" ] Benchmark Script Structure \u00b6 Individual benchmark scripts follow a common pattern, illustrated here with StrategyQA. File: benchmark/bench_strategyqa.py config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = BACKEND , # Modified by experiments_pipeline.py max_attempts = 3 , cache_dir = f \"output/ { BACKEND } _programs_strategyqa\" , ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = f \"output/ { BACKEND } _evaluation_strategyqa\" , num_workers = 10 , # ThreadPoolExecutor for parallel processing ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , id_field = \"qid\" , max_samples = 100 , skip_existing = True , # Resume interrupted runs ) Dataset Format \u00b6 Datasets should be formatted as JSON arrays of objects: [ { \"question\" : \"Can fish breathe underwater?\" , \"answer\" : true }, { \"question\" : \"Do humans have wings?\" , \"answer\" : false } ] You can optionally include an ID field: { \"qid\" : \"sample_123\" , \"question\" : \"...\" , \"answer\" : true } Use the question_field , answer_field , and id_field parameters to specify custom field names. Saving Programs \u00b6 To save generated programs to disk for inspection: result = pot . query ( \"Can fish breathe underwater?\" , save_program = True , program_path = \"output/my_program.smt2\" ) If you don't specify a path, the default is: {cache_dir}/{auto_generated}{ext} Advanced Configuration \u00b6 For more control over the reasoning process, you can customize various parameters: pot = ProofOfThought ( llm_client = client , model = \"gpt-5\" , backend = \"smt2\" , max_attempts = 5 , # More retries verify_timeout = 20000 , # 20s timeout z3_path = \"/custom/z3\" # Custom Z3 binary )","title":"Examples"},{"location":"examples/#examples","text":"This page demonstrates common usage patterns through example scripts. All examples are located in the examples/ directory and should be run from the project root: python examples/ { script } .py","title":"Examples"},{"location":"examples/#basic-query","text":"The simplest way to use ProofOfThought is through a single query. File: examples/simple_usage.py from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = os . getenv ( \"OPENAI_API_KEY\" )) pot = ProofOfThought ( llm_client = client , model = \"gpt-4o\" ) result = pot . query ( \"Would Nancy Pelosi publicly denounce abortion?\" ) print ( result . answer ) # False","title":"Basic Query"},{"location":"examples/#azure-openai","text":"For Azure OpenAI deployments, use the provided configuration utility. File: examples/azure_simple_example.py from utils.azure_config import get_client_config from z3adapter.reasoning import ProofOfThought config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ]) result = pot . query ( \"Can fish breathe underwater?\" ) print ( result . answer ) # True","title":"Azure OpenAI"},{"location":"examples/#backend-comparison","text":"You can compare how the two backends perform on the same question. File: examples/backend_comparison.py config = get_client_config () question = \"Can fish breathe underwater?\" pot_json = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"json\" ) pot_smt2 = ProofOfThought ( llm_client = config [ \"llm_client\" ], backend = \"smt2\" ) result_json = pot_json . query ( question ) result_smt2 = pot_smt2 . query ( question ) print ( f \"JSON: { result_json . answer } \" ) print ( f \"SMT2: { result_smt2 . answer } \" )","title":"Backend Comparison"},{"location":"examples/#batch-evaluation","text":"For evaluating multiple questions from a dataset, use the evaluation pipeline. File: examples/batch_evaluation.py from z3adapter.reasoning import EvaluationPipeline , ProofOfThought pot = ProofOfThought ( llm_client = client ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = \"results/\" ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , question_field = \"question\" , answer_field = \"answer\" , max_samples = 100 ) print ( f \"Accuracy: { result . metrics . accuracy : .2% } \" ) print ( f \"F1 Score: { result . metrics . f1_score : .4f } \" )","title":"Batch Evaluation"},{"location":"examples/#azure-smt2-evaluation","text":"This example combines Azure OpenAI with the SMT2 backend for batch evaluation. File: examples/batch_evaluation_smt2_azure.py config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = \"smt2\" ) evaluator = EvaluationPipeline ( proof_of_thought = pot ) result = evaluator . evaluate ( \"data/strategyQA_train.json\" , max_samples = 50 )","title":"Azure + SMT2 Evaluation"},{"location":"examples/#full-benchmark-suite","text":"For comprehensive benchmarking, the experiments pipeline runs all datasets with both backends. File: experiments_pipeline.py This script runs all 5 benchmarks (ProntoQA, FOLIO, ProofWriter, ConditionalQA, StrategyQA) with both backends: python experiments_pipeline.py Implementation details: Modifies benchmark/bench_*.py files to set the backend via regex substitution Runs each benchmark script as a subprocess with a 1-hour timeout Collects metrics from output/{backend}_evaluation_{benchmark}/ directories Generates a markdown table and updates README.md with results Configuration ( experiments_pipeline.py:29-41 ): BENCHMARKS = { \"prontoqa\" : \"benchmark/bench_prontoqa.py\" , \"folio\" : \"benchmark/bench_folio.py\" , \"proofwriter\" : \"benchmark/bench_proofwriter.py\" , \"conditionalqa\" : \"benchmark/bench_conditionalqa.py\" , \"strategyqa\" : \"benchmark/bench_strategyqa.py\" , } BACKENDS = [ \"smt2\" , \"json\" ]","title":"Full Benchmark Suite"},{"location":"examples/#benchmark-script-structure","text":"Individual benchmark scripts follow a common pattern, illustrated here with StrategyQA. File: benchmark/bench_strategyqa.py config = get_client_config () pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = BACKEND , # Modified by experiments_pipeline.py max_attempts = 3 , cache_dir = f \"output/ { BACKEND } _programs_strategyqa\" , ) evaluator = EvaluationPipeline ( proof_of_thought = pot , output_dir = f \"output/ { BACKEND } _evaluation_strategyqa\" , num_workers = 10 , # ThreadPoolExecutor for parallel processing ) result = evaluator . evaluate ( dataset = \"data/strategyQA_train.json\" , id_field = \"qid\" , max_samples = 100 , skip_existing = True , # Resume interrupted runs )","title":"Benchmark Script Structure"},{"location":"examples/#dataset-format","text":"Datasets should be formatted as JSON arrays of objects: [ { \"question\" : \"Can fish breathe underwater?\" , \"answer\" : true }, { \"question\" : \"Do humans have wings?\" , \"answer\" : false } ] You can optionally include an ID field: { \"qid\" : \"sample_123\" , \"question\" : \"...\" , \"answer\" : true } Use the question_field , answer_field , and id_field parameters to specify custom field names.","title":"Dataset Format"},{"location":"examples/#saving-programs","text":"To save generated programs to disk for inspection: result = pot . query ( \"Can fish breathe underwater?\" , save_program = True , program_path = \"output/my_program.smt2\" ) If you don't specify a path, the default is: {cache_dir}/{auto_generated}{ext}","title":"Saving Programs"},{"location":"examples/#advanced-configuration","text":"For more control over the reasoning process, you can customize various parameters: pot = ProofOfThought ( llm_client = client , model = \"gpt-5\" , backend = \"smt2\" , max_attempts = 5 , # More retries verify_timeout = 20000 , # 20s timeout z3_path = \"/custom/z3\" # Custom Z3 binary )","title":"Advanced Configuration"},{"location":"foreword/","text":"Foreword \u00b6 The Need for Verified Reasoning \u00b6 Sound logical reasoning is essential for reliable decision-making systems, yet modern language models excel at pattern matching while struggling with rigorous deductive inference. These models can generate plausible-sounding explanations but lack the formal guarantees necessary for high-stakes applications. When deploying AI systems in domains requiring precise logical reasoning\u2014such as legal analysis, scientific hypothesis testing, or automated theorem proving\u2014we need mechanisms that go beyond statistical correlation to provide verifiable correctness. Consider common challenges in automated reasoning: Can we trust the conclusion? Does a language model's logical chain actually hold under formal scrutiny? Why is this conclusion valid? What axioms and inference rules justify a particular deduction? How should we solve this problem? What formal representation best captures the logical structure of a reasoning task? What are the limitations? Where do language models succeed or fail in logical reasoning, and why? Can the system explain its reasoning? How can we make the logical inference process transparent and auditable? While large language models have demonstrated impressive capabilities in natural language understanding and generation, a fundamental challenge remains: translating informal reasoning into formal logic while ensuring soundness . Without external verification, we cannot distinguish correct deductions from plausible-sounding but logically invalid conclusions. ProofOfThought: Bridging Natural Language and Formal Logic \u00b6 ProofOfThought addresses this challenge by combining the flexibility of language models with the rigor of automated theorem provers. It makes three key contributions: Provides a systematic pipeline for translating natural language questions into formal logic , ensuring that informal reasoning is grounded in verifiable formal representations through both SMT-LIB 2.0 and a structured JSON DSL. Leverages the Z3 theorem prover to provide sound logical verification , moving beyond language model predictions to mathematically guaranteed correctness through satisfiability checking. Implements iterative refinement with error feedback , allowing language models to learn from verification failures and improve program generation through multi-turn conversations with concrete error diagnostics. ProofOfThought's architecture bridges two complementary paradigms: the remarkable natural language understanding of modern LLMs and the formal soundness guarantees of automated theorem provers. By making the translation process explicit and the verification results interpretable, the system provides both correctness and explainability\u2014essential properties for trustworthy AI reasoning systems.","title":"Foreword"},{"location":"foreword/#foreword","text":"","title":"Foreword"},{"location":"foreword/#the-need-for-verified-reasoning","text":"Sound logical reasoning is essential for reliable decision-making systems, yet modern language models excel at pattern matching while struggling with rigorous deductive inference. These models can generate plausible-sounding explanations but lack the formal guarantees necessary for high-stakes applications. When deploying AI systems in domains requiring precise logical reasoning\u2014such as legal analysis, scientific hypothesis testing, or automated theorem proving\u2014we need mechanisms that go beyond statistical correlation to provide verifiable correctness. Consider common challenges in automated reasoning: Can we trust the conclusion? Does a language model's logical chain actually hold under formal scrutiny? Why is this conclusion valid? What axioms and inference rules justify a particular deduction? How should we solve this problem? What formal representation best captures the logical structure of a reasoning task? What are the limitations? Where do language models succeed or fail in logical reasoning, and why? Can the system explain its reasoning? How can we make the logical inference process transparent and auditable? While large language models have demonstrated impressive capabilities in natural language understanding and generation, a fundamental challenge remains: translating informal reasoning into formal logic while ensuring soundness . Without external verification, we cannot distinguish correct deductions from plausible-sounding but logically invalid conclusions.","title":"The Need for Verified Reasoning"},{"location":"foreword/#proofofthought-bridging-natural-language-and-formal-logic","text":"ProofOfThought addresses this challenge by combining the flexibility of language models with the rigor of automated theorem provers. It makes three key contributions: Provides a systematic pipeline for translating natural language questions into formal logic , ensuring that informal reasoning is grounded in verifiable formal representations through both SMT-LIB 2.0 and a structured JSON DSL. Leverages the Z3 theorem prover to provide sound logical verification , moving beyond language model predictions to mathematically guaranteed correctness through satisfiability checking. Implements iterative refinement with error feedback , allowing language models to learn from verification failures and improve program generation through multi-turn conversations with concrete error diagnostics. ProofOfThought's architecture bridges two complementary paradigms: the remarkable natural language understanding of modern LLMs and the formal soundness guarantees of automated theorem provers. By making the translation process explicit and the verification results interpretable, the system provides both correctness and explainability\u2014essential properties for trustworthy AI reasoning systems.","title":"ProofOfThought: Bridging Natural Language and Formal Logic"},{"location":"installation/","text":"Installation \u00b6 Dependencies \u00b6 ProofOfThought requires Python 3.12 or higher (as specified in pyproject.toml ). Core Dependencies \u00b6 Install the core dependencies using: pip install -r requirements.txt Z3 Verification Setup \u00b6 JSON Backend \u00b6 The JSON backend requires no additional setup beyond installing z3-solver , which includes the Python API. SMT2 Backend \u00b6 The SMT2 backend requires the Z3 CLI to be available in your PATH: z3 --version If Z3 is not found, note that the z3-solver package includes a CLI binary in site-packages . You can locate it with: python -c \"import z3; print(z3.__file__)\" # The CLI is typically located at: .../site-packages/z3/bin/z3 On macOS/Linux, you can either add it to your PATH or specify the path in your code: ProofOfThought ( ... , z3_path = \"/path/to/z3\" ) API Keys \u00b6 OpenAI \u00b6 For OpenAI access, create a .env file with: OPENAI_API_KEY = sk-... Azure OpenAI \u00b6 For Azure OpenAI deployments, configure these variables in .env : AZURE_OPENAI_API_KEY = ... AZURE_OPENAI_ENDPOINT = https://....openai.azure.com/ AZURE_OPENAI_API_VERSION = 2024 -08-01-preview AZURE_GPT5_DEPLOYMENT_NAME = gpt-5 AZURE_GPT4O_DEPLOYMENT_NAME = gpt-4o Then use it in your code: from utils.azure_config import get_client_config config = get_client_config () # Returns {\"llm_client\": AzureOpenAI(...), \"model\": str} pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ]) Verification \u00b6 To verify your installation is working correctly: python examples/simple_usage.py You should see output similar to: Question: Would Nancy Pelosi publicly denounce abortion? Answer: False Success: True Attempts: 1 Troubleshooting \u00b6 Common issues and their solutions: Z3 CLI not found (SMT2 backend) \u00b6 Error: FileNotFoundError: Z3 executable not found: 'z3' Solutions: Switch to JSON backend: ProofOfThought(backend=\"json\") Specify the Z3 path explicitly: ProofOfThought(z3_path=\"/path/to/z3\") Add Z3 to your PATH: export PATH=$PATH:/path/to/z3/bin Import errors when running examples \u00b6 Incorrect approach: cd examples python simple_usage.py # \u274c ModuleNotFoundError Correct approach: cd /path/to/proofofthought python examples/simple_usage.py # \u2713 Reason: The example scripts use sys.path.insert(0, str(Path(__file__).parent.parent)) to locate the z3adapter and utils modules from the project root. Azure authentication errors \u00b6 First, verify that all .env variables are properly set and that your endpoint URL is correct. You can test the configuration with: from utils.azure_config import get_client_config config = get_client_config () # Should not raise Version Constraints \u00b6 The following version constraints are defined in pyproject.toml and requirements.txt : Python: >=3.12 Z3: >=4.15.0 (tested with 4.15.3.0 ) OpenAI: >=2.0.0 (tested with 2.0.1 ) scikit-learn: >=1.7.0 (tested with 1.7.2 ) NumPy: >=2.3.0 (tested with 2.3.3 )","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#dependencies","text":"ProofOfThought requires Python 3.12 or higher (as specified in pyproject.toml ).","title":"Dependencies"},{"location":"installation/#core-dependencies","text":"Install the core dependencies using: pip install -r requirements.txt","title":"Core Dependencies"},{"location":"installation/#z3-verification-setup","text":"","title":"Z3 Verification Setup"},{"location":"installation/#json-backend","text":"The JSON backend requires no additional setup beyond installing z3-solver , which includes the Python API.","title":"JSON Backend"},{"location":"installation/#smt2-backend","text":"The SMT2 backend requires the Z3 CLI to be available in your PATH: z3 --version If Z3 is not found, note that the z3-solver package includes a CLI binary in site-packages . You can locate it with: python -c \"import z3; print(z3.__file__)\" # The CLI is typically located at: .../site-packages/z3/bin/z3 On macOS/Linux, you can either add it to your PATH or specify the path in your code: ProofOfThought ( ... , z3_path = \"/path/to/z3\" )","title":"SMT2 Backend"},{"location":"installation/#api-keys","text":"","title":"API Keys"},{"location":"installation/#openai","text":"For OpenAI access, create a .env file with: OPENAI_API_KEY = sk-...","title":"OpenAI"},{"location":"installation/#azure-openai","text":"For Azure OpenAI deployments, configure these variables in .env : AZURE_OPENAI_API_KEY = ... AZURE_OPENAI_ENDPOINT = https://....openai.azure.com/ AZURE_OPENAI_API_VERSION = 2024 -08-01-preview AZURE_GPT5_DEPLOYMENT_NAME = gpt-5 AZURE_GPT4O_DEPLOYMENT_NAME = gpt-4o Then use it in your code: from utils.azure_config import get_client_config config = get_client_config () # Returns {\"llm_client\": AzureOpenAI(...), \"model\": str} pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ])","title":"Azure OpenAI"},{"location":"installation/#verification","text":"To verify your installation is working correctly: python examples/simple_usage.py You should see output similar to: Question: Would Nancy Pelosi publicly denounce abortion? Answer: False Success: True Attempts: 1","title":"Verification"},{"location":"installation/#troubleshooting","text":"Common issues and their solutions:","title":"Troubleshooting"},{"location":"installation/#z3-cli-not-found-smt2-backend","text":"Error: FileNotFoundError: Z3 executable not found: 'z3' Solutions: Switch to JSON backend: ProofOfThought(backend=\"json\") Specify the Z3 path explicitly: ProofOfThought(z3_path=\"/path/to/z3\") Add Z3 to your PATH: export PATH=$PATH:/path/to/z3/bin","title":"Z3 CLI not found (SMT2 backend)"},{"location":"installation/#import-errors-when-running-examples","text":"Incorrect approach: cd examples python simple_usage.py # \u274c ModuleNotFoundError Correct approach: cd /path/to/proofofthought python examples/simple_usage.py # \u2713 Reason: The example scripts use sys.path.insert(0, str(Path(__file__).parent.parent)) to locate the z3adapter and utils modules from the project root.","title":"Import errors when running examples"},{"location":"installation/#azure-authentication-errors","text":"First, verify that all .env variables are properly set and that your endpoint URL is correct. You can test the configuration with: from utils.azure_config import get_client_config config = get_client_config () # Should not raise","title":"Azure authentication errors"},{"location":"installation/#version-constraints","text":"The following version constraints are defined in pyproject.toml and requirements.txt : Python: >=3.12 Z3: >=4.15.0 (tested with 4.15.3.0 ) OpenAI: >=2.0.0 (tested with 2.0.1 ) scikit-learn: >=1.7.0 (tested with 1.7.2 ) NumPy: >=2.3.0 (tested with 2.3.3 )","title":"Version Constraints"},{"location":"postprocessors/","text":"Postprocessing Techniques \u00b6 This page describes the postprocessing techniques available to enhance reasoning quality and reliability. All postprocessors should work seamlessly with both JSON and SMT2 backends. Overview \u00b6 Postprocessors apply advanced prompting techniques to improve the quality of reasoning results. They work by taking an initial answer and applying various strategies to verify, refine, or enhance it. Available Techniques \u00b6 Self-Refine - Iterative refinement through self-critique Self-Consistency - Majority voting across multiple reasoning paths Decomposed Prompting - Breaking complex questions into sub-questions Least-to-Most Prompting - Progressive problem solving from simple to complex Quick Start \u00b6 Basic Usage \u00b6 from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = \"...\" ) # Enable Self-Refine postprocessor pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_refine\" ], postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 }} ) result = pot . query ( \"Your complex reasoning question here\" ) print ( result . answer ) Multiple Postprocessors \u00b6 Postprocessors can be chained together: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_refine\" , \"self_consistency\" ], postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 }, \"self_consistency\" : { \"num_samples\" : 5 } } ) Per-Query Control \u00b6 Disable postprocessing for specific queries: # Postprocessors configured but disabled for this query result = pot . query ( question , enable_postprocessing = False ) Postprocessor Details \u00b6 1. Self-Refine \u00b6 Based on: \"Self-Refine: Iterative Refinement with Self-Feedback\" (Madaan et al., 2023) How it works: 1. Generates initial solution 2. LLM critiques its own solution 3. Uses feedback to refine the solution 4. Repeats until convergence or max iterations Configuration: postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 # Number of refinement iterations (default: 2) } } Best for: Questions where the initial solution might have subtle logical errors that can be caught through self-critique. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_refine\" ], postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 3 }} ) 2. Self-Consistency \u00b6 Based on: \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\" (Wang et al., 2022) How it works: 1. Generates multiple independent reasoning paths (with higher temperature) 2. Collects answers from all paths 3. Selects most consistent answer via majority voting Configuration: postprocessor_configs = { \"self_consistency\" : { \"num_samples\" : 5 # Number of independent samples (default: 5) } } Best for: Reducing variance and improving reliability on questions where random errors might occur in single attempts. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_consistency\" ], postprocessor_configs = { \"self_consistency\" : { \"num_samples\" : 7 }} ) 3. Decomposed Prompting \u00b6 Based on: \"Decomposed Prompting: A Modular Approach for Solving Complex Tasks\" (Khot et al., 2022) How it works: 1. Breaks complex question into simpler sub-questions 2. Solves each sub-question independently 3. Combines sub-answers to solve main question Configuration: postprocessor_configs = { \"decomposed\" : { \"max_subquestions\" : 5 # Maximum sub-questions to generate (default: 5) } } Best for: Multi-hop reasoning or questions requiring several logical steps. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"decomposed\" ], postprocessor_configs = { \"decomposed\" : { \"max_subquestions\" : 4 }} ) 4. Least-to-Most Prompting \u00b6 Based on: \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\" (Zhou et al., 2022) How it works: 1. Decomposes problem into progressive sub-problems (least to most complex) 2. Solves them sequentially 3. Uses solutions from simpler problems to inform complex ones Configuration: postprocessor_configs = { \"least_to_most\" : { \"max_steps\" : 5 # Maximum progressive steps (default: 5) } } Best for: Problems with natural dependencies where simpler sub-problems build up to the main question. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"least_to_most\" ], postprocessor_configs = { \"least_to_most\" : { \"max_steps\" : 4 }} ) Advanced Usage \u00b6 Using Postprocessor Instances \u00b6 For more control, create postprocessor instances directly: from z3adapter.postprocessors import SelfRefine , SelfConsistency custom_refine = SelfRefine ( num_iterations = 3 , name = \"CustomRefine\" ) custom_consistency = SelfConsistency ( num_samples = 10 , name = \"CustomConsistency\" ) pot = ProofOfThought ( llm_client = client , postprocessors = [ custom_refine , custom_consistency ] ) Registry Access \u00b6 Query available postprocessors and their defaults: from z3adapter.postprocessors import PostprocessorRegistry # List all available available = PostprocessorRegistry . list_available () print ( available ) # ['self_refine', 'self_consistency', 'decomposed', 'least_to_most'] # Get default configuration config = PostprocessorRegistry . get_default_config ( 'self_refine' ) print ( config ) # {'num_iterations': 2} # Create postprocessor postprocessor = PostprocessorRegistry . get ( 'self_refine' , num_iterations = 5 ) Creating Multiple Postprocessors \u00b6 postprocessors = PostprocessorRegistry . get_multiple ( names = [ \"self_refine\" , \"self_consistency\" ], configs = { \"self_refine\" : { \"num_iterations\" : 3 }, \"self_consistency\" : { \"num_samples\" : 7 } } ) pot = ProofOfThought ( llm_client = client , postprocessors = postprocessors ) Backend Compatibility \u00b6 All postprocessors are backend-agnostic and work with: - JSON backend ( backend=\"json\" ) - SMT2 backend ( backend=\"smt2\" ) Example with both backends: # JSON backend with postprocessing pot_json = ProofOfThought ( llm_client = client , backend = \"json\" , postprocessors = [ \"self_refine\" ] ) # SMT2 backend with postprocessing pot_smt2 = ProofOfThought ( llm_client = client , backend = \"smt2\" , postprocessors = [ \"self_refine\" ] ) Performance Considerations \u00b6 LLM Call Costs \u00b6 Postprocessors make additional LLM calls: Postprocessor Additional Calls Self-Refine 2 * num_iterations Self-Consistency num_samples - 1 Decomposed Prompting max_subquestions + 2 Least-to-Most Prompting max_steps + 2 Benchmarking with Postprocessors \u00b6 You can test postprocessors on benchmarks by modifying the benchmark scripts: # In benchmark/bench_folio.py pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = \"smt2\" , postprocessors = [ \"self_refine\" ], # Add this postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 }} # Add this ) API Reference \u00b6 ProofOfThought Parameters \u00b6 ProofOfThought ( llm_client : Any , model : str = \"gpt-5\" , backend : Literal [ \"json\" , \"smt2\" ] = \"smt2\" , postprocessors : list [ str ] | list [ Postprocessor ] | None = None , postprocessor_configs : dict [ str , dict ] | None = None , ... ) Parameters: - postprocessors : List of postprocessor names or instances - postprocessor_configs : Dictionary mapping names to configuration dicts Query Parameters \u00b6 pot . query ( question : str , enable_postprocessing : bool = True , ... ) Parameters: - enable_postprocessing : Enable/disable postprocessing for this query Examples \u00b6 See examples/postprocessor_example.py for comprehensive demonstrations of all features. Future Extensions \u00b6 The postprocessor architecture is designed to be extensible. To add new postprocessing techniques: Create a new class inheriting from Postprocessor Implement the process() method Register in PostprocessorRegistry._POSTPROCESSOR_MAP See z3adapter/postprocessors/abstract.py for the base interface.","title":"Postprocessors"},{"location":"postprocessors/#postprocessing-techniques","text":"This page describes the postprocessing techniques available to enhance reasoning quality and reliability. All postprocessors should work seamlessly with both JSON and SMT2 backends.","title":"Postprocessing Techniques"},{"location":"postprocessors/#overview","text":"Postprocessors apply advanced prompting techniques to improve the quality of reasoning results. They work by taking an initial answer and applying various strategies to verify, refine, or enhance it.","title":"Overview"},{"location":"postprocessors/#available-techniques","text":"Self-Refine - Iterative refinement through self-critique Self-Consistency - Majority voting across multiple reasoning paths Decomposed Prompting - Breaking complex questions into sub-questions Least-to-Most Prompting - Progressive problem solving from simple to complex","title":"Available Techniques"},{"location":"postprocessors/#quick-start","text":"","title":"Quick Start"},{"location":"postprocessors/#basic-usage","text":"from openai import OpenAI from z3adapter.reasoning import ProofOfThought client = OpenAI ( api_key = \"...\" ) # Enable Self-Refine postprocessor pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_refine\" ], postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 }} ) result = pot . query ( \"Your complex reasoning question here\" ) print ( result . answer )","title":"Basic Usage"},{"location":"postprocessors/#multiple-postprocessors","text":"Postprocessors can be chained together: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_refine\" , \"self_consistency\" ], postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 }, \"self_consistency\" : { \"num_samples\" : 5 } } )","title":"Multiple Postprocessors"},{"location":"postprocessors/#per-query-control","text":"Disable postprocessing for specific queries: # Postprocessors configured but disabled for this query result = pot . query ( question , enable_postprocessing = False )","title":"Per-Query Control"},{"location":"postprocessors/#postprocessor-details","text":"","title":"Postprocessor Details"},{"location":"postprocessors/#1-self-refine","text":"Based on: \"Self-Refine: Iterative Refinement with Self-Feedback\" (Madaan et al., 2023) How it works: 1. Generates initial solution 2. LLM critiques its own solution 3. Uses feedback to refine the solution 4. Repeats until convergence or max iterations Configuration: postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 # Number of refinement iterations (default: 2) } } Best for: Questions where the initial solution might have subtle logical errors that can be caught through self-critique. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_refine\" ], postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 3 }} )","title":"1. Self-Refine"},{"location":"postprocessors/#2-self-consistency","text":"Based on: \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\" (Wang et al., 2022) How it works: 1. Generates multiple independent reasoning paths (with higher temperature) 2. Collects answers from all paths 3. Selects most consistent answer via majority voting Configuration: postprocessor_configs = { \"self_consistency\" : { \"num_samples\" : 5 # Number of independent samples (default: 5) } } Best for: Reducing variance and improving reliability on questions where random errors might occur in single attempts. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"self_consistency\" ], postprocessor_configs = { \"self_consistency\" : { \"num_samples\" : 7 }} )","title":"2. Self-Consistency"},{"location":"postprocessors/#3-decomposed-prompting","text":"Based on: \"Decomposed Prompting: A Modular Approach for Solving Complex Tasks\" (Khot et al., 2022) How it works: 1. Breaks complex question into simpler sub-questions 2. Solves each sub-question independently 3. Combines sub-answers to solve main question Configuration: postprocessor_configs = { \"decomposed\" : { \"max_subquestions\" : 5 # Maximum sub-questions to generate (default: 5) } } Best for: Multi-hop reasoning or questions requiring several logical steps. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"decomposed\" ], postprocessor_configs = { \"decomposed\" : { \"max_subquestions\" : 4 }} )","title":"3. Decomposed Prompting"},{"location":"postprocessors/#4-least-to-most-prompting","text":"Based on: \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\" (Zhou et al., 2022) How it works: 1. Decomposes problem into progressive sub-problems (least to most complex) 2. Solves them sequentially 3. Uses solutions from simpler problems to inform complex ones Configuration: postprocessor_configs = { \"least_to_most\" : { \"max_steps\" : 5 # Maximum progressive steps (default: 5) } } Best for: Problems with natural dependencies where simpler sub-problems build up to the main question. Example: pot = ProofOfThought ( llm_client = client , postprocessors = [ \"least_to_most\" ], postprocessor_configs = { \"least_to_most\" : { \"max_steps\" : 4 }} )","title":"4. Least-to-Most Prompting"},{"location":"postprocessors/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"postprocessors/#using-postprocessor-instances","text":"For more control, create postprocessor instances directly: from z3adapter.postprocessors import SelfRefine , SelfConsistency custom_refine = SelfRefine ( num_iterations = 3 , name = \"CustomRefine\" ) custom_consistency = SelfConsistency ( num_samples = 10 , name = \"CustomConsistency\" ) pot = ProofOfThought ( llm_client = client , postprocessors = [ custom_refine , custom_consistency ] )","title":"Using Postprocessor Instances"},{"location":"postprocessors/#registry-access","text":"Query available postprocessors and their defaults: from z3adapter.postprocessors import PostprocessorRegistry # List all available available = PostprocessorRegistry . list_available () print ( available ) # ['self_refine', 'self_consistency', 'decomposed', 'least_to_most'] # Get default configuration config = PostprocessorRegistry . get_default_config ( 'self_refine' ) print ( config ) # {'num_iterations': 2} # Create postprocessor postprocessor = PostprocessorRegistry . get ( 'self_refine' , num_iterations = 5 )","title":"Registry Access"},{"location":"postprocessors/#creating-multiple-postprocessors","text":"postprocessors = PostprocessorRegistry . get_multiple ( names = [ \"self_refine\" , \"self_consistency\" ], configs = { \"self_refine\" : { \"num_iterations\" : 3 }, \"self_consistency\" : { \"num_samples\" : 7 } } ) pot = ProofOfThought ( llm_client = client , postprocessors = postprocessors )","title":"Creating Multiple Postprocessors"},{"location":"postprocessors/#backend-compatibility","text":"All postprocessors are backend-agnostic and work with: - JSON backend ( backend=\"json\" ) - SMT2 backend ( backend=\"smt2\" ) Example with both backends: # JSON backend with postprocessing pot_json = ProofOfThought ( llm_client = client , backend = \"json\" , postprocessors = [ \"self_refine\" ] ) # SMT2 backend with postprocessing pot_smt2 = ProofOfThought ( llm_client = client , backend = \"smt2\" , postprocessors = [ \"self_refine\" ] )","title":"Backend Compatibility"},{"location":"postprocessors/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"postprocessors/#llm-call-costs","text":"Postprocessors make additional LLM calls: Postprocessor Additional Calls Self-Refine 2 * num_iterations Self-Consistency num_samples - 1 Decomposed Prompting max_subquestions + 2 Least-to-Most Prompting max_steps + 2","title":"LLM Call Costs"},{"location":"postprocessors/#benchmarking-with-postprocessors","text":"You can test postprocessors on benchmarks by modifying the benchmark scripts: # In benchmark/bench_folio.py pot = ProofOfThought ( llm_client = config [ \"llm_client\" ], model = config [ \"model\" ], backend = \"smt2\" , postprocessors = [ \"self_refine\" ], # Add this postprocessor_configs = { \"self_refine\" : { \"num_iterations\" : 2 }} # Add this )","title":"Benchmarking with Postprocessors"},{"location":"postprocessors/#api-reference","text":"","title":"API Reference"},{"location":"postprocessors/#proofofthought-parameters","text":"ProofOfThought ( llm_client : Any , model : str = \"gpt-5\" , backend : Literal [ \"json\" , \"smt2\" ] = \"smt2\" , postprocessors : list [ str ] | list [ Postprocessor ] | None = None , postprocessor_configs : dict [ str , dict ] | None = None , ... ) Parameters: - postprocessors : List of postprocessor names or instances - postprocessor_configs : Dictionary mapping names to configuration dicts","title":"ProofOfThought Parameters"},{"location":"postprocessors/#query-parameters","text":"pot . query ( question : str , enable_postprocessing : bool = True , ... ) Parameters: - enable_postprocessing : Enable/disable postprocessing for this query","title":"Query Parameters"},{"location":"postprocessors/#examples","text":"See examples/postprocessor_example.py for comprehensive demonstrations of all features.","title":"Examples"},{"location":"postprocessors/#future-extensions","text":"The postprocessor architecture is designed to be extensible. To add new postprocessing techniques: Create a new class inheriting from Postprocessor Implement the process() method Register in PostprocessorRegistry._POSTPROCESSOR_MAP See z3adapter/postprocessors/abstract.py for the base interface.","title":"Future Extensions"}]}