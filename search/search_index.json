{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ProofOfThought","text":"<p>LLM-based reasoning using Z3 theorem proving.</p>"},{"location":"#what-is-this","title":"What is this?","text":"<p>ProofOfThought combines large language models with Z3 theorem proving to solve logical reasoning tasks. Instead of asking an LLM to \"think\" about a problem, we ask it to write formal logic programs that Z3 can verify.</p> <p>Think of it as giving your LLM a calculator for logic.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from openai import OpenAI\nfrom z3adapter.reasoning import ProofOfThought\n\nclient = OpenAI(api_key=\"...\")\npot = ProofOfThought(llm_client=client)\n\nresult = pot.query(\"Would Nancy Pelosi publicly denounce abortion?\")\nprint(result.answer)  # False\n</code></pre> <p>That's it. Three lines.</p>"},{"location":"#why-use-this","title":"Why use this?","text":"<p>Problem: LLMs hallucinate on logical reasoning tasks.</p> <p>Solution: Let Z3 do the reasoning. The LLM just writes the program.</p> <p>Result: Better accuracy on reasoning benchmarks (see Benchmarks).</p>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>You ask a question</li> <li>LLM generates a Z3 program (in JSON or SMT2 format)</li> <li>Z3 proves whether the statement is satisfiable</li> <li>You get a boolean answer</li> </ol> <p>The LLM doesn't reason\u2014it just translates natural language to formal logic. Z3 handles the reasoning.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Dual backends: Choose SMT2 (standard) or JSON (easier for LLMs)</li> <li>Azure OpenAI: Native support for GPT-4o and GPT-5</li> <li>Battle-tested: Evaluated on 5 reasoning benchmarks</li> <li>Simple API: Three-line setup, one-line queries</li> <li>Batch evaluation: Built-in pipeline for dataset evaluation</li> </ul>"},{"location":"#whats-next","title":"What's next?","text":"<ul> <li>Installation - Get set up</li> <li>API Reference - Core usage</li> <li>Backends - SMT2 vs JSON</li> <li>Examples - Real code</li> <li>Benchmarks - Performance data</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#proofofthought","title":"ProofOfThought","text":"<p>Main API for Z3-based reasoning.</p> <pre><code>from z3adapter.reasoning import ProofOfThought\n</code></pre>"},{"location":"api-reference/#constructor","title":"Constructor","text":"<pre><code>ProofOfThought(\n    llm_client,\n    model=\"gpt-5\",\n    backend=\"smt2\",\n    max_attempts=3,\n    verify_timeout=10000,\n    optimize_timeout=100000,\n    cache_dir=None,\n    z3_path=\"z3\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_client</code> - OpenAI, AzureOpenAI, or compatible LLM client</li> <li><code>model</code> (str) - Model/deployment name (default: \"gpt-5\")</li> <li><code>backend</code> (str) - Execution backend: \"smt2\" or \"json\" (default: \"smt2\")</li> <li><code>max_attempts</code> (int) - Max retries for program generation (default: 3)</li> <li><code>verify_timeout</code> (int) - Z3 verification timeout in ms (default: 10000)</li> <li><code>optimize_timeout</code> (int) - Z3 optimization timeout in ms (default: 100000)</li> <li><code>cache_dir</code> (str|None) - Directory for caching programs (default: temp dir)</li> <li><code>z3_path</code> (str) - Path to Z3 executable for SMT2 backend (default: \"z3\")</li> </ul>"},{"location":"api-reference/#query","title":"query()","text":"<p>Ask a reasoning question.</p> <pre><code>result = pot.query(\n    question,\n    max_attempts=None,\n    save_program=False\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>question</code> (str) - Natural language question</li> <li><code>max_attempts</code> (int|None) - Override max attempts for this query</li> <li><code>save_program</code> (bool) - Save generated program to disk (default: False)</li> </ul> <p>Returns: <code>QueryResult</code></p> <pre><code>@dataclass\nclass QueryResult:\n    question: str           # Original question\n    answer: bool | None     # True, False, or None if error\n    json_program: dict | None  # Generated program (if JSON backend)\n    sat_count: int          # Number of SAT results\n    unsat_count: int        # Number of UNSAT results\n    output: str             # Raw Z3 output\n    success: bool           # Did query complete?\n    num_attempts: int       # Attempts taken\n    error: str | None       # Error message if failed\n</code></pre> <p>Example:</p> <pre><code>result = pot.query(\"Can fish breathe underwater?\")\nprint(result.answer)  # True\nprint(f\"Took {result.num_attempts} attempts\")\n</code></pre>"},{"location":"api-reference/#evaluationpipeline","title":"EvaluationPipeline","text":"<p>Batch evaluation on datasets.</p> <pre><code>from z3adapter.reasoning import EvaluationPipeline\n</code></pre>"},{"location":"api-reference/#constructor_1","title":"Constructor","text":"<pre><code>EvaluationPipeline(\n    proof_of_thought,\n    output_dir=\"results/\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>proof_of_thought</code> (ProofOfThought) - Configured ProofOfThought instance</li> <li><code>output_dir</code> (str) - Directory for saving results (default: \"results/\")</li> </ul>"},{"location":"api-reference/#evaluate","title":"evaluate()","text":"<p>Run evaluation on a dataset.</p> <pre><code>result = evaluator.evaluate(\n    dataset,\n    question_field=\"question\",\n    answer_field=\"answer\",\n    max_samples=None,\n    save_results=True\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>dataset</code> (str) - Path to JSON dataset</li> <li><code>question_field</code> (str) - JSON field containing questions (default: \"question\")</li> <li><code>answer_field</code> (str) - JSON field containing answers (default: \"answer\")</li> <li><code>max_samples</code> (int|None) - Limit number of samples (default: all)</li> <li><code>save_results</code> (bool) - Save detailed results to disk (default: True)</li> </ul> <p>Returns: <code>EvaluationResult</code></p> <pre><code>@dataclass\nclass EvaluationResult:\n    metrics: EvaluationMetrics\n    predictions: list\n    dataset_name: str\n    timestamp: str\n</code></pre> <p>EvaluationMetrics:</p> <pre><code>@dataclass\nclass EvaluationMetrics:\n    accuracy: float       # Percentage correct\n    precision: float      # True positives / (TP + FP)\n    recall: float         # True positives / (TP + FN)\n    f1: float            # Harmonic mean of precision/recall\n    success_rate: float  # Percentage of queries that completed\n    total_samples: int\n    correct: int\n    incorrect: int\n    failed: int\n</code></pre> <p>Example:</p> <pre><code>from z3adapter.reasoning import ProofOfThought, EvaluationPipeline\n\npot = ProofOfThought(llm_client=client)\nevaluator = EvaluationPipeline(proof_of_thought=pot)\n\nresult = evaluator.evaluate(\n    dataset=\"data/strategyQA_train.json\",\n    max_samples=100\n)\n\nprint(f\"Accuracy: {result.metrics.accuracy:.2%}\")\nprint(f\"F1 Score: {result.metrics.f1:.4f}\")\n</code></pre>"},{"location":"api-reference/#data-classes","title":"Data Classes","text":""},{"location":"api-reference/#queryresult","title":"QueryResult","text":"<p>Returned by <code>ProofOfThought.query()</code>.</p> <p>Fields:</p> <ul> <li><code>question</code> (str) - The input question</li> <li><code>answer</code> (bool|None) - Reasoning result (True/False/None)</li> <li><code>json_program</code> (dict|None) - Generated program if using JSON backend</li> <li><code>sat_count</code> (int) - Number of SAT (satisfiable) results</li> <li><code>unsat_count</code> (int) - Number of UNSAT (unsatisfiable) results</li> <li><code>output</code> (str) - Raw Z3 output</li> <li><code>success</code> (bool) - Whether execution succeeded</li> <li><code>num_attempts</code> (int) - Generation attempts used</li> <li><code>error</code> (str|None) - Error message if failed</li> </ul>"},{"location":"api-reference/#verificationresult","title":"VerificationResult","text":"<p>Low-level result from backend execution.</p> <pre><code>from z3adapter.backends.abstract import VerificationResult\n</code></pre> <p>Fields:</p> <ul> <li><code>answer</code> (bool|None) - True (SAT), False (UNSAT), None (error)</li> <li><code>sat_count</code> (int) - Count of SAT results</li> <li><code>unsat_count</code> (int) - Count of UNSAT results</li> <li><code>output</code> (str) - Raw execution output</li> <li><code>success</code> (bool) - Execution completed</li> <li><code>error</code> (str|None) - Error message</li> </ul>"},{"location":"api-reference/#azure-openai-helper","title":"Azure OpenAI Helper","text":"<p>Utility for Azure OpenAI configuration.</p> <pre><code>from utils.azure_config import get_client_config\n</code></pre>"},{"location":"api-reference/#get_client_config","title":"get_client_config()","text":"<p>Returns configured Azure OpenAI client and settings.</p> <pre><code>config = get_client_config()\n</code></pre> <p>Returns: <code>dict</code></p> <pre><code>{\n    \"llm_client\": AzureOpenAI(...),  # Configured client\n    \"model\": \"gpt-5\"                 # Deployment name\n}\n</code></pre> <p>Environment variables required:</p> <pre><code>AZURE_OPENAI_API_KEY=...\nAZURE_OPENAI_ENDPOINT=https://....openai.azure.com/\nAZURE_OPENAI_API_VERSION=2024-08-01-preview\nAZURE_GPT5_DEPLOYMENT_NAME=gpt-5\n</code></pre> <p>Example:</p> <pre><code>from utils.azure_config import get_client_config\nfrom z3adapter.reasoning import ProofOfThought\n\nconfig = get_client_config()\npot = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    model=config[\"model\"]\n)\n</code></pre>"},{"location":"api-reference/#low-level-components","title":"Low-Level Components","text":"<p>Most users won't need these\u2014use <code>ProofOfThought</code> instead.</p>"},{"location":"api-reference/#z3programgenerator","title":"Z3ProgramGenerator","text":"<p>Generates Z3 programs from natural language.</p> <pre><code>from z3adapter.reasoning import Z3ProgramGenerator\n</code></pre>"},{"location":"api-reference/#z3verifier","title":"Z3Verifier","text":"<p>Executes and verifies Z3 programs.</p> <pre><code>from z3adapter.reasoning import Z3Verifier\n</code></pre>"},{"location":"api-reference/#backends","title":"Backends","text":"<p>Backend implementations.</p> <pre><code>from z3adapter.backends import JSONBackend, SMT2Backend\n</code></pre> <p>See Backends for details.</p>"},{"location":"backends/","title":"Backends","text":"<p>ProofOfThought supports two execution backends. Both use Z3, but they speak different languages.</p>"},{"location":"backends/#tldr","title":"TL;DR","text":"<ul> <li>Use SMT2 (default) for standard SMT-LIB format and portability</li> <li>Use JSON for better LLM reliability and richer features</li> </ul>"},{"location":"backends/#smt2-backend","title":"SMT2 Backend","text":"<p>The SMT2 backend generates programs in SMT-LIB 2.0 format and runs them via the Z3 CLI.</p>"},{"location":"backends/#example-program","title":"Example program","text":"<pre><code>(declare-sort Person 0)\n(declare-const nancy Person)\n(declare-fun supports_abortion (Person) Bool)\n(assert (supports_abortion nancy))\n(declare-const query Bool)\n(assert (= query (not (supports_abortion nancy))))\n(check-sat)\n</code></pre>"},{"location":"backends/#when-to-use","title":"When to use","text":"<ul> <li>You want industry-standard SMT-LIB format</li> <li>You need portability to other SMT solvers</li> <li>You want standalone executable programs</li> <li>You're comfortable debugging SMT2 syntax</li> </ul>"},{"location":"backends/#setup","title":"Setup","text":"<pre><code>from z3adapter.reasoning import ProofOfThought\n\npot = ProofOfThought(\n    llm_client=client,\n    backend=\"smt2\",      # default\n    z3_path=\"z3\"         # optional: path to Z3 executable\n)\n</code></pre>"},{"location":"backends/#pros","title":"Pros","text":"<p>\u2713 Standard format \u2713 No Python overhead \u2713 Portable to other solvers \u2713 Standalone executability</p>"},{"location":"backends/#cons","title":"Cons","text":"<p>\u2717 Harder for LLMs to generate correctly \u2717 Less structured error messages \u2717 Requires Z3 CLI installed</p>"},{"location":"backends/#json-backend","title":"JSON Backend","text":"<p>The JSON backend uses a custom DSL that's parsed and executed via the Z3 Python API.</p>"},{"location":"backends/#example-program_1","title":"Example program","text":"<pre><code>{\n  \"sorts\": [\"Person\"],\n  \"constants\": {\n    \"nancy\": \"Person\"\n  },\n  \"functions\": [{\n    \"name\": \"supports_abortion\",\n    \"domain\": [\"Person\"],\n    \"range\": \"Bool\"\n  }],\n  \"knowledge_base\": [\n    {\"type\": \"assert\", \"value\": \"supports_abortion(nancy)\"}\n  ],\n  \"verifications\": [{\n    \"type\": \"check_sat\",\n    \"hypotheses\": [\"Not(supports_abortion(nancy))\"]\n  }]\n}\n</code></pre>"},{"location":"backends/#when-to-use_1","title":"When to use","text":"<ul> <li>LLMs struggle with SMT2 syntax</li> <li>You want better error messages</li> <li>You don't want to install Z3 CLI</li> <li>You need richer DSL features (optimization, complex rules)</li> </ul>"},{"location":"backends/#setup_1","title":"Setup","text":"<pre><code>from z3adapter.reasoning import ProofOfThought\n\npot = ProofOfThought(\n    llm_client=client,\n    backend=\"json\"\n)\n</code></pre>"},{"location":"backends/#pros_1","title":"Pros","text":"<p>\u2713 Easier for LLMs to generate \u2713 Better error messages \u2713 No CLI dependency \u2713 Richer DSL features</p>"},{"location":"backends/#cons_1","title":"Cons","text":"<p>\u2717 Not a standard format \u2717 Only works with Z3 Python API \u2717 Not portable to other solvers</p>"},{"location":"backends/#how-it-works","title":"How it works","text":""},{"location":"backends/#program-generation","title":"Program generation","text":"<p>The LLM sees different prompts for each backend:</p> <ul> <li>SMT2: Gets examples of SMT-LIB 2.0 programs</li> <li>JSON: Gets examples of the JSON DSL</li> </ul> <p>Templates are in: - <code>z3adapter/reasoning/smt2_prompt_template.py</code> - <code>z3adapter/reasoning/prompt_template.py</code></p>"},{"location":"backends/#execution","title":"Execution","text":"<p>Both backends return the same <code>VerificationResult</code>:</p> <pre><code>@dataclass\nclass VerificationResult:\n    answer: bool | None      # True (SAT), False (UNSAT), None (error)\n    sat_count: int          # Number of SAT results\n    unsat_count: int        # Number of UNSAT results\n    output: str             # Raw Z3 output\n    success: bool           # Did execution complete?\n    error: str | None       # Error message if failed\n</code></pre>"},{"location":"backends/#file-extensions","title":"File extensions","text":"<p>Programs are saved with appropriate extensions:</p> <ul> <li>SMT2: <code>.smt2</code></li> <li>JSON: <code>.json</code></li> </ul> <p>Use <code>save_program=True</code> in <code>query()</code> to keep generated programs.</p>"},{"location":"backends/#benchmark-comparison","title":"Benchmark comparison","text":"<p>Results on 100 samples per dataset:</p> Dataset SMT2 Accuracy JSON Accuracy ProntoQA 100% 99% FOLIO 69% 76% ProofWriter 99% 96% ConditionalQA 83% 76% StrategyQA 84% 68% <p>SMT2 edges out JSON on most benchmarks, but both are viable.</p>"},{"location":"backends/#switching-backends","title":"Switching backends","text":"<p>You can compare backends on the same question:</p> <pre><code>question = \"Can fish breathe underwater?\"\n\n# Try both\npot_smt2 = ProofOfThought(llm_client=client, backend=\"smt2\")\npot_json = ProofOfThought(llm_client=client, backend=\"json\")\n\nresult_smt2 = pot_smt2.query(question)\nresult_json = pot_json.query(question)\n\nprint(f\"SMT2: {result_smt2.answer}\")\nprint(f\"JSON: {result_json.answer}\")\n</code></pre> <p>See <code>examples/backend_comparison.py</code> for a full example.</p>"},{"location":"backends/#architecture","title":"Architecture","text":"<p>Both backends implement the same interface:</p> <pre><code>class Backend(ABC):\n    @abstractmethod\n    def execute(self, program_path: str) -&gt; VerificationResult:\n        pass\n\n    @abstractmethod\n    def get_file_extension(self) -&gt; str:\n        pass\n\n    @abstractmethod\n    def get_prompt_template(self) -&gt; str:\n        pass\n</code></pre> <p>Location: <code>z3adapter/backends/</code></p> <ul> <li><code>abstract.py</code> - Interface definition</li> <li><code>smt2_backend.py</code> - SMT-LIB 2.0 implementation</li> <li><code>json_backend.py</code> - JSON DSL implementation</li> </ul>"},{"location":"backends/#troubleshooting","title":"Troubleshooting","text":"<p>SMT2: \"z3 command not found\"</p> <p>Install Z3 CLI or switch to JSON backend.</p> <p>JSON: Slower execution?</p> <p>JSON uses Python API, which has slight overhead. Usually negligible.</p> <p>Different answers between backends?</p> <p>Both backends are correct\u2014differences likely come from LLM generation variance. Run multiple queries or increase <code>max_attempts</code>.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>ProofOfThought has been evaluated on 5 logical reasoning datasets.</p>"},{"location":"benchmarks/#results","title":"Results","text":"<p>Last Updated: 2025-10-16 18:14:07</p> Benchmark Backend Samples Accuracy Precision Recall F1 Score Success Rate ProntoQA SMT2 100 100.00% 1.0000 1.0000 1.0000 100.00% FOLIO SMT2 100 69.00% 0.6949 0.7736 0.7321 99.00% ProofWriter SMT2 96 98.96% 1.0000 1.0000 1.0000 98.96% ConditionalQA SMT2 100 83.00% 0.9375 0.8219 0.8759 100.00% StrategyQA SMT2 100 84.00% 0.8205 0.7805 0.8000 100.00% ProntoQA JSON 100 99.00% 1.0000 0.9815 0.9907 100.00% FOLIO JSON 100 76.00% 0.7619 0.9412 0.8421 94.00% ProofWriter JSON 96 95.83% 1.0000 1.0000 1.0000 95.83% ConditionalQA JSON 100 76.00% 0.9180 0.8750 0.8960 89.00% StrategyQA JSON 100 68.00% 0.7500 0.7895 0.7692 86.00%"},{"location":"benchmarks/#datasets","title":"Datasets","text":""},{"location":"benchmarks/#prontoqa","title":"ProntoQA","text":"<p>Synthetic first-order logic problems with clear provable answers.</p> <p>Example:</p> <pre><code>Question: \"Stella is a lion. All lions are brown. Is Stella brown?\"\nAnswer: True\n</code></pre> <p>Performance: Near-perfect accuracy with both backends. This is the easiest dataset.</p>"},{"location":"benchmarks/#folio","title":"FOLIO","text":"<p>First-order logic inference problems derived from Wikipedia.</p> <p>Example:</p> <pre><code>Question: \"All cats are mammals. Fluffy is a cat. Is Fluffy a mammal?\"\nAnswer: True\n</code></pre> <p>Performance: Most challenging dataset. SMT2 gets 69%, JSON gets 76%. The higher accuracy with JSON suggests LLMs struggle with SMT2 syntax for complex logic.</p>"},{"location":"benchmarks/#proofwriter","title":"ProofWriter","text":"<p>Deductive reasoning over facts and rules.</p> <p>Example:</p> <pre><code>Facts: \"The bear is red. If something is red, then it is kind.\"\nQuestion: \"Is the bear kind?\"\nAnswer: True\n</code></pre> <p>Performance: Near-perfect with SMT2 (99%), very good with JSON (96%).</p>"},{"location":"benchmarks/#conditionalqa","title":"ConditionalQA","text":"<p>Reasoning with conditional statements.</p> <p>Example:</p> <pre><code>Question: \"If it rains, the ground is wet. It is raining. Is the ground wet?\"\nAnswer: True\n</code></pre> <p>Performance: SMT2 achieves 83%, JSON achieves 76%. Both backends handle conditionals well.</p>"},{"location":"benchmarks/#strategyqa","title":"StrategyQA","text":"<p>Multi-hop reasoning requiring implicit knowledge.</p> <p>Example:</p> <pre><code>Question: \"Would a vegetarian eat a burger made of plants?\"\nAnswer: True\n</code></pre> <p>Performance: SMT2 gets 84%, JSON gets 68%. This dataset requires more complex reasoning chains.</p>"},{"location":"benchmarks/#metrics-explained","title":"Metrics Explained","text":"<ul> <li>Accuracy: Percentage of correct predictions</li> <li>Precision: Of all positive predictions, how many were correct?</li> <li><code>True Positives / (True Positives + False Positives)</code></li> <li>Recall: Of all actual positives, how many did we catch?</li> <li><code>True Positives / (True Positives + False Negatives)</code></li> <li>F1 Score: Harmonic mean of precision and recall</li> <li><code>2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall)</code></li> <li>Success Rate: Percentage of queries that completed without errors</li> </ul>"},{"location":"benchmarks/#backend-comparison","title":"Backend Comparison","text":""},{"location":"benchmarks/#smt2-backend","title":"SMT2 Backend","text":"<p>Strengths: - Better on ProofWriter (99% vs 96%) - Better on ConditionalQA (83% vs 76%) - Better on StrategyQA (84% vs 68%) - Higher overall accuracy on 4/5 benchmarks</p> <p>Weaknesses: - Slightly worse on FOLIO (69% vs 76%) - More generation errors (lower success rate on some datasets)</p>"},{"location":"benchmarks/#json-backend","title":"JSON Backend","text":"<p>Strengths: - Better on FOLIO (76% vs 69%) - More reliable generation (higher success rates) - Better error messages for debugging</p> <p>Weaknesses: - Lower accuracy on StrategyQA (68% vs 84%) - Lower accuracy on ConditionalQA (76% vs 83%)</p>"},{"location":"benchmarks/#running-your-own-benchmarks","title":"Running Your Own Benchmarks","text":""},{"location":"benchmarks/#full-suite","title":"Full suite","text":"<pre><code>python experiments_pipeline.py\n</code></pre> <p>This runs all 5 benchmarks with both backends and updates the README.</p>"},{"location":"benchmarks/#single-benchmark","title":"Single benchmark","text":"<pre><code>from utils.azure_config import get_client_config\nfrom z3adapter.reasoning import ProofOfThought, EvaluationPipeline\n\nconfig = get_client_config()\n\npot = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    model=config[\"model\"],\n    backend=\"smt2\"\n)\n\nevaluator = EvaluationPipeline(proof_of_thought=pot)\n\nresult = evaluator.evaluate(\n    dataset=\"data/strategyQA_train.json\",\n    max_samples=100\n)\n\nprint(f\"Accuracy: {result.metrics.accuracy:.2%}\")\n</code></pre>"},{"location":"benchmarks/#custom-datasets","title":"Custom datasets","text":"<p>See Examples for dataset format requirements.</p>"},{"location":"benchmarks/#performance-notes","title":"Performance Notes","text":"<p>Generation time: 2-5 seconds per query on average with GPT-5.</p> <p>Success rate: SMT2 achieves 99-100% success rate. JSON achieves 86-100% success rate.</p> <p>Timeouts: Default timeouts (10s verify, 100s optimize) work well for these datasets.</p>"},{"location":"benchmarks/#why-the-differences","title":"Why the differences?","text":"<p>Backend performance varies because:</p> <ol> <li>LLM generation reliability: JSON is more structured, easier for LLMs to generate correctly</li> <li>Syntax complexity: SMT2 requires precise S-expression syntax</li> <li>Error recovery: JSON provides better error messages, leading to better retries</li> <li>Dataset characteristics: Some logical patterns are easier in one DSL vs the other</li> </ol> <p>Recommendation: Start with SMT2 (default). Switch to JSON if you see low success rates.</p>"},{"location":"benchmarks/#reproducing-results","title":"Reproducing Results","text":"<p>All results are from:</p> <ul> <li>Model: GPT-5 (Azure deployment)</li> <li>Max attempts: 3</li> <li>Verify timeout: 10000ms</li> <li>Optimize timeout: 100000ms</li> <li>Sample size: 100 per dataset (96 for ProofWriter)</li> </ul> <p>To reproduce:</p> <pre><code># Set up Azure config in .env\n# Then run experiments\npython experiments_pipeline.py\n</code></pre> <p>Results are saved to <code>results/</code> with timestamp and full metrics.</p>"},{"location":"examples/","title":"Examples","text":"<p>All examples are in the <code>examples/</code> directory. Run them from the project root:</p> <pre><code>cd /path/to/proofofthought\npython examples/simple_usage.py\n</code></pre>"},{"location":"examples/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/#single-query","title":"Single query","text":"<pre><code>from openai import OpenAI\nfrom z3adapter.reasoning import ProofOfThought\n\n# Set up client\nclient = OpenAI(api_key=\"...\")\npot = ProofOfThought(llm_client=client, model=\"gpt-4o\")\n\n# Ask a question\nresult = pot.query(\"Would Nancy Pelosi publicly denounce abortion?\")\n\n# Check the answer\nprint(result.answer)  # False\nprint(f\"Successful: {result.success}\")\nprint(f\"Attempts: {result.num_attempts}\")\n</code></pre> <p>File: <code>examples/simple_usage.py</code></p>"},{"location":"examples/#azure-openai","title":"Azure OpenAI","text":"<pre><code>from utils.azure_config import get_client_config\nfrom z3adapter.reasoning import ProofOfThought\n\n# Get Azure config from environment\nconfig = get_client_config()\n\n# Initialize with Azure client\npot = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    model=config[\"model\"]\n)\n\n# Use normally\nresult = pot.query(\"Can fish breathe underwater?\")\nprint(result.answer)  # True\n</code></pre> <p>File: <code>examples/azure_simple_example.py</code></p> <p>Required <code>.env</code> variables:</p> <pre><code>AZURE_OPENAI_API_KEY=...\nAZURE_OPENAI_ENDPOINT=https://....openai.azure.com/\nAZURE_OPENAI_API_VERSION=2024-08-01-preview\nAZURE_GPT5_DEPLOYMENT_NAME=gpt-5\n</code></pre>"},{"location":"examples/#backend-comparison","title":"Backend Comparison","text":""},{"location":"examples/#test-both-backends","title":"Test both backends","text":"<pre><code>from utils.azure_config import get_client_config\nfrom z3adapter.reasoning import ProofOfThought\n\nconfig = get_client_config()\nquestion = \"Can fish breathe underwater?\"\n\n# JSON backend\npot_json = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    backend=\"json\"\n)\nresult_json = pot_json.query(question)\n\n# SMT2 backend\npot_smt2 = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    backend=\"smt2\"\n)\nresult_smt2 = pot_smt2.query(question)\n\nprint(f\"JSON backend: {result_json.answer}\")\nprint(f\"SMT2 backend: {result_smt2.answer}\")\n</code></pre> <p>File: <code>examples/backend_comparison.py</code></p>"},{"location":"examples/#batch-evaluation","title":"Batch Evaluation","text":""},{"location":"examples/#evaluate-on-a-dataset","title":"Evaluate on a dataset","text":"<pre><code>from z3adapter.reasoning import ProofOfThought, EvaluationPipeline\n\n# Set up evaluator\npot = ProofOfThought(llm_client=client)\nevaluator = EvaluationPipeline(\n    proof_of_thought=pot,\n    output_dir=\"results/\"\n)\n\n# Run evaluation\nresult = evaluator.evaluate(\n    dataset=\"data/strategyQA_train.json\",\n    question_field=\"question\",\n    answer_field=\"answer\",\n    max_samples=100\n)\n\n# Print metrics\nprint(f\"Accuracy: {result.metrics.accuracy:.2%}\")\nprint(f\"Precision: {result.metrics.precision:.4f}\")\nprint(f\"Recall: {result.metrics.recall:.4f}\")\nprint(f\"F1 Score: {result.metrics.f1:.4f}\")\nprint(f\"Success Rate: {result.metrics.success_rate:.2%}\")\n</code></pre> <p>File: <code>examples/batch_evaluation.py</code></p>"},{"location":"examples/#with-azure-and-smt2-backend","title":"With Azure and SMT2 backend","text":"<pre><code>from utils.azure_config import get_client_config\nfrom z3adapter.reasoning import ProofOfThought, EvaluationPipeline\n\nconfig = get_client_config()\n\npot = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    model=config[\"model\"],\n    backend=\"smt2\"\n)\n\nevaluator = EvaluationPipeline(proof_of_thought=pot)\nresult = evaluator.evaluate(\n    dataset=\"data/strategyQA_train.json\",\n    max_samples=50\n)\n\nprint(f\"Accuracy: {result.metrics.accuracy:.2%}\")\n</code></pre> <p>File: <code>examples/batch_evaluation_smt2_azure.py</code></p>"},{"location":"examples/#running-experiments","title":"Running Experiments","text":""},{"location":"examples/#full-benchmark-suite","title":"Full benchmark suite","text":"<p>Run all 5 benchmarks with both backends:</p> <pre><code>python experiments_pipeline.py\n</code></pre> <p>This evaluates:</p> <ul> <li>ProntoQA (100 samples)</li> <li>FOLIO (100 samples)</li> <li>ProofWriter (96 samples)</li> <li>ConditionalQA (100 samples)</li> <li>StrategyQA (100 samples)</li> </ul> <p>Results are saved to <code>results/</code> and the README is auto-updated.</p> <p>File: <code>experiments_pipeline.py</code></p>"},{"location":"examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/#save-generated-programs","title":"Save generated programs","text":"<pre><code>result = pot.query(\n    \"Can fish breathe underwater?\",\n    save_program=True\n)\n\n# Programs saved to cache_dir with .json or .smt2 extension\nprint(f\"Program saved: {result.json_program}\")\n</code></pre>"},{"location":"examples/#custom-timeouts","title":"Custom timeouts","text":"<pre><code>pot = ProofOfThought(\n    llm_client=client,\n    verify_timeout=5000,      # 5 seconds\n    optimize_timeout=50000,   # 50 seconds\n    max_attempts=5\n)\n</code></pre>"},{"location":"examples/#custom-z3-path","title":"Custom Z3 path","text":"<pre><code>pot = ProofOfThought(\n    llm_client=client,\n    backend=\"smt2\",\n    z3_path=\"/usr/local/bin/z3\"\n)\n</code></pre>"},{"location":"examples/#override-max-attempts-per-query","title":"Override max attempts per query","text":"<pre><code># Default max_attempts=3\npot = ProofOfThought(llm_client=client)\n\n# Override for specific query\nresult = pot.query(\n    \"Complex question...\",\n    max_attempts=10\n)\n</code></pre>"},{"location":"examples/#dataset-format","title":"Dataset Format","text":"<p>Evaluation expects JSON with question/answer fields:</p> <pre><code>[\n  {\n    \"question\": \"Can fish breathe underwater?\",\n    \"answer\": true\n  },\n  {\n    \"question\": \"Do humans have wings?\",\n    \"answer\": false\n  }\n]\n</code></pre> <p>Custom field names:</p> <pre><code>result = evaluator.evaluate(\n    dataset=\"custom_dataset.json\",\n    question_field=\"query\",\n    answer_field=\"label\",\n    max_samples=100\n)\n</code></pre>"},{"location":"examples/#testing-strategy","title":"Testing Strategy","text":"<p>Want to test on a specific dataset? See <code>examples/test_strategyqa.py</code>:</p> <pre><code>from utils.azure_config import get_client_config\nfrom z3adapter.reasoning import ProofOfThought, EvaluationPipeline\n\nconfig = get_client_config()\n\n# JSON backend\npot_json = ProofOfThought(\n    llm_client=config[\"llm_client\"],\n    model=config[\"model\"],\n    backend=\"json\"\n)\n\nevaluator = EvaluationPipeline(proof_of_thought=pot_json)\nresult = evaluator.evaluate(\n    dataset=\"data/strategyQA_train.json\",\n    max_samples=100\n)\n\nprint(f\"StrategyQA JSON Backend Accuracy: {result.metrics.accuracy:.2%}\")\n</code></pre>"},{"location":"examples/#troubleshooting","title":"Troubleshooting","text":"<p>Examples fail with import errors?</p> <p>Make sure you're running from the project root, not from <code>examples/</code>:</p> <pre><code># Wrong\ncd examples\npython simple_usage.py  # \u274c Import error\n\n# Right\ncd /path/to/proofofthought\npython examples/simple_usage.py  # \u2713 Works\n</code></pre> <p>Azure config not found?</p> <p>Check your <code>.env</code> file has all required Azure variables. See Installation.</p> <p>Z3 not found?</p> <p>Either install Z3 CLI or use JSON backend:</p> <pre><code>pot = ProofOfThought(llm_client=client, backend=\"json\")\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>Z3 theorem prover</li> <li>OpenAI-compatible LLM API</li> </ul>"},{"location":"installation/#install-python-dependencies","title":"Install Python dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre> <p>This installs: - <code>z3-solver</code> - Z3 Python bindings - <code>openai</code> - LLM client - <code>scikit-learn</code>, <code>numpy</code> - For evaluation metrics - <code>python-dotenv</code> - Environment variable management</p>"},{"location":"installation/#install-z3-cli-for-smt2-backend","title":"Install Z3 CLI (for SMT2 backend)","text":"<p>The SMT2 backend needs the Z3 command-line tool.</p>"},{"location":"installation/#macos","title":"macOS","text":"<pre><code>brew install z3\n</code></pre>"},{"location":"installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt-get install z3\n</code></pre>"},{"location":"installation/#from-source","title":"From source","text":"<pre><code>git clone https://github.com/Z3Prover/z3.git\ncd z3\npython scripts/mk_make.py\ncd build\nmake\nsudo make install\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify installation","text":"<pre><code>z3 --version\n</code></pre> <p>Note: The JSON backend doesn't need the CLI\u2014it uses Z3's Python API.</p>"},{"location":"installation/#set-up-api-keys","title":"Set up API keys","text":""},{"location":"installation/#openai","title":"OpenAI","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code>OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"installation/#azure-openai","title":"Azure OpenAI","text":"<p>For Azure GPT-4o or GPT-5, add these to <code>.env</code>:</p> <pre><code>AZURE_OPENAI_API_KEY=...\nAZURE_OPENAI_ENDPOINT=https://....openai.azure.com/\nAZURE_OPENAI_API_VERSION=2024-08-01-preview\nAZURE_GPT5_DEPLOYMENT_NAME=gpt-5\nAZURE_GPT4O_DEPLOYMENT_NAME=gpt-4o\n</code></pre> <p>See <code>examples/azure_simple_example.py</code> for Azure usage.</p>"},{"location":"installation/#verify-setup","title":"Verify setup","text":"<p>Run a quick test:</p> <pre><code>python examples/simple_usage.py\n</code></pre> <p>You should see output like:</p> <pre><code>Question: Would Nancy Pelosi publicly denounce abortion?\nAnswer: False\nSuccess: True\n</code></pre>"},{"location":"installation/#development-setup","title":"Development setup","text":"<p>For contributors:</p> <pre><code>pip install -e \".[dev]\"\npre-commit install\n</code></pre> <p>This installs development tools: - <code>black</code> - Code formatter - <code>ruff</code> - Linter - <code>mypy</code> - Type checker - <code>pytest</code> - Test runner - <code>pre-commit</code> - Git hooks</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>Z3 not found?</p> <p>If you get <code>z3: command not found</code>, either: 1. Install the Z3 CLI (see above) 2. Use the JSON backend instead: <code>ProofOfThought(backend=\"json\")</code></p> <p>API key errors?</p> <p>Make sure your <code>.env</code> file is in the project root and contains valid keys.</p> <p>Import errors?</p> <p>Run examples from the project root:</p> <pre><code>cd /path/to/proofofthought\npython examples/simple_usage.py\n</code></pre> <p>Not from the <code>examples/</code> directory.</p>"}]}